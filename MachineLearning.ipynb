{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "- [Caltech's Machine Learning Course - CS 156](https://work.caltech.edu/lectures.html)\n",
    "- Textbook: [Learning from Data](https://www.amazon.com/gp/product/1600490069)\n",
    " - E-Chapters [here](https://amlbook.com/eChapters.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Learning Problem\n",
    "[Lecture](https://youtu.be/mbyG85GZ0PI)\n",
    "Textbook chapters 1.1, 1.2\n",
    "\n",
    "## Examples of machine learning\n",
    "\n",
    "Example: predicting how a viewer will rate a movie.  This is worth millions to Netflix.  Even small improvements are worth a huge amount, e.g. in financial forecasting.\n",
    "\n",
    "The essence of machine learning:\n",
    "- A pattern exists\n",
    "- We cannot pin it down mathematically.\n",
    "- We have data on it\n",
    "\n",
    "In the example of predicting movie ratings, we could consider the viewer and the movie as vectors of different attributes like \"enjoys comedy\", \"is comedy movie\", etc, and combines those to output a predicted rating.  But this would be impractical unless we could interview people, watch the movies, etc.  \n",
    "\n",
    "Instead, machine learning starts with the rating and tries to reverse engineer the viewer and movie vectors.  We could start with just random numbers representing the viewer and movie and vectors.  When ratings happen, we nudge the factors in those vectors toward those ratings; make the inner product of the viewer and movie vectors get closer to that rating.  With millions of ratings, this process will drive us towards meaningful factors.\n",
    "\n",
    "## Components of Learning\n",
    "\n",
    "What are the mathematical components that make up the learning problem?\n",
    "\n",
    "- Input: $x$, can think of it as a $d$ dimensional vector. (e.g. customer credit application).  The input space $\\mathcal{X}$ is the set of all possible inputs $x$.\n",
    "- Output $y$.  Might be a binary (e.g. good or bad credit score determination) or numeric or whatever.  The output space $\\mathcal{Y}$ is the set of all possible outputs $y$.\n",
    "- **Target Function**: $f: x \\rightarrow y$  (e.g. ideal credit approval formula)\n",
    "- Data $\\mathcal{D}$: $(x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)$.  Pairs of input/output examples, where $y_n = f(X_n)$ (e.g. historical records).  The $y$ values here are what the result should've been, according to the ideal target function.\n",
    "- Hypothesis: $g: x \\rightarrow y$.  From the data we generate our hypothesis, which lives in the same world as the target function.  The hope is that it approximates the target function.\n",
    "\n",
    "In all our endeavours in machine learning, the target function is unknown to us.  If it were known, we wouldn't need learning.  Throughout the course we will use the above notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](images/learning-components.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target function and the training examples are not under control.  The 2 solution components are the learning algorithm and the hypothesis set.  These are the things we choose to solve the problem.\n",
    "\n",
    "## A simple learning model \n",
    "\n",
    "The hypothesis set is $\\mathcal{H} = \\{h\\}$.  The final hypothesis $g \\in \\mathcal{H}$. Together the hypothesis set and the learning algorithm are called the **learning model**.\n",
    "\n",
    "Let's consider a simple hypothesis set-- the perceptron.\n",
    "\n",
    "For input $x = (x_1,\\cdots, x_d)$, customer attributes:\n",
    "\n",
    "$$\\text{Approve credit if } \\sum^d_{i=1} w_ix_i > \\text{threshold}$$\n",
    "\n",
    "$w$ is a vector of our weights.  The weights will be negative for attributes on $x$ that are bad when they go higher, like more outstanding debt.  What makes this a perceptron\n",
    "\n",
    "Our formula can be written more compactly as:\n",
    "\n",
    "$$\n",
    "h(\\mathbf{x})=\\operatorname{sign}\\left(\\left(\\sum_{i=1}^d w_i x_i\\right)+b\\right)\n",
    "$$\n",
    "\n",
    "where $h(x) = +1$ means approve credit, and $h(x) = -1$ means deny credit.\n",
    "\n",
    "This model of $\\mathcal{H}$ is called the **perceptron**.  The learning algorithm will search $\\mathcal{H}$ by looking for weights and bias that perform well on the data set. \n",
    "\n",
    "The bias value $b$ may end up being large or small, depending on how lenient or stringent the bank should be in extending credit.\n",
    "\n",
    "To simplify the notation of the perceptron formula, we will treat the bias $b$ as a weight $w_0 = b$ and merge it with the other weights into one vector $w = [w_0,w_1,\\cdots,w_d]^T$, so $w$ is a column vector.  Formally speaking, the input space is now:\n",
    "$$\n",
    "\\mathcal{X}=\\{1\\} \\times \\mathbb{R}^d=\\left\\{\\left[x_0, x_1, \\cdots, x_d\\right]^{\\mathrm{T}} \\mid x_0=1, x_1 \\in \\mathbb{R}, \\cdots, x_d \\in \\mathbb{R}\\right\\}\n",
    "$$\n",
    "\n",
    "With this convention, $w^Tx = \\sum^d_{i=0} w_ix_i$ and so our equation can be written in vector form as:\n",
    "\n",
    "$$\n",
    "h(\\mathbf{x})=\\operatorname{sign}\\left(\\mathbf{w}^{\\mathrm{T}} \\mathbf{x}\\right)\n",
    "$$\n",
    "\n",
    "We now introduce the **perceptron learning algorithm** (PLA).  The algorithm will determine what $w$ should be, based on the data.  Let's assume that the data set is **linearly separable**, which means that there is a vector $w$ that makes our $h(x_n) = y_n$ on all the training examples.\n",
    "\n",
    "Our learning algorithm will find this $w$ using a simple iterative method.  At iteration $t = 0, 1, 2, \\dots$, there is a current value of the weight vector, call it $w(t)$.  the algorithm picks an example from $(x_1, y_1)\\cdots(x_N,y_N)$ that is currently misclassified, call it $(x(t),y(t))$, and uses it to update $w(t)$.  Since the example is misclassified, we have $y(t) \\ne \\operatorname{sign}(w^T(t)x(t))$.  The update rule is:\n",
    "\n",
    "$$w(t + 1) = w(t) + y(t)x(t)$$\n",
    "\n",
    "Although the update rule considers only one training example at a time and may mess up the classification of the other examples that are not involved in the current iteration, it turns out that the algorithm is guaranteed to arrive at the right solution in the end.\n",
    "\n",
    "## Types of learning\n",
    "\n",
    "When the training data contains explicit examples of what the correct output should be for given inputs, then we are within the **supervised learning** setting that we have covered so far.  The learning is supervised  in the sense that some 'supervisor' has taken the trouble to look at each input and determine the correct output.\n",
    "\n",
    "It is worth noting that two variations of this protocol have attracted a significant body of work.  One is **active learning**, where the data set is acquired through queries that we make.  Thus, we get to choose a point $x$ in the input space, and the supervisor reports to us the target value for $x$.  This opens the possiblity for strategic choice of the point $x$ to maximize its information value, similar to asking a strategic question in a game of 20 questions.\n",
    "\n",
    "Another variation is called **online learning**, where the data set is given to the algorithm one example at a time.  This happens when we have streaming data that the algorithm has to process 'on the run.' Online learning is also useful when we have limitations on computing and storage that preclude us from processing the whole data as a batch.  We should note that online learning can be used in different paradigms of learning, not just in supervised learning.\n",
    "\n",
    "When the training data does not explicitly contain the correct output for each input, we are no longer in a supervised learning setting.  Consider a toddler learning not to touch a hot cup of tea.  The training examples would not spell out what the toddler should have done, but instead graded different actions they have taken.    Nevertheless, they use the examples to reinforce the better actions, eventually learning what they should do in similar situations.  This characterizes **reinforcement learning**, where the training example does not contain the target output, but instead contains some possible output  together with a measure of how good that output is.  In contrast to supervised learning where the training examples were of the form (input, correct output), the examples in reinforcement learning are of the form (input, some output, grade for this output).\n",
    "\n",
    "Importantly, the example does not say how good other outputs would have been for this particular input.\n",
    "\n",
    "Reinforcement learning is especially useful for learning how to play a game.  It is not a trivial task to ascertain what the best action is at a given stage of the game, so we cannot easily create supervised learning examples.  If you use reinforcement learning instead, all you need to do is to take some action and report how well things went, and you have a training example.  The reinforcement learning algorithm is left with the task of sorting out the information coming from different examples to find the best line of play.\n",
    "\n",
    "In the **unsupervised** setting, the training data does not contain any output information at all.  We are just given input examples $x_1,\\cdots,x_N$.  You may wonder how we could possibly learn anything from mere inputs.  Consider the problem of coin classification.  Suppose that we didn't know the denomination of any of the coins in the data set.  This is **unlabeled data**.  We still get similar clusters as with supervision, but they are now unlabeled so all points have the same 'color.'  The decision regions in unsupervised learning may be identical to those in supervised learning, but without the labels.  However, the correct clustering is less obvious now, and even the number of clusters may be ambiguous.\n",
    "\n",
    "Nonetheless, this example shows that we can learn something from the inputs by themselves.  Unsupervised learning can be viewed as the task of spontaneously finding patterns and structure in input data.  For instance, if our task is to categorize a set of books into topics, and we only use general properties of various books, we can identify books that have similar properties and put them together in one category, without naming that category.\n",
    "\n",
    "Unsupervised learning can also be viewed as a way to create higher-level representation of the data.  Imagine you decide to learn Spanish by just listening to Spanish radio; this is an unsupervised learning experience since you don't know the meaning of the words.  However, you gradually develop a better representation of the language in your brain by becoming more tuned to its common sounds and structures.  Unsupervised learning can be a precursor to supervised learning.  In other cases, it is a stand-alone technique.\n",
    "\n",
    "Our main focused will be supervised learning, which is the most popular form of learning from data.\n",
    "\n",
    "The study of learning has evolved somewhat independently in a number of fields, which have their own approach.  The main field dedicated to the subject is called **machine learning**, a name which distinguishes it from human learning.  \n",
    "\n",
    "**Statistics** shares the basic premise of learning from data, namely the use of a set of observations to uncover an underlying process.  In this case, the process is a probability distribution and the observations are samples from that distribution.  Because stats is a mathematical field, emphasis is given to situations where most of the questions can be answered with rigorous proofs.  As a result, stats focuses on somewhat idealized models and analyzes them in great detail.  This is the main difference between the statistical approach to learning and how we approach the subject here.  We make less restrictive assumptions and deal with more general models than in stats.  Therefore, we end up with weaker results that are nonetheless broadly applicable.  \n",
    "\n",
    "**Data mining** is a practical field that focuses on finding patterns, correlations, or anomalies in large relational databases.  For example, we could be looking at medical records of patients and trying to detect a cause-effect relationship between a particular drug and long-term effects.  We could also be looking at credit card spending patterns and trying to detect potential fraud.  Technically, data mining is the same as learning from data, with more emphasis on data analysis than on prediction.  Because databases are usually huge, computational issues are often critical in data mining.  Recommendation systems are also considered part of data mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is Learning Feasible?\n",
    "\n",
    "[Lecture](https://youtu.be/MEG35RDD7RA)\n",
    "Textbook chapter 1.3\n",
    "\n",
    "The target function $f$ is the object of learning.  the most important assertion about the target function is that it is unknown.  \n",
    "\n",
    "This raises a natural question.  How could a limited data set reveal enough information to pin down the entire target function?  We saw the difficulties of this with the pattern matching example, where you could come up with multiple valid patterns for the provided data.  \n",
    "\n",
    "As long as $f$ is an unknown function, knowing $\\mathcal{D}$ cannot exclude any pattern of values for $f$ outside $\\mathcal{D}$.  Therefore, the predictions of $g$ outside $\\mathcal{D}$ are meaningless.\n",
    "\n",
    "## Probability to the rescue\n",
    "\n",
    "We will show that we can indeed infer something outside $\\mathcal{D}$ using only $\\mathcal{D}$, but in a probabilistic way.  What we infer may not be much compared to learning a full target function, but it will establish the principle that we can reach outside $\\mathcal{D}$.  Once we establish that, we will take it to the general learning problem and pin down what we can and cannot learn.\n",
    "\n",
    "Let's take the simplest case of picking a sample, and see when we can say something about the objects outside the sample.  Consider a bin that contains red and green marbles, possibly infinitely many.  The proportion of red and green marbles in the bin is such that if we pick a marble at random, the proabbility that it will be red is $\\mu$ and the probability that it will be green is $1 - \\mu$.  We assume that the value of $\\mu$ is unknown to us.\n",
    "\n",
    "We pick a random sample of $N$ independent marbles (with replacement) from this bin, and observe the fraction $\\nu$ of red marbles within the sample.  what does the value of $\\nu$ tell us about the value of $\\mu$?\n",
    "\n",
    "One answer is that regardless of the colors of the $N$ marbles that we picked, we still don't know the color of any marble that we didn't pick.  We can get mostly green marbles in the sample while the bin has mostly red marbles.  Although this is certainly possible, it is by no means probable.\n",
    "\n",
    "The situation is similar to taking a poll.  A random sample from a population tends to agree with the views of the population at large.  The probability distribution of the random variable $\\nu$ in terms of the parameter $\\mu$ is well understood, and when the sample size is big, $\\nu$ tends to be close to $\\mu$.\n",
    "\n",
    "To quantify the relationship between $\\nu$ and $\\mu$, we use a simple bound called the **Hoeffding Inequality**.  It states that for any sample size $N$,\n",
    "\n",
    "$$\n",
    "\\mathbb{P}[|\\nu-\\mu|>\\epsilon] \\leq 2 e^{-2 \\epsilon^2 N} \\quad \\text { for any } \\epsilon>0\n",
    "$$\n",
    "\n",
    "Here $\\mathbb{P}$ denotes the probabilty of an event, in this case with respect to the random sample we pick, and $\\epsilon$ is any positive value we choose.  Putting it into words, it says that as the sample size $N$ grows, it becomes exponentially unlikely that $\\nu$ will deviate from $\\mu$ by more than our 'tolerance' $\\epsilon$.\n",
    "\n",
    "The only quantity that is random here is $\\nu$ which depends on the random sample.  By contrast, $\\mu$ is not random.  It is just a constant, albeit unknown to us.  There is a subtle point here.  The utility of this inequality is to infer the value of $\\mu$ using the value of $\\nu$, although it is $\\mu$ that affects $\\nu$, not vice versa.  However, since the effect is that $\\nu$ tends to be close to $\\mu$, we infer that $\\mu$ 'tends' to be close to $\\nu$.\n",
    "\n",
    "Although $\\mathbb{P}[|\\nu-\\mu|>\\epsilon]$ depends on $\\mu$, as $\\mu$ appears in the argument and also affects the distribution of $\\nu$, we are able to bound the probability by $2 e^{-2 \\epsilon^2 N}$ which does not depend on $\\mu$.  Notice that only the size $N$ of the sample affects the bound, not the size of the bin.  The bin can be large or small, finite or infinite, and we still get the same bound when we use the same sample size.\n",
    "\n",
    "If we choose $\\epsilon$ to be very small in order to make $\\nu$ a good approximation of $\\mu$, we need a larger sample size $N$ to make the RHS of the inequality small.  We can then assert that it is likely that $\\nu$ will indeed be a good approximation of $\\mu$.  Although this assertion does not give us the exact value of $\\mu$, and doesn't even guarantee that the approximate value holds, knowing that we are within $\\pm \\epsilon$ of $\\mu$ most of the time is a significant improvement over not knowing anything at all.\n",
    "\n",
    "the fact that the sample was randomly selected from the bin is the reason we are able to make any kind of statement about $\\mu$ being close to $\\nu$.  If the sample was not randomly selected but picked in a particular way, we would lose the benefit of the probabilistic analysis and we would again be in the dark outside of the sample.\n",
    "\n",
    "How does the bin model relate to the learning problem?  It seems that the unknown here was just the value of $\\mu$ while the unknown in learning is an entire function $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$.  The two situations can be connected.  Take any single hypothesis $h \\in \\mathcal{H}$ and compare it to $f$ on each point $x \\in \\mathcal{X}$.  If $h(x) = f(x)$, color the point $x$ green.  If $h(x) \\ne f(x)$, color the point $x$ red.  The color that each point gets is not known to us, since $f$ is unknown.  However, if we pick $x$ at random according to some probability distribution $P$ over the input space $\\mathcal{X}$, we know that $x$ will be red with some probability, call it $\\mu$, and green with some probability $1 - \\mu$.  Regardless of the value of $\\mu$, the space $\\mathcal{X}$ now behaves like the bin.\n",
    "\n",
    "The training examples play the role of a sample from the bin.  If the inputs $x_1, \\cdots, x_N$ in $\\mathcal{D}$ are picked independently according to $P$, we wil get a random sample of red ($h(x_n) \\ne f(x_n)$) and green ($h(x_n) \\= f(x_n)$) points.  Each point will be red with probability $\\mu$ and green with probability $1 - \\mu$.  The color of each point will be known to us since both $h(x_n)$ and $f(x_n)$ are known for each $n = 1, \\cdots, N$ (the function $h$ is our hypothesis so we can evaluate it on any point, and $f(x_n) = y_n$ is given to us for all points in the data set $\\mathcal{D}$).  The learning problem is now reduced to a bin problem, under the assumption that the inputs in $\\mathcal{D}$ are picked independently according to some distribution $P$ on $\\mathcal{X}$.  Any $P$ will translate to some $\\mu$ in the equivalent bin.  since $\\mu$ is allowed to be unknown, $P$ can be unknown to us as well.  The figure below adds this probabilistic component to the basic learning setup we saw earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](images/learning-prob.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this equivalence, the Hoeffding Inequality can be applied to the learning problem, allowing us to make a prediction out of $\\mathcal{D}$.  Using $\\nu$ to predict $\\mu$ tells us something about $f$, although it doesn't tell us what $f$ is.  What $\\mu$ tells us is the error rate $h$ makes in approximating $f$.  If $\\nu$ happens to be close to zero, we can predict that $h$ will approximate $f$ well over the entire input space.  If not, we are out of luck.\n",
    "\n",
    "Unfortunately, we have no control over $\\nu$ in our current situation, since $\\nu$ is based on a particular hypothesis $h$.  In real learning, we explore an entire hypothesis set $\\mathcal{H}$, looking for some $h \\in \\mathcal{H}$ that has a small error rate.  If we have only one hypothesis to begin with, we are not really learning, but rather 'verifying' whether that particular hypothesis is good or bad.  Let us see if we can extend the bin equivalence to the case where we have multiple hypotheses in order to capture real learning.\n",
    "\n",
    "To do that, we start by introducing more descriptive names for the different components that we will use.  The error rate within the sample, which corresponds to $\\nu$ in the bin model, will be called the **in-sample error**,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E_{\\text {in }}(h) & =\\text { (fraction of } \\mathcal{D} \\text { where } f \\text { and } h \\text { disagree) } \\\\\n",
    "& =\\frac{1}{N} \\sum_{n=1}^N \\llbracket h\\left(\\mathbf{x}_n\\right) \\neq f\\left(\\mathbf{x}_n\\right) \\rrbracket,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\llbracket$ statement $\\rrbracket=1$ if the statement is true, and = 0 if the statement is false.  We have made explicit the dependency of $E_{\\text{in}}$ on the particular $h$ that we are considering.   Int he same way, we define the **out-of-sample error**\n",
    "\n",
    "$$\n",
    "E_{\\text {out }}(h)=\\mathbb{P}[h(\\mathbf{x}) \\neq f(\\mathbf{x})]\n",
    "$$\n",
    "\n",
    "which corresponds to $\\mu$ in the bin model.  The probability is based on the distribution $P$ over $\\mathcal{X}$ which is used to sample the data points $x$.\n",
    "\n",
    "Substituting the new notation $E_{\\text{in}}$ for $\\nu$ and $E_{\\text{out}}$ for $\\mu$, the Hoeffding Inequality can be written as:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left[\\left|E_{\\text {in }}(h)-E_{\\text {out }}(h)\\right|>\\epsilon\\right] \\leq 2 e^{-2 \\epsilon^2 N} \\quad \\text { for any } \\epsilon>0\n",
    "$$\n",
    "\n",
    "where $N$ is the number of training examples.  The in-sample error $E_{\\text{in}}$, just like $\\nu$, is a random variable that depends on the sample.  The out-of-sample error $E_{\\text{out}}$, just lik $\\mu$, is unknown but not random.\n",
    "\n",
    "Let us consider an entire hypothesis set $\\mathcal{H}$ instead of just one hypothesis $h$, and assume for the moment that $\\mathcal{H}$ has a finite number of hypotheses:\n",
    "\n",
    "$$\\mathcal{H} = \\{h_1, h_2, \\cdots, h_M\\}$$\n",
    "\n",
    "We can construct a bin equivalent in this case by considering $M$ bins.  Each bin still represents the input space $\\mathcal{X}$, with the red marbles in the $m$th bin corresponding to the points $x \\in \\mathcal{X}$ where $h_m(x) \\ne f(x)$.  The probability of red marbles in the $m$th bin is $E_{\\text{out}}(h_m)$ and the fraction of red marbles in the $m$th sample is $E_{\\text{in}}(h_m)$, for $m = 1,\\cdots, M$.  Although the Hoeffding Inequality still applies to each bin individually, the situation becomes more complicated when we consider all the bins simultaneously.  Why is that?  The inequality stated that \n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left[\\left|E_{\\text {in }}(h)-E_{\\text {out }}(h)\\right|>\\epsilon\\right] \\leq 2 e^{-2 \\epsilon^2 N} \\quad \\text { for any } \\epsilon>0\n",
    "$$\n",
    "\n",
    "where the hypothesis $h$ is fixed before you generate the data set, and the probability is with respect to random data sets $\\mathcal{D}$; we emphasize that the assumption \"h is fixed _before_ you generate the data set\" is critical to the validity of this bound.  If you are allowed to change $h$ after you generate the data set, the assumption that are needed to prove the Hoeffding Inequality no longer hold.  With multiple hypotheses in $\\mathcal{H}$, the learning algorithm picks the final hypothesis $g$ based on $\\mathcal{D}$, i.e. _after_ generating the data set.  The statement we would like to make is not\n",
    "\n",
    "$$\n",
    "\\text { \" } \\mathbb{P}\\left[\\left|E_{\\text {in }}\\left(h_m\\right)-E_{\\text {out }}\\left(h_m\\right)\\right|>\\epsilon\\right] \\text { is small\" }\n",
    "$$\n",
    "\n",
    "(for any particular, fixed $h_m \\in \\mathcal{H}$), but rather:\n",
    "\n",
    "$$\n",
    "\\text { \"P }\\left[\\left|E_{\\text {in }}(g)-E_{\\text {out }}(g)\\right|>\\epsilon\\right] \\text { is small\" for the final hypothesis } g \\text {. }\n",
    "$$\n",
    "\n",
    "The hypothesis $g$ is _not fixed_ ahead of time before generating the data, because which hypothesis is selected to be $g$ depends on the data.  So, we cannot just plug in $g$ for $h$ in the Hoeffding inequality.\n",
    "\n",
    "[have decided to just read through the book, may come back to taking detailed notes like this]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid World\n",
    "Grid World: A 5x5 grid.\n",
    "States: Each cell in the grid is a state the agent can be in.\n",
    "Actions: The agent can move in four directions: up (0), down (1), left (2), and right (3).\n",
    "Goal: The goal is at (4, 4), providing a positive reward.\n",
    "Penalty: There's a penalty state at (3, 3) to avoid.\n",
    "Transitions: Moving off the grid keeps the agent in its current state.\n",
    "Reward: -1 for each step, +10 for reaching the goal, and -10 for entering the penalty state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "grid_size = 5\n",
    "actions = [(0, -1), (0, 1), (-1, 0), (1, 0)]  # Left, Right, Up, Down\n",
    "n_actions = len(actions)\n",
    "goal_state = (4, 4)\n",
    "penalty_state = (3, 3)\n",
    "epsilon = 0.1\n",
    "alpha = 0.5 # Learning rate\n",
    "gamma = 0.9 # Discount rate\n",
    "n_episodes = 100\n",
    "\n",
    "\n",
    "\n",
    "def step(state, action):\n",
    "    next_state = (max(0, min(grid_size - 1, state[0] + actions[action][0])),\n",
    "                  max(0, min(grid_size - 1, state[1] + actions[action][1])))\n",
    "    \n",
    "    reward = -1\n",
    "    if next_state == goal_state:\n",
    "        reward = 10\n",
    "    elif next_state == penalty_state:\n",
    "        reward = -10\n",
    "        \n",
    "    return next_state, reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-Policy Temporal Difference Control (SARSA)\n",
    "$Q(S, A) \\leftarrow Q(S, A)+\\alpha\\left[R+\\gamma Q\\left(S^{\\prime}, A^{\\prime}\\right)-Q(S, A)\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derived Policy:\n",
      "[['Right' 'Down' 'Down' 'Right' 'Down']\n",
      " ['Right' 'Down' 'Down' 'Right' 'Down']\n",
      " ['Right' 'Down' 'Down' 'Right' 'Down']\n",
      " ['Right' 'Down' 'Down' 'Right' 'Down']\n",
      " ['Right' 'Right' 'Right' 'Right' '']]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Q-table\n",
    "Q = np.zeros((grid_size, grid_size, n_actions))\n",
    "\n",
    "def choose_action(state):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, n_actions - 1)\n",
    "    else:\n",
    "        return np.argmax(Q[state[0], state[1], :])\n",
    "\n",
    "# SARSA Algorithm\n",
    "for episode in range(n_episodes):\n",
    "    # Random start\n",
    "    state = (random.randint(0, grid_size - 1), random.randint(0, grid_size - 1))\n",
    "\n",
    "    while state != goal_state:\n",
    "        action = choose_action(state)\n",
    "        next_state, reward = step(state, action)\n",
    "        next_action = choose_action(next_state)\n",
    "\n",
    "        #CONTINYA here, make sense of this..\n",
    "        # SARSA update\n",
    "        Q[state[0], state[1], action] += alpha * (reward + gamma * Q[next_state[0], next_state[1], next_action] - Q[state[0], state[1], action])\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "# Derive policy from learned Q-table\n",
    "policy = np.zeros((grid_size, grid_size), dtype=int)  # Initialize policy array\n",
    "\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        if (i,j) == goal_state:\n",
    "            continue\n",
    "        best_action = np.argmax(Q[i, j, :])  # Find the best action for this state\n",
    "        policy[i, j] = best_action  # Update the policy with the best action\n",
    "\n",
    "# Convert numeric actions in the policy to their corresponding directions for readability\n",
    "action_names = {0: 'Left', 1: 'Right', 2: 'Up', 3: 'Down'}\n",
    "policy_readable = np.vectorize(action_names.get)(policy)\n",
    "policy_readable[goal_state[0], goal_state[1]] = ''\n",
    "\n",
    "print(\"Derived Policy:\")\n",
    "print(policy_readable)\n",
    "\n",
    "#TODO: Derive actual policy from value table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-Policy Temporal Difference Control (Q-learning)\n",
    "\n",
    "$Q(S, A) \\leftarrow Q(S, A)+\\alpha\\left[R+\\gamma \\max _a Q\\left(S^{\\prime}, a\\right)-Q(S, A)\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derived Policy:\n",
      "[['Right' 'Down' 'Down' 'Down' 'Down']\n",
      " ['Right' 'Right' 'Right' 'Down' 'Left']\n",
      " ['Down' 'Down' 'Right' 'Right' 'Down']\n",
      " ['Down' 'Down' 'Down' 'Down' 'Down']\n",
      " ['Right' 'Right' 'Right' 'Right' '']]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Q-table\n",
    "Q = np.zeros((grid_size, grid_size, n_actions))\n",
    "\n",
    "def choose_action(state):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, n_actions - 1)\n",
    "    else:\n",
    "        return np.argmax(Q[state[0], state[1], :])\n",
    "\n",
    "# Q-learning Algorithm\n",
    "for episode in range(n_episodes):\n",
    "    # Random start\n",
    "    state = (random.randint(0, grid_size - 1), random.randint(0, grid_size - 1))\n",
    "    \n",
    "    while state != goal_state:\n",
    "        action = choose_action(state)\n",
    "        next_state, reward = step(state, action)\n",
    "\n",
    "        # Q-learning update\n",
    "        Q[state[0], state[1], action] += alpha * (reward + gamma * np.max(Q[next_state[0], next_state[1], :]) - Q[state[0], state[1], action])\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "# Derive policy from learned Q-table\n",
    "policy = np.zeros((grid_size, grid_size), dtype=int)  # Initialize policy array\n",
    "\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        if (i,j) == goal_state:\n",
    "            continue\n",
    "        best_action = np.argmax(Q[i, j, :])  # Find the best action for this state\n",
    "        policy[i, j] = best_action  # Update the policy with the best action\n",
    "\n",
    "# Convert numeric actions in the policy to their corresponding directions for readability\n",
    "action_names = {0: 'Left', 1: 'Right', 2: 'Up', 3: 'Down'}\n",
    "policy_readable = np.vectorize(action_names.get)(policy)\n",
    "policy_readable[goal_state[0], goal_state[1]] = ''\n",
    "\n",
    "print(\"Derived Policy:\")\n",
    "print(policy_readable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

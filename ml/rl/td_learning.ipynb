{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid World\n",
    "Grid World: A 5x5 grid.\n",
    "States: Each cell in the grid is a state the agent can be in.\n",
    "Actions: The agent can move in four directions: up (0), down (1), left (2), and right (3).\n",
    "Goal: The goal is at (4, 4), providing a positive reward.\n",
    "Penalty: There's a penalty state at (3, 3) to avoid.\n",
    "Transitions: Moving off the grid keeps the agent in its current state.\n",
    "Reward: -1 for each step, +10 for reaching the goal, and -10 for entering the penalty state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: setup Grid World here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-Policy Temporal Difference Control (SARSA)\n",
    "$Q(S, A) \\leftarrow Q(S, A)+\\alpha\\left[R+\\gamma Q\\left(S^{\\prime}, A^{\\prime}\\right)-Q(S, A)\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-Policy Temporal Difference Control (Q-learning)\n",
    "\n",
    "$Q(S, A) \\leftarrow Q(S, A)+\\alpha\\left[R+\\gamma \\max _a Q\\left(S^{\\prime}, a\\right)-Q(S, A)\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement Q-learning"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

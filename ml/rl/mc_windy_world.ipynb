{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class WindyGridworld:\n",
    "    def __init__(self, size=5, start=(4, 0), goal=(0, 4), wind_column=2):\n",
    "        self.size = size\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.wind_column = wind_column\n",
    "        self.state = start\n",
    "        self.actions = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
    "        self.action_names = ['E', 'S', 'W', 'N']\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Apply wind effect if in the wind column\n",
    "        if self.state[1] == self.wind_column:\n",
    "            action = self.apply_wind(action)\n",
    "        \n",
    "        new_state = (self.state[0] + action[0], self.state[1] + action[1])\n",
    "\n",
    "        # Enforce boundaries\n",
    "        new_state = self.check_boundaries(new_state)\n",
    "\n",
    "        # Check if goal is reached\n",
    "        done = new_state == self.goal\n",
    "        reward = 1 if done else -1\n",
    "\n",
    "        self.state = new_state\n",
    "        return new_state, reward, done\n",
    "\n",
    "    # Wind has 1/3 probability of keeping action the same, 1/3 of flipping E/W, and 1/3 of flipping N/S\n",
    "    def apply_wind(self, action):\n",
    "        # Simulate wind effect with a probability\n",
    "        wind_effects = [action, (-action[0], action[1]), (action[0], -action[1])]\n",
    "        return random.choice(wind_effects)\n",
    "\n",
    "    def check_boundaries(self, state):\n",
    "        x, y = state\n",
    "        x = min(max(x, 0), self.size - 1)\n",
    "        y = min(max(y, 0), self.size - 1)\n",
    "        return (x, y)\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start\n",
    "        return self.state    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Approach\n",
    "\n",
    "Compare to Dynamic Programming, we would use a model-free approach like Monte Carlo when we don't know the environment dynamics.  Technically I could code this more efficiently with DP, but that would involve taking advantage of knowledge of how the wind works.  We want to show that a method can learn how to deal with randomized effects like the wind.\n",
    "\n",
    "We'll use the first-visit MC approach for estimating state-values.  After generating a set of episodes, for each state, s, the set of episodes that all pass through state s is considered for calculating the value of state s. MC updates for estimating the valuefunction are based on the total return obtained in that episode starting from the first time that state s is visited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env):\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = random.choice(env.actions)  # Simple random policy\n",
    "        next_state, reward, done = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return episode\n",
    "\n",
    "def first_visit_mc(env, num_episodes, gamma=0.99):\n",
    "    # Initialize state values and returns\n",
    "    value_table = np.zeros((env.size, env.size))\n",
    "    \n",
    "    \n",
    "    returns = {}\n",
    "    for x in range(env.size):\n",
    "        for y in range(env.size):\n",
    "            # Initialize the list of returns for this state as empty\n",
    "            returns[(x, y)] = []\n",
    "\n",
    "\n",
    "    # CONTINYA: UNDERSTAND calculation of returns/ value_table.\n",
    "    for _ in range(num_episodes):\n",
    "        episode = generate_episode(env)\n",
    "        G = 0\n",
    "        # The return of a given state is the total return from the time the state has been visited.\n",
    "        # So looping in reverse, we can keep building up the reward with discounting.  We can check if the current state is the first instance of that state.  If so, we can store the return in the returns dictionary.\n",
    "        # the value_table keeps a running average of the returns for a given state.\n",
    "\n",
    "        for i in reversed(range(len(episode))):\n",
    "            state, _, reward = episode[i]\n",
    "            G = gamma * G + reward\n",
    "            # checks if the current state at index i is the first occurrence of that state in the episode\n",
    "            if state not in [x[0] for x in episode[:i]]:\n",
    "                returns[state].append(G)\n",
    "                value_table[state[0], state[1]] = np.mean(returns[state])\n",
    "\n",
    "    return value_table\n",
    "\n",
    "def derive_policy_from_value_table(env, value_table):\n",
    "    policy = np.zeros((env.size, env.size), dtype='U1')\n",
    "\n",
    "    for x in range(env.size):\n",
    "        for y in range(env.size):\n",
    "            if (x,y) == env.goal:\n",
    "                continue\n",
    "            best_value = -float('inf')\n",
    "            best_action = None\n",
    "            for action_index, (dx, dy) in enumerate(env.actions):\n",
    "                next_x, next_y = x + dx, y + dy\n",
    "                # Check if next state is within grid boundaries\n",
    "                if 0 <= next_x < env.size and 0 <= next_y < env.size:\n",
    "                    value = value_table[next_x, next_y]\n",
    "                    if value > best_value:\n",
    "                        best_value = value\n",
    "                        best_action = action_index\n",
    "            policy[x, y] = env.action_names[best_action]\n",
    "    return policy\n",
    "\n",
    "\n",
    "env = WindyGridworld()\n",
    "value_table = first_visit_mc(env, 100)\n",
    "print(value_table)\n",
    "policy = derive_policy_from_value_table(env, value_table)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory start\n",
    "\n",
    "If we wanted to have it start a random position while generating episodes (exploratory start method), we'd just adjust the episode generator as follows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_with_exploratory_start(env):\n",
    "    episode = []\n",
    "    # Choose a random state from the grid as the start state, specifically excluding terminal states\n",
    "    start_states = [(x, y) for x in range(env.size) for y in range(env.size) if (x, y) not in env.terminal_states]\n",
    "    state = random.choice(start_states)\n",
    "    env.state = state  # Manually set the environment's state to the chosen start state\n",
    "\n",
    "    while True:\n",
    "        action = random.choice(env.actions)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-greedy policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['E' 'E' 'E' 'E' '']\n",
      " ['E' 'E' 'E' 'E' 'N']\n",
      " ['E' 'E' 'E' 'N' 'N']\n",
      " ['N' 'N' 'E' 'N' 'N']\n",
      " ['N' 'N' 'E' 'N' 'N']]\n"
     ]
    }
   ],
   "source": [
    "#TODO: understand why generate_episode isn't completing\n",
    "def generate_episode(env, value_table, epsilon):\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = epsilon_greedy_policy(state, value_table, epsilon, env)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return episode\n",
    "\n",
    "def first_visit_mc(env, num_episodes, gamma=0.99, epsilon=0.5):\n",
    "    value_table = np.zeros((env.size, env.size))\n",
    "\n",
    "    returns = {}\n",
    "    for x in range(env.size):\n",
    "        for y in range(env.size):\n",
    "            # Initialize the list of returns for this state as empty\n",
    "            returns[(x, y)] = []\n",
    "\n",
    "    for ep_num in range(num_episodes):\n",
    "        episode = generate_episode(env, value_table, epsilon)\n",
    "        G = 0\n",
    "        for i in reversed(range(len(episode))):\n",
    "            state, _, reward = episode[i]\n",
    "            G = gamma * G + reward\n",
    "            if state not in [x[0] for x in episode[:i]]:\n",
    "                returns[state].append(G)\n",
    "                value_table[state[0], state[1]] = np.mean(returns[state])\n",
    "    \n",
    "    return value_table\n",
    "\n",
    "# CONTINYA: the issue is that it keeps going back and forth between\n",
    "def epsilon_greedy_policy(state, value_table, epsilon, env):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return random.choice(env.actions)  # Explore: choose a random action\n",
    "    else:\n",
    "        # Exploit: choose the best action based on the current value_table\n",
    "        return derive_action_from_value_table(state, value_table, env)\n",
    "\n",
    "def derive_action_from_value_table(state, value_table, env):\n",
    "    best_value = -float('inf')\n",
    "    best_actions = []\n",
    "    for action in env.actions:\n",
    "        next_x, next_y = (state[0] + action[0] , state[1] + action[1])\n",
    "        if 0 <= next_x < env.size and 0 <= next_y < env.size:\n",
    "            # Only consider actions that move within the grid\n",
    "            value = value_table[next_x, next_y]\n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_actions = [action]\n",
    "            elif value == best_value:\n",
    "                best_actions.append(action)\n",
    "\n",
    "    return random.choice(best_actions)\n",
    "\n",
    "def action_to_action_name(action):\n",
    "    if (action == (0,1)):\n",
    "        return 'E'\n",
    "    if (action == (0,-1)):\n",
    "        return 'W'\n",
    "    if (action == (1,0)):\n",
    "        return 'S'\n",
    "    if (action == (-1,0)):\n",
    "        return 'N'\n",
    "\n",
    "def derive_policy_from_value_table(env, value_table):\n",
    "    policy = np.zeros((env.size, env.size), dtype='U1')\n",
    "\n",
    "    for x in range(env.size):\n",
    "        for y in range(env.size):\n",
    "            state = (x,y)\n",
    "            if state == env.goal:\n",
    "                continue\n",
    "            best_action = derive_action_from_value_table(state, value_table, env)\n",
    "        \n",
    "            policy[x, y] = action_to_action_name(best_action)\n",
    "    return policy\n",
    "\n",
    "env = WindyGridworld()\n",
    "value_table = first_visit_mc(env, 1000)\n",
    "policy = derive_policy_from_value_table(env, value_table)\n",
    "print(policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

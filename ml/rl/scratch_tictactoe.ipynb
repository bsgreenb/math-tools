{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "import random\n",
    "\n",
    "# TODO: use gymnasium for tictactoe, replace it here\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.column_count = 3\n",
    "        self.action_size = self.row_count * self.column_count\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"TicTacToe\"\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "\n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        state[row, column] = player\n",
    "        return state\n",
    "\n",
    "    def get_valid_moves(self, state):\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "\n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            # Handle root node edge case\n",
    "            return False\n",
    "\n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        player = state[row, column]\n",
    "\n",
    "        return (\n",
    "            np.sum(state[row, :]) == player * self.column_count\n",
    "            or np.sum(state[:, column]) == player * self.row_count\n",
    "            or np.sum(np.diag(state)) == player * self.row_count\n",
    "            or np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        else:\n",
    "            return 0, False\n",
    "\n",
    "    def get_opponent(self, player):\n",
    "        return -player \n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "\n",
    "    def change_perspective(self, state, player=-1):\n",
    "        return state * player\n",
    "\n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        if len(state.shape) == 3:\n",
    "            # Dealing with batch\n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectFour:\n",
    "    def __init__(self):\n",
    "        self.row_count = 6\n",
    "        self.column_count = 7\n",
    "        self.action_size = self.column_count\n",
    "        self.in_a_row = 4\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ConnectFour\"\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "\n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = np.max(np.where(state[:, action] == 0))\n",
    "        state[row, action] = player\n",
    "        return state\n",
    "\n",
    "    def get_valid_moves(self, state):\n",
    "        return (state[0] == 0).astype(np.uint8)\n",
    "\n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            # Handle root node edge case\n",
    "            return False\n",
    "\n",
    "        row = np.min(np.where(state[:, action] != 0))\n",
    "        column = action\n",
    "        player = state[row][column]\n",
    "\n",
    "        def count(offset_row, offset_column):\n",
    "            for i in range(1, self.in_a_row):\n",
    "                r = row + offset_row * i\n",
    "                c = action + offset_column * i\n",
    "                if (\n",
    "                    r < 0\n",
    "                    or r >= self.row_count\n",
    "                    or c < 0\n",
    "                    or c >= self.column_count\n",
    "                    or state[r][c] != player\n",
    "                ):\n",
    "                    return i - 1\n",
    "            return self.in_a_row - 1 \n",
    "\n",
    "        return (\n",
    "            count(1, 0) >= self.in_a_row - 1 # vertical\n",
    "            or (count(0, 1) + count(0, -1)) >= self.in_a_row - 1 # horizontal\n",
    "            or (count(1, 1) + count(-1, -1)) >= self.in_a_row - 1 # top left diagonal\n",
    "            or (count(1, -1) + count(-1, 1)) >= self.in_a_row - 1 # top right diagonal\n",
    "        )\n",
    "\n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        else:\n",
    "            return 0, False\n",
    "\n",
    "    def get_opponent(self, player):\n",
    "        return -player \n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "\n",
    "    def change_perspective(self, state, player=-1):\n",
    "        return state * player\n",
    "\n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        if len(state.shape) == 3:\n",
    "            # Dealing with batch\n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlocks, num_filters, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        feature_planes = 3 # This will differ once we move beyond tic tac toe and connect four into chess\n",
    "        board_size = game.row_count * game.column_count\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(feature_planes, num_filters, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_filters) for i in range(num_resBlocks)]\n",
    "        )\n",
    "\n",
    "        # Note, I changed the output channels on policy head from 32->2 compared to the code.  Also removed padding.\n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 2, kernel_size=1),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2 * board_size, game.action_size)\n",
    "        )\n",
    "\n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 1, kernel_size=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(board_size, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_filters):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_filters)\n",
    "        self.conv2 = nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.04872504994273186\n",
      "[[ 0.  0. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 1.  0.  1.]]\n",
      "tensor([[[[0., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 0., 1.],\n",
      "          [0., 1., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [1., 0., 1.]]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tictactoe = TicTacToe()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "state = tictactoe.get_next_state(state, 2, -1)\n",
    "state = tictactoe.get_next_state(state, 4, -1)\n",
    "state = tictactoe.get_next_state(state, 6, 1)\n",
    "state = tictactoe.get_next_state(state, 8, 1)\n",
    "\n",
    "encoded_state = tictactoe.get_encoded_state(state)\n",
    "\n",
    "tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64, device=device)\n",
    "model.load_state_dict(torch.load('model_2.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value)\n",
    "\n",
    "print(state)\n",
    "print(tensor_state)\n",
    "\n",
    "plt.bar(range(tictactoe.action_size), policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "\n",
    "        self.children = []\n",
    "\n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "\n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "\n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "        \n",
    "        return best_child\n",
    "\n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            # We want minimal q_value for child, because it's our opponent.\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "\n",
    "    def expand(self, policy):\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1) # The board is always from perspective of P1 moving\n",
    "                child_state = self.game.change_perspective(child_state)\n",
    "\n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "\n",
    "        return child\n",
    "    \n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "\n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None: # True outside root node\n",
    "            self.parent.backpropagate(value)\n",
    "\n",
    "class MCTSParallel:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "    \n",
    "    @torch.no_grad() # Don't use MCTS for training neural network parameters\n",
    "    def search(self, states, spGames):\n",
    "        # Increased temp for start policy\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "        # Dirichlet noise\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size, size=policy.shape[0])\n",
    "        \n",
    "        for i, spg in enumerate(spGames):\n",
    "            spg_policy = policy[i]\n",
    "            valid_moves = self.game.get_valid_moves(states[i])\n",
    "            spg_policy *= valid_moves\n",
    "            spg_policy /= np.sum(spg_policy)\n",
    "\n",
    "            spg.root = Node(self.game, self.args, states[i], visit_count=1)\n",
    "            spg.root.expand(spg_policy)\n",
    "\n",
    "        for _ in range(self.args['num_searches']):\n",
    "            for spg in spGames:\n",
    "                spg.node = None\n",
    "                node = spg.root\n",
    "\n",
    "                # Selection\n",
    "                while node.is_fully_expanded():\n",
    "                    node = node.select()\n",
    "                \n",
    "                value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "                value = self.game.get_opponent_value(value) # The value is from the perspective of the opponent of the person who made the move.\n",
    "\n",
    "                if is_terminal:\n",
    "                    # Back Propagation\n",
    "                    node.backpropagate(value)\n",
    "                else:\n",
    "                    spg.node = node\n",
    "\n",
    "            expandable_spGames = [mappingIdx for mappingIdx in range(len(spGames)) if spGames[mappingIdx].node is not None]\n",
    "\n",
    "            if len(expandable_spGames) > 0:\n",
    "                states = np.stack([spGames[mappingIdx].node.state for mappingIdx in expandable_spGames])\n",
    "                \n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "\n",
    "            for i, mappingIdx in enumerate(expandable_spGames):\n",
    "                node = spGames[mappingIdx].node\n",
    "                spg_policy, spg_value = policy[i], value[i]\n",
    "\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                spg_policy *= valid_moves # only consider legal moves\n",
    "                spg_policy /= np.sum(spg_policy)\n",
    "\n",
    "                # Expansion\n",
    "                node.expand(spg_policy)\n",
    "\n",
    "                node.backpropagate(spg_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroParallel:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTSParallel(game, args, model)\n",
    "    \n",
    "    def selfPlay(self):\n",
    "        return_memory = []\n",
    "        player = 1\n",
    "        spGames = [SelfPlayGame(self.game) for spg in range(self.args['num_parallel_games'])]\n",
    "\n",
    "        while len(spGames) > 0:\n",
    "            states = np.stack([spg.state for spg in spGames])\n",
    "\n",
    "            neutral_states = self.game.change_perspective(states, player)\n",
    "            self.mcts.search(neutral_states, spGames)\n",
    "\n",
    "            for i in range(len(spGames))[::-1]:\n",
    "                spg = spGames[i]\n",
    "\n",
    "                action_probs = np.zeros(self.game.action_size)\n",
    "                for child in spg.root.children:\n",
    "                    action_probs[child.action_taken] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "\n",
    "                spg.memory.append((spg.root.state, action_probs, player))\n",
    "\n",
    "                # Introduce temperature into probs\n",
    "                temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "                temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "                action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
    "\n",
    "                spg.state = self.game.get_next_state(spg.state, action, player)\n",
    "\n",
    "                value, is_terminal = self.game.get_value_and_terminated(spg.state, action)\n",
    "\n",
    "                if is_terminal:\n",
    "                    for hist_neutral_state, hist_action_probs, hist_player in spg.memory:\n",
    "                        hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                        return_memory.append((\n",
    "                            self.game.get_encoded_state(hist_neutral_state),\n",
    "                            hist_action_probs,\n",
    "                            hist_outcome\n",
    "                        ))\n",
    "                    del spGames[i]\n",
    "            \n",
    "            player = self.game.get_opponent(player)\n",
    "\n",
    "            return return_memory\n",
    "\n",
    "    def train(self, memory):\n",
    "        # Want to shuffle training data to avoid getting same batches all the time\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) -1,batchIdx + self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "\n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "\n",
    "            out_policy, out_value = self.model(state)\n",
    "\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    # Main method, runs self-play, and uses that data for training.\n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "\n",
    "            self.model.eval()\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations'] // self.args['num_parallel_games']):\n",
    "                memory += self.selfPlay()\n",
    "\n",
    "            self.model.train()\n",
    "            for epoch in range(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "\n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")\n",
    "\n",
    "class SelfPlayGame:\n",
    "    def __init__(self, game):\n",
    "        self.state = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.root = None\n",
    "        self.node = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFour()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(game, 9, 128, device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 600,\n",
    "    'num_iterations': 8,\n",
    "    'num_selfPlay_iterations': 500,\n",
    "    'num_parallel_games': 100,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 128,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZeroParallel(model, optimizer, game, args)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "valid moves [0, 1, 2, 3, 4, 5, 6]\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0. -1.  0.  0.]]\n",
      "valid moves [0, 1, 2, 3, 4, 5, 6]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0. -1.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0.  0.  0. -1.  0.  0.]]\n",
      "valid moves [0, 1, 2, 3, 4, 5, 6]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0.  0.  0. -1.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0.  0.  0. -1.  0.  0.]]\n",
      "valid moves [0, 1, 2, 3, 4, 5, 6]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0.  0.  0. -1.  0.  0.]]\n",
      "1 won\n"
     ]
    }
   ],
   "source": [
    "game = ConnectFour()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 20,\n",
    "    'dirichlet_epsilon': 0,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(game, 9, 128, device)\n",
    "model.load_state_dict(torch.load(\"model_0_ConnectFour.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTSParallel(game, args, model)\n",
    "\n",
    "state = game.get_initial_state()\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "\n",
    "    if player == 1:\n",
    "        valid_moves = game.get_valid_moves(state)\n",
    "        print(\"valid moves\", [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{player}:\"))\n",
    "\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not valid\")\n",
    "            continue\n",
    "    else:\n",
    "        neutral_state = game.change_perspective(state)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "\n",
    "    state = game.get_next_state(state, action, player)\n",
    "\n",
    "    value, is_terminal = game.get_value_and_terminated(state, action)\n",
    "\n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, \"won\")\n",
    "        else:\n",
    "            print(\"draw\")\n",
    "        break\n",
    "\n",
    "    player = game.get_opponent(player)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

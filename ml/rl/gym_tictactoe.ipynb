{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up pure human self-play with Gymnasium \n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# see https://stackoverflow.com/a/78030262/378622 for why we need a custom method for this\n",
    "def display_board(obs, agent):\n",
    "    rvl = obs.ravel()\n",
    "    arr = np.empty(rvl.shape, int)\n",
    "    arr[::2] = 1\n",
    "    arr[1::2] = 2\n",
    "    rvl *= arr\n",
    "\n",
    "    grp_x = np.array(rvl[::2]).reshape(3, 3).T\n",
    "    grp_o = np.array(rvl[1::2]).reshape(3, 3).T\n",
    "    res = grp_x + grp_o\n",
    "\n",
    "    if agent == 'player_1':\n",
    "        dct = {1: \"X\", 2: \"O\", 0: \" \"}\n",
    "    else:\n",
    "        dct = {1: \"O\", 2: \"X\", 0: \" \"}\n",
    "    print(np.vectorize(dct.get)(res), flush=True)\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=None)\n",
    "env.reset(seed=1)\n",
    "\n",
    "env = copy.deepcopy(env)\n",
    "env.step(0)\n",
    "\n",
    "opposing_agent = None\n",
    "for agent in env.agent_iter():\n",
    "    \n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    display_board(observation['observation'], agent)\n",
    "\n",
    "    if termination:\n",
    "        if reward in [1,-1]:\n",
    "            print(opposing_agent, \"won\", flush=True)\n",
    "        else:\n",
    "            print(\"draw\", flush=True)\n",
    "        break\n",
    "\n",
    "    valid_moves = observation[\"action_mask\"]\n",
    "    \n",
    "    while True:\n",
    "        print(\"valid_moves\", [i for i in range(len(valid_moves)) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{agent}:\"))\n",
    "\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not valid\", flush=True)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    env.step(action)   \n",
    "    opposing_agent = agent\n",
    "    \n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to have it be human vs computer\n",
    "env = tictactoe_v3.env(render_mode=None)\n",
    "env.reset(seed=1) # state stored internally\n",
    "\n",
    "args = {\n",
    "    'C': 1.41,\n",
    "    'num_searches': 1000\n",
    "}\n",
    "\n",
    "mcts = MCTS(env, args)\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    display_board(observation['observation'], agent)\n",
    "\n",
    "    if termination:\n",
    "        if reward in [1,-1]:\n",
    "            print(opposing_agent, \"won\", flush=True)\n",
    "        else:\n",
    "            print(\"draw\", flush=True)\n",
    "        break\n",
    "\n",
    "    if agent == 'player_1':\n",
    "        valid_moves = observation[\"action_mask\"]\n",
    "        while True:\n",
    "            print(\"valid_moves\", [i for i in range(len(valid_moves)) if valid_moves[i] == 1])\n",
    "            action = int(input(f\"{agent}:\"))\n",
    "\n",
    "            if valid_moves[action] == 0:\n",
    "                print(\"action not valid\", flush=True)\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "    else:\n",
    "        # computer move\n",
    "        #CONTINYA here..\n",
    "            # We will want a GymMCTS class, GymNode class. \n",
    "            # Use deep copy when doing state transitions, I'm thinking. copy.deepcopy()    \n",
    "\n",
    "\n",
    "    opposing_agent = agent\n",
    "\n",
    "env.close()\n",
    "    \n",
    "\n",
    "# TODO: setup MCTS to actually work, lots of self.game stuff to update\n",
    "    # replace game.get_valid_moves with "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

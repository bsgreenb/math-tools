{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "$$\n",
    "s=\\sum_{i=0}^d w_i x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "h(\\mathbf{x})=\\theta(s)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/logistic-regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic function $\\theta$:\n",
    "\n",
    "$$\n",
    "\\theta(s)=\\frac{e^s}{1+e^s}\n",
    "$$\n",
    "\n",
    "It ranges from 0 to 1.  Called \"soft threshold\" or \"sigmoid\" (cus it looks like an S).\n",
    "\n",
    "# Target function\n",
    "\n",
    "Data $(x, y)$ with binary $y$, generated by a noisy target:\n",
    "\n",
    "$$\n",
    "P(y \\mid \\mathbf{x})= \\begin{cases}f(\\mathbf{x}) & \\text { for } y=+1 \\\\ 1-f(\\mathbf{x}) & \\text { for } y=-1\\end{cases}\n",
    "$$\n",
    "\n",
    "The target $f: \\mathbb{R}^d \\rightarrow[0,1]$ is the probability.\n",
    "\n",
    "We will learn $g(\\mathbf{x})=\\theta\\left(\\mathbf{w}^{\\top} \\mathbf{x}\\right) \\approx f(\\mathbf{x})$.\n",
    "\n",
    "# Error measure\n",
    "\n",
    "Plausible error based on likelihood: If $h = f$, how likely to get $y$ from $x$?\n",
    "\n",
    "$$\n",
    "P(y \\mid \\mathbf{x})= \\begin{cases}h(\\mathbf{x}) & \\text { for } y=+1 \\\\ 1-h(\\mathbf{x}) & \\text { for } y=-1\\end{cases}\n",
    "$$\n",
    "\n",
    "Substitute $h(\\mathbf{x})=\\theta\\left(\\mathbf{w}^{\\top} \\mathbf{x}\\right)$, noting $\\theta(-s)=1-\\theta(s)$ we get:\n",
    "\n",
    "$$\n",
    "P(y \\mid \\mathbf{x})=\\theta\\left(y \\mathbf{w}^{\\top} \\mathbf{x}\\right)\n",
    "$$\n",
    "\n",
    "So if $h = f$, likelihood of entire data set $\\mathcal{D}$ is given by:\n",
    "\n",
    "$$\n",
    "\\prod_{n=1}^N P\\left(y_n \\mid \\mathbf{x}_n\\right)=\\prod_{n=1}^N \\theta\\left(y_n \\mathbf{w}^{\\top} \\mathbf{x}_n\\right)\n",
    "$$\n",
    "\n",
    "Maximizing this likelihood is the same as maximizing:\n",
    "\n",
    "$$\n",
    "\\frac{1}{N} \\ln \\left(\\prod_{n=1}^N \\theta\\left(y_n \\mathbf{w}^{\\top} \\mathbf{x}_n\\right)\\right)\n",
    "$$\n",
    "\n",
    "Which is the same as minimizing the negative of it:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& -\\frac{1}{N} \\ln \\left(\\prod_{n=1}^N \\theta\\left(y_n \\mathbf{w}^{\\top} \\mathbf{x}_n\\right)\\right) \\\\\n",
    "& =\\frac{1}{N} \\sum_{n=1}^N \\ln \\left(\\frac{1}{\\theta\\left(y_n \\mathbf{w}^{\\top} \\mathbf{x}_n\\right)}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Substituting $\\theta(s)=\\frac{1}{1+e^{-s}}$, we get our \"cross-entropy error\":\n",
    "\n",
    "$$\n",
    "E_{\\text {in }}(\\mathbf{w})=\\frac{1}{N} \\sum_{n=1}^N \\underbrace{\\ln \\left(1+e^{-y_n \\mathbf{w}^{\\top} \\mathbf{x}_n}\\right)}_{\\mathrm{e}\\left(h\\left(\\mathbf{x}_n\\right), y_n\\right)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Learning Algorithm: Gradient Descent\n",
    "\n",
    "$$\n",
    "\\Delta E_{\\text {in }}=E_{\\text {in }}(\\mathbf{w}(0)+\\eta \\hat{\\mathbf{v}})-E_{\\text {in }}(\\mathbf{w}(0))\n",
    "$$\n",
    "\n",
    "This equals the derivative times the difference:\n",
    "\n",
    "$$\n",
    "=\\eta \\nabla E_{\\text {in }}(\\mathbf{w}(0))^{\\mathrm{T}} \\hat{\\mathbf{v}}+O\\left(\\eta^2\\right)\n",
    "$$\n",
    "\n",
    "We will neglect the 2nd order term.  The least we can get is the negative of the norm, if $\\hat{v}$ goes in opposite direction of the gradient:\n",
    "\n",
    "$$\n",
    "\\geq-\\eta\\left\\|\\nabla E_{\\text {in }}(\\mathbf{w}(0))\\right\\|\n",
    "$$\n",
    "\n",
    "So if $\\hat{v}$ is a unit vector going in the opposite direction of the gradient, getting us the lowest possible value for $\\Delta E_{\\text {in }}$ which we desire:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{v}}=-\\frac{\\nabla E_{\\text {in }}(\\mathbf{w}(0))}{\\left\\|\\nabla E_{\\text {in }}(\\mathbf{w}(0))\\right\\|}\n",
    "$$\n",
    "\n",
    "We want to scale our step size to the steepness of the gradient.  We can do this by having our $\\hat{v}$ not be a unit vector, by removing the denominator, so it auto-scales with the gradient:\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{w}=-\\eta \\nabla E_{\\text {in }}(\\mathbf{w}(0))\n",
    "$$\n",
    "\n",
    "The gradient is composed of our partials of the error function $e(h(\\mathbf{x_n}), y_n)=\\ln \\left(1+e^{-y_n \\mathbf{w}^T \\mathbf{x_n}}\\right)$ with regard to each of the weights:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial e(h(x_n), y_n)}{\\partial w_k}=\\frac{1}{1+e^{-y_n w^T \\mathbf{x_n}}} \\cdot e^{-y_n \\mathbf{w}^T \\mathbf{x_n}} \\cdot\\left(-y_n x_{nk}\\right)=-\\frac{y_n \\cdot x_{nk}}{e^{y_n w^T \\mathbf{x_n}}+1}\n",
    "$$\n",
    "\n",
    "The entire gradient for all the weights is given by:\n",
    "\n",
    "$$\n",
    "\\nabla e = \\frac{-y_n \\mathbf{x_n}}{e^{y_n \\mathbf{w}^T \\mathbf{x_n}}+1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10293439143927566, 344.04)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Problems 8-9 on https://work.caltech.edu/homework/hw5.pdf\n",
    "# https://nbviewer.org/github/homefish/edX_Learning_From_Data_2017/blob/master/homework_5/hw5_p8_9_logistic_regression.ipynb is helpful\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "RUNS = 100\n",
    "TRAINING_N = 100\n",
    "TESTING_N = 1000\n",
    "ETA = 0.01\n",
    "\n",
    "\n",
    "def runs():\n",
    "    epoch_total = 0\n",
    "    e_out_total = 0\n",
    "\n",
    "    for run in range(RUNS):\n",
    "        slope, intercept = generate_line()\n",
    "        xs = np.random.uniform(-1,1, (TRAINING_N, 2))\n",
    "        line_heights = (slope * xs[:, 0]) + intercept\n",
    "        ys = np.where(xs[:, 1] > line_heights, 1, -1)\n",
    "        xs = np.hstack((np.ones((TRAINING_N,1)), xs)) # Set x_0 = 1 for all xs\n",
    "        \n",
    "        weights = np.zeros(3)\n",
    "        \n",
    "        while True:\n",
    "            # Permutation of N points\n",
    "            input_perm = np.random.permutation(len(xs))\n",
    "            old_weights = weights\n",
    "\n",
    "            for input_id in input_perm:\n",
    "                x_n = xs[input_id]\n",
    "                y_n = ys[input_id]\n",
    "                gradient = (-y_n * x_n) / (1 + math.exp(y_n * np.dot(weights.T, x_n)))\n",
    "\n",
    "                weights = weights - (ETA * gradient)\n",
    "\n",
    "            epoch_total +=1\n",
    "\n",
    "            if np.linalg.norm(weights - old_weights) < 0.01:\n",
    "                break\n",
    "        \n",
    "        # Generate 1000 points to test E-out\n",
    "        xs = np.random.uniform(-1,1, (TESTING_N, 2))\n",
    "        line_heights = (slope * xs[:, 0]) + intercept\n",
    "        ys = np.where(xs[:, 1] > line_heights, 1, -1)\n",
    "        xs = np.hstack((np.ones((TESTING_N,1)), xs)) # Set x_0 = 1 for all xs\n",
    "\n",
    "        # e_out is the cross_entropy error\n",
    "        e_out = 0\n",
    "        for i in range(TESTING_N):\n",
    "            x_n = xs[i]\n",
    "            y_n = ys[i]\n",
    "            e_out += math.log(1 + math.exp(-y_n * np.dot(weights.T, x_n)))\n",
    "\n",
    "        e_out = e_out / TESTING_N\n",
    "        e_out_total += e_out\n",
    "    \n",
    "    e_out_avg = e_out_total / RUNS\n",
    "    epoch_avg = epoch_total / RUNS\n",
    "\n",
    "    return (e_out_avg, epoch_avg)\n",
    "\n",
    "def generate_line():\n",
    "    rng = np.random.default_rng()\n",
    "    point_1, point_2 = rng.uniform(-1,1, (2,2))\n",
    "    return slope_and_intercept(point_1, point_2)\n",
    "\n",
    "def slope_and_intercept(point_1, point_2): \n",
    "    slope = (point_2[1] - point_1[1]) / (point_2[0] - point_1[0])\n",
    "    intercept = point_1[1] - (slope * point_1[0])\n",
    "    return slope, intercept\n",
    "\n",
    "runs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

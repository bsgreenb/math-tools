{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb.train: \n",
    "    # early_stopping_rounds = 10\n",
    "        # \"Early stopping can be enabled by the parameter early_stopping_rounds. Early stopping can help prevent overfitting and save time during training.\"\n",
    "    # num_boost_round = 1000\n",
    "    # params\n",
    "#xgb.XGBClassifier\n",
    "# XGBClassifier follows the scikit-learn classifier API\n",
    "    # QSTN: are params the same as xgb train? I see early_stopping_rounds\n",
    "    # tree_method=\"hist\" (QSTN: when to use hist or other methods?)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#QSTN: diff classes\n",
    "#QSTN: setting the right parameters\n",
    "# TODO: handle categorical data\n",
    "\n",
    "#TODO: make sure to account for categorical data correctly.\n",
    "\n",
    "# https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html\n",
    "# There are in general two ways that you can control overfitting in XGBoost:\n",
    "    # The first way is to directly control model complexity. This includes max_depth, min_child_weight and gamma.\n",
    "    # The second way is to add randomness to make training robust to noise. This includes subsample and colsample_bytree. You can also reduce stepsize eta. Remember to increase num_round when you do so.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  With the sklearn estimator interface, we can train a classification model with only a couple lines of Python code. Here’s an example for training a classification model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.45997\n",
      "[1]\tvalidation_0-logloss:0.34184\n",
      "[2]\tvalidation_0-logloss:0.27076\n",
      "[3]\tvalidation_0-logloss:0.22399\n",
      "[4]\tvalidation_0-logloss:0.19346\n",
      "[5]\tvalidation_0-logloss:0.16814\n",
      "[6]\tvalidation_0-logloss:0.15393\n",
      "[7]\tvalidation_0-logloss:0.14081\n",
      "[8]\tvalidation_0-logloss:0.13269\n",
      "[9]\tvalidation_0-logloss:0.12515\n",
      "[10]\tvalidation_0-logloss:0.11551\n",
      "[11]\tvalidation_0-logloss:0.11184\n",
      "[12]\tvalidation_0-logloss:0.10799\n",
      "[13]\tvalidation_0-logloss:0.10541\n",
      "[14]\tvalidation_0-logloss:0.10493\n",
      "[15]\tvalidation_0-logloss:0.10326\n",
      "[16]\tvalidation_0-logloss:0.10300\n",
      "[17]\tvalidation_0-logloss:0.10339\n",
      "[18]\tvalidation_0-logloss:0.10160\n",
      "[19]\tvalidation_0-logloss:0.09892\n",
      "[20]\tvalidation_0-logloss:0.09478\n",
      "[21]\tvalidation_0-logloss:0.09359\n",
      "[22]\tvalidation_0-logloss:0.09247\n",
      "[23]\tvalidation_0-logloss:0.09284\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=94)\n",
    "\n",
    "# Use \"hist\" for constructing the trees, with early stopping enabled.\n",
    "clf = xgb.XGBClassifier(tree_method=\"hist\", early_stopping_rounds=2)\n",
    "# Fit the model, test sets are used for early stopping.\n",
    "clf.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "# At present, XGBoost doesn’t implement data spliting logic within the estimator and relies on the eval_set parameter of the xgboost.XGBModel.fit() method. If you want to use early stopping to prevent overfitting, you’ll need to manually split your data into training and testing sets using the sklearn.model_selection.train_test_split() function from the sklearn library.\n",
    "\n",
    "# Save model into JSON format.\n",
    "clf.save_model(\"clf.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some other machine learning algorithms, like those in sklearn, include early stopping as part of the estimator and may work with cross validation. However, using early stopping during cross validation may not be a perfect approach because it changes the model’s number of trees for each validation fold, leading to different model. A better approach is to retrain the model after cross validation using the best hyperparameters along with early stopping. If you want to experiment with idea of using cross validation with early stopping, here is a snippet to begin with:\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "\n",
    "def fit_and_score(estimator, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Fit the estimator on the train set and score it on both sets\"\"\"\n",
    "    estimator.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "\n",
    "    train_score = estimator.score(X_train, y_train)\n",
    "    test_score = estimator.score(X_test, y_test)\n",
    "\n",
    "    return estimator, train_score, test_score\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=94)\n",
    "\n",
    "clf = xgb.XGBClassifier(tree_method=\"hist\", early_stopping_rounds=3)\n",
    "\n",
    "resutls = {}\n",
    "\n",
    "for train, test in cv.split(X, y):\n",
    "    X_train = X[train]\n",
    "    X_test = X[test]\n",
    "    y_train = y[train]\n",
    "    y_test = y[test]\n",
    "    est, train_score, test_score = fit_and_score(\n",
    "        clone(clf), X_train, X_test, y_train, y_test\n",
    "    )\n",
    "    resutls[est] = (train_score, test_score)\n",
    "\n",
    "# n-jobs and threads:\n",
    "# If both XGBoost and sklearn are set to use all threads, your computer may start to slow down significantly due to something called “thread thrashing”. To avoid this, you can simply set the n_jobs parameter for XGBoost to None (which uses all threads) and the n_jobs parameter for sklearn to 1. This way, both programs will be able to work together smoothly without causing any unnecessary computer strain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeros and Ones from the Digits dataset: binary classification\n",
      "[[87  0]\n",
      " [ 1 92]]\n",
      "[[91  0]\n",
      " [ 3 86]]\n",
      "Iris: multiclass classification\n",
      "[[19  0  0]\n",
      " [ 1 31  2]\n",
      " [ 0  3 19]]\n",
      "[[31  0  0]\n",
      " [ 0 16  0]\n",
      " [ 0  1 27]]\n",
      "California Housing: regression\n",
      "0.2310805601814026\n",
      "0.23533764411819863\n",
      "Parameter optimization\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "0.6838972456687431\n",
      "{'max_depth': 4, 'n_estimators': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [15:15:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"param\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08224154667269369\n"
     ]
    }
   ],
   "source": [
    "# https://xgboost.readthedocs.io/en/stable/python/examples/sklearn_examples.html\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing, load_digits, load_iris\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "rng = np.random.RandomState(31337)\n",
    "\n",
    "print(\"Zeros and Ones from the Digits dataset: binary classification\")\n",
    "digits = load_digits(n_class=2)\n",
    "y = digits['target']\n",
    "X = digits['data']\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=rng)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    xgb_model = xgb.XGBClassifier(n_jobs=1).fit(X[train_index], y[train_index])\n",
    "    predictions = xgb_model.predict(X[test_index])\n",
    "    actuals = y[test_index]\n",
    "    print(confusion_matrix(actuals, predictions))\n",
    "\n",
    "print(\"Iris: multiclass classification\")\n",
    "iris = load_iris()\n",
    "y = iris['target']\n",
    "X = iris['data']\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=rng)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    xgb_model = xgb.XGBClassifier(n_jobs=1).fit(X[train_index], y[train_index])\n",
    "    predictions = xgb_model.predict(X[test_index])\n",
    "    actuals = y[test_index]\n",
    "    print(confusion_matrix(actuals, predictions))\n",
    "\n",
    "print(\"California Housing: regression\")\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=rng)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    xgb_model = xgb.XGBRegressor(n_jobs=1).fit(X[train_index], y[train_index])\n",
    "    predictions = xgb_model.predict(X[test_index])\n",
    "    actuals = y[test_index]\n",
    "    print(mean_squared_error(actuals, predictions))\n",
    "\n",
    "print(\"Parameter optimization\")\n",
    "xgb_model = xgb.XGBRegressor(n_jobs=1)\n",
    "clf = GridSearchCV(xgb_model,\n",
    "                   {'max_depth': [2, 4],\n",
    "                    'n_estimators': [50, 100]}, verbose=1, n_jobs=1, cv=3)\n",
    "clf.fit(X, y)\n",
    "print(clf.best_score_)\n",
    "print(clf.best_params_)\n",
    "xgb_model = xgb.XGBRegressor(max_depth = 4, n_estimators = 100)\n",
    "xgb_model.fit(X, y)\n",
    "predictions = xgb_model.predict(X[test_index])\n",
    "actuals = y[test_index]\n",
    "print(mean_squared_error(actuals, predictions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

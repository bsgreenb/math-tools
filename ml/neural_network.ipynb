{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "- Initialize $\\mathbf{w}(0)$\n",
    "- For $t=0,1,2, \\cdots \\quad$ [to termination]\n",
    "$$\n",
    "\\mathbf{w}(t+1)=\\mathbf{w}(t)-\\eta \\nabla E_{\\text {in }}(\\mathbf{w}(t))\n",
    "$$\n",
    "- Return final w\n",
    "\n",
    "With **Stochastic** gradient descent, we pick one $(x_n, y_n)$ at a time, applying Gradient Descent to $e(h(x_n), y_n)$.\n",
    "\n",
    "Rule of thumb: $\\eta = .1$ works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the Network Operates\n",
    "\n",
    "$$\n",
    "w_{i j}^{(l)} \\quad \\begin{cases}1 \\leq l \\leq L & \\text { layers } \\\\ 0 \\leq i \\leq d^{(l-1)} & \\text { inputs } \\\\ 1 \\leq j \\leq d^{(l)} & \\text { outputs }\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta(s)=\\tanh (s)=\\frac{e^s-e^{-s}}{e^s+e^{-s}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_j^{(l)}=\\theta\\left(s_j^{(l)}\\right)=\\theta\\left(\\sum_{i=0}^{d^{(l-1)}} w_{i j}^{(l)} x_i^{(l-1)}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text { Apply } \\mathbf{x} \\text { to } x_1^{(0)} \\cdots x_{d^{(0)}}^{(0)} \\rightarrow \\rightarrow x_1^{(L)}=h(\\mathbf{x})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying SGD\n",
    "\n",
    "Error on example $\\left(\\mathbf{x}_n, y_n\\right)$ is\n",
    "$$\n",
    "\\mathrm{e}\\left(h\\left(\\mathbf{x}_n\\right), y_n\\right)=\\mathrm{e}(\\mathrm{w})\n",
    "$$\n",
    "\n",
    "To implement SGD, we need the gradient: \n",
    "\n",
    "$$\n",
    "\\nabla \\mathrm{e}(\\mathbf{w}): \\frac{\\partial \\mathrm{e}(\\mathrm{w})}{\\partial w_{i j}^{(l)}} \\text { for all } i, j, l\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing $\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial w_{i j}^{(l)}}$\n",
    "\n",
    "A trick for efficient computation:\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial w_{i j}^{(l)}}=\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_j^{(l)}} \\times \\frac{\\partial s_j^{(l)}}{\\partial w_{i j}^{(l)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text { We have } \\frac{\\partial s_j^{(l)}}{\\partial w_{i j}^{(l)}}=x_i^{(l-1)} \\quad \\text { We only need: } \\frac{\\partial \\mathrm{e}(\\mathrm{w})}{\\partial s_j^{(l)}}=\\delta_j^{(l)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\delta$ for the final layer\n",
    "\n",
    "For the final layer $l=L$ and $j=1$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\delta_1^{(L)}=\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_1^{(L)}} \\\\\n",
    "& = \\frac{\\partial(\\theta(s_1^{(L)}) - y_n)^2}{\\partial s_1^{(L)}} \\\\\n",
    "& = 2(\\theta(s_1^{(L)}) - y_n)(\\theta^{\\prime}(s_1^{(L)})) \\\\\n",
    "& = 2(x_1^{(L)} - y_n)(\\theta^{\\prime}(s_1^{(L)})) \\\\\n",
    "& = 2(x_1^{(L)} - y_n)(1 - \\theta^2(s_1^{(L)})) \\\\\n",
    "& = 2(x_1^{(L)} - y_n)(1 - (x_1^{(L)})^2)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back propagation of $\\delta$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta_i^{(l-1)} & =\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_i^{(l-1)}} \\\\\n",
    "& =\\sum_{j=1}^{d^{(l)}} \\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_j^{(l)}} \\times \\frac{\\partial s_j^{(l)}}{\\partial x_i^{(l-1)}} \\times \\frac{\\partial x_i^{(l-1)}}{\\partial s_i^{(l-1)}} \\\\\n",
    "& =\\sum_{j=1}^{d^{(l)}} \\delta_j^{(l)} \\times w_{i j}^{(l)} \\times \\theta^{\\prime}\\left(s_i^{(l-1)}\\right) \\\\\n",
    "\\delta_i^{(l-1)} & =\\left(1-\\left(x_i^{(l-1)}\\right)^2\\right) \\sum_{j=1}^{d^{(l)}} w_{i j}^{(l)} \\delta_j^{(l)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/network-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Avg E_in: 1.080632067283989\n",
      "Avg E_out: 0.9682425514585163\n",
      "Epoch 1\n",
      "Avg E_in: 0.9981968364142746\n",
      "Avg E_out: 1.0272051574753187\n",
      "Epoch 2\n",
      "Avg E_in: 1.0440719140124097\n",
      "Avg E_out: 1.141552601276721\n",
      "Epoch 3\n",
      "Avg E_in: 0.9913724880066347\n",
      "Avg E_out: 1.019369097188882\n",
      "Epoch 4\n",
      "Avg E_in: 1.0202011724374942\n",
      "Avg E_out: 1.0762673740368058\n",
      "Epoch 5\n",
      "Avg E_in: 0.99933942293521\n",
      "Avg E_out: 1.049470956543511\n",
      "Epoch 6\n",
      "Avg E_in: 1.014020858676124\n",
      "Avg E_out: 1.0695361150633313\n",
      "Epoch 7\n",
      "Avg E_in: 1.0136890518107353\n",
      "Avg E_out: 1.073089136713895\n",
      "Epoch 8\n",
      "Avg E_in: 1.023961833025377\n",
      "Avg E_out: 1.0801484290432295\n",
      "Epoch 9\n",
      "Avg E_in: 1.0444126224054644\n",
      "Avg E_out: 1.1189418737045982\n",
      "Epoch 10\n",
      "Avg E_in: 1.012502800449149\n",
      "Avg E_out: 1.0697946275784729\n",
      "Epoch 11\n",
      "Avg E_in: 1.0471034943302457\n",
      "Avg E_out: 1.1204552750738246\n",
      "Epoch 12\n",
      "Avg E_in: 1.0042761199094374\n",
      "Avg E_out: 1.051956861675655\n",
      "Epoch 13\n",
      "Avg E_in: 1.0077170833318354\n",
      "Avg E_out: 1.0124062082910947\n",
      "Epoch 14\n",
      "Avg E_in: 1.008597524180453\n",
      "Avg E_out: 1.0526005480463785\n",
      "Epoch 15\n",
      "Avg E_in: 1.06838735809084\n",
      "Avg E_out: 1.1564131020987418\n",
      "Epoch 16\n",
      "Avg E_in: 1.0518218555760968\n",
      "Avg E_out: 1.1124151951308001\n",
      "Epoch 17\n",
      "Avg E_in: 1.016221334774009\n",
      "Avg E_out: 1.074935701807798\n",
      "Epoch 18\n",
      "Avg E_in: 0.999202280278839\n",
      "Avg E_out: 1.045865366097615\n",
      "Epoch 19\n",
      "Avg E_in: 1.0349996281312632\n",
      "Avg E_out: 1.1041909181437046\n",
      "Epoch 20\n",
      "Avg E_in: 1.057122115219491\n",
      "Avg E_out: 1.1395842388854396\n",
      "Epoch 21\n",
      "Avg E_in: 1.0580491889487915\n",
      "Avg E_out: 1.1538283286772824\n",
      "Epoch 22\n",
      "Avg E_in: 1.0288129856152755\n",
      "Avg E_out: 1.0995156005165958\n",
      "Epoch 23\n",
      "Avg E_in: 1.0228546346399314\n",
      "Avg E_out: 1.0920512791295358\n",
      "Epoch 24\n",
      "Avg E_in: 1.0311094756470087\n",
      "Avg E_out: 1.1271651269575194\n",
      "Epoch 25\n",
      "Avg E_in: 1.009798873351299\n",
      "Avg E_out: 1.1016291131296516\n",
      "Epoch 26\n",
      "Avg E_in: 1.0350196634549316\n",
      "Avg E_out: 1.122841544730482\n",
      "Epoch 27\n",
      "Avg E_in: 1.064572340981953\n",
      "Avg E_out: 1.131100843575313\n",
      "Epoch 28\n",
      "Avg E_in: 1.0386658115670007\n",
      "Avg E_out: 1.1257627389330425\n",
      "Epoch 29\n",
      "Avg E_in: 1.0126836269141557\n",
      "Avg E_out: 1.110694084693338\n",
      "Epoch 30\n",
      "Avg E_in: 1.0424322995727566\n",
      "Avg E_out: 1.1400394046529205\n",
      "Epoch 31\n",
      "Avg E_in: 1.0073002863239986\n",
      "Avg E_out: 1.1250548397252191\n",
      "Epoch 32\n",
      "Avg E_in: 0.9876251190421698\n",
      "Avg E_out: 1.1036535433092658\n",
      "Epoch 33\n",
      "Avg E_in: 0.9971195833058909\n",
      "Avg E_out: 1.1024614371591215\n",
      "Epoch 34\n",
      "Avg E_in: 1.01330959447702\n",
      "Avg E_out: 1.1424659916477624\n",
      "Epoch 35\n",
      "Avg E_in: 0.9904590549912046\n",
      "Avg E_out: 1.1223825240809437\n",
      "Epoch 36\n",
      "Avg E_in: 0.9761251465662886\n",
      "Avg E_out: 1.1199764864400152\n",
      "Epoch 37\n",
      "Avg E_in: 0.9510712674022151\n",
      "Avg E_out: 1.1212090154206114\n",
      "Epoch 38\n",
      "Avg E_in: 0.9230155014250123\n",
      "Avg E_out: 1.0784884094532237\n",
      "Epoch 39\n",
      "Avg E_in: 0.9639351297587202\n",
      "Avg E_out: 1.1500119564384446\n",
      "Epoch 40\n",
      "Avg E_in: 0.8960090631252949\n",
      "Avg E_out: 1.1010113985807917\n",
      "Epoch 41\n",
      "Avg E_in: 0.8780852335879346\n",
      "Avg E_out: 1.1086305046236327\n",
      "Epoch 42\n",
      "Avg E_in: 0.8824279854677494\n",
      "Avg E_out: 1.1001756326883196\n",
      "Epoch 43\n",
      "Avg E_in: 0.8161209973766097\n",
      "Avg E_out: 1.1030399361441525\n",
      "Epoch 44\n",
      "Avg E_in: 0.7603591621548077\n",
      "Avg E_out: 1.1488856070298068\n",
      "Epoch 45\n",
      "Avg E_in: 0.821504396883609\n",
      "Avg E_out: 1.1739496493632746\n",
      "Epoch 46\n",
      "Avg E_in: 0.7064376172888609\n",
      "Avg E_out: 1.1269767103428396\n",
      "Epoch 47\n",
      "Avg E_in: 0.7020913034595732\n",
      "Avg E_out: 1.1305482124432256\n",
      "Epoch 48\n",
      "Avg E_in: 0.8494778462201553\n",
      "Avg E_out: 1.1807275090665037\n",
      "Epoch 49\n",
      "Avg E_in: 0.6789647368871994\n",
      "Avg E_out: 1.1043188678627076\n",
      "Epoch 50\n",
      "Avg E_in: 0.7049374307318794\n",
      "Avg E_out: 1.0930797314685945\n",
      "Epoch 51\n",
      "Avg E_in: 0.6338835064110515\n",
      "Avg E_out: 1.12324394342673\n",
      "Epoch 52\n",
      "Avg E_in: 0.7020894884260199\n",
      "Avg E_out: 1.1167215465426983\n",
      "Epoch 53\n",
      "Avg E_in: 0.6103218763096037\n",
      "Avg E_out: 1.1052510184025524\n",
      "Epoch 54\n",
      "Avg E_in: 0.6304857384406544\n",
      "Avg E_out: 1.1804153370842911\n",
      "Epoch 55\n",
      "Avg E_in: 0.6842270776517004\n",
      "Avg E_out: 1.16919075818302\n",
      "Epoch 56\n",
      "Avg E_in: 0.6693521218139455\n",
      "Avg E_out: 1.1221040318314175\n",
      "Epoch 57\n",
      "Avg E_in: 0.6049567034576739\n",
      "Avg E_out: 1.0941887122429368\n",
      "Epoch 58\n",
      "Avg E_in: 0.6331105918479883\n",
      "Avg E_out: 1.174596651387253\n",
      "Epoch 59\n",
      "Avg E_in: 0.6645466536812394\n",
      "Avg E_out: 1.1934518824819458\n",
      "Epoch 60\n",
      "Avg E_in: 0.625424499081991\n",
      "Avg E_out: 1.11550166384194\n",
      "Epoch 61\n",
      "Avg E_in: 0.6039604972712161\n",
      "Avg E_out: 1.1032864375607636\n",
      "Epoch 62\n",
      "Avg E_in: 0.6375930590663318\n",
      "Avg E_out: 1.1353421961768642\n",
      "Epoch 63\n",
      "Avg E_in: 0.6098786834023504\n",
      "Avg E_out: 1.1000211388669736\n",
      "Epoch 64\n",
      "Avg E_in: 0.5950738583644102\n",
      "Avg E_out: 1.125815955652826\n",
      "Epoch 65\n",
      "Avg E_in: 0.6262626534070261\n",
      "Avg E_out: 1.0977994441236667\n",
      "Epoch 66\n",
      "Avg E_in: 0.6505403943586243\n",
      "Avg E_out: 1.1356663585812345\n",
      "Epoch 67\n",
      "Avg E_in: 0.608042648734809\n",
      "Avg E_out: 1.1293043039976463\n",
      "Epoch 68\n",
      "Avg E_in: 0.6255549511781127\n",
      "Avg E_out: 1.1263031472866132\n",
      "Epoch 69\n",
      "Avg E_in: 0.6543820173455832\n",
      "Avg E_out: 1.2171175611166007\n",
      "Epoch 70\n",
      "Avg E_in: 0.5884694139723223\n",
      "Avg E_out: 1.1568258426845346\n",
      "Epoch 71\n",
      "Avg E_in: 0.6318162756570054\n",
      "Avg E_out: 1.0925074795365448\n",
      "Epoch 72\n",
      "Avg E_in: 0.6739697764329783\n",
      "Avg E_out: 1.127833259359704\n",
      "Epoch 73\n",
      "Avg E_in: 0.6845584781858498\n",
      "Avg E_out: 1.136649393333658\n",
      "Epoch 74\n",
      "Avg E_in: 0.6669665416519657\n",
      "Avg E_out: 1.1625611181771183\n",
      "Epoch 75\n",
      "Avg E_in: 0.6389942331046794\n",
      "Avg E_out: 1.0956064208485152\n",
      "Epoch 76\n",
      "Avg E_in: 0.6543227455808156\n",
      "Avg E_out: 1.0683808945025086\n",
      "Epoch 77\n",
      "Avg E_in: 0.6513630545417185\n",
      "Avg E_out: 1.1006078621230997\n",
      "Epoch 78\n",
      "Avg E_in: 0.6438997431502884\n",
      "Avg E_out: 1.157507752472397\n",
      "Epoch 79\n",
      "Avg E_in: 0.6037381501107122\n",
      "Avg E_out: 1.0773000907254082\n",
      "Epoch 80\n",
      "Avg E_in: 0.6153798638586203\n",
      "Avg E_out: 1.1052768585763029\n",
      "Epoch 81\n",
      "Avg E_in: 0.6249912508769385\n",
      "Avg E_out: 1.0882424778490167\n",
      "Epoch 82\n",
      "Avg E_in: 0.58915601107119\n",
      "Avg E_out: 1.1137706008963422\n",
      "Epoch 83\n",
      "Avg E_in: 0.5829979271663758\n",
      "Avg E_out: 1.1004286923178497\n",
      "Epoch 84\n",
      "Avg E_in: 0.5987277911412389\n",
      "Avg E_out: 1.1243048467828614\n",
      "Epoch 85\n",
      "Avg E_in: 0.8740668794391949\n",
      "Avg E_out: 1.2084956664621154\n",
      "Epoch 86\n",
      "Avg E_in: 0.6373527741618652\n",
      "Avg E_out: 1.1281501862034693\n",
      "Epoch 87\n",
      "Avg E_in: 0.5838651220707531\n",
      "Avg E_out: 1.0301230499441965\n",
      "Epoch 88\n",
      "Avg E_in: 0.6209121259775625\n",
      "Avg E_out: 1.0944728949494489\n",
      "Epoch 89\n",
      "Avg E_in: 0.7189215345864698\n",
      "Avg E_out: 1.1750142833225408\n",
      "Epoch 90\n",
      "Avg E_in: 0.6624541399761111\n",
      "Avg E_out: 1.1174987720195453\n",
      "Epoch 91\n",
      "Avg E_in: 0.5767103385716031\n",
      "Avg E_out: 1.0758514763081999\n",
      "Epoch 92\n",
      "Avg E_in: 0.5878728424692629\n",
      "Avg E_out: 1.0819150402366662\n",
      "Epoch 93\n",
      "Avg E_in: 0.6024776231979996\n",
      "Avg E_out: 1.0640899048704315\n",
      "Epoch 94\n",
      "Avg E_in: 0.6160898721261058\n",
      "Avg E_out: 1.0598155692876694\n",
      "Epoch 95\n",
      "Avg E_in: 0.6139247426444944\n",
      "Avg E_out: 1.0619106296497762\n",
      "Epoch 96\n",
      "Avg E_in: 0.6433225807301011\n",
      "Avg E_out: 1.0855490281280618\n",
      "Epoch 97\n",
      "Avg E_in: 0.6357794908103191\n",
      "Avg E_out: 1.129525520165438\n",
      "Epoch 98\n",
      "Avg E_in: 0.636060889269244\n",
      "Avg E_out: 1.1573989447968938\n",
      "Epoch 99\n",
      "Avg E_in: 0.6003699121670274\n",
      "Avg E_out: 1.124269489261379\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Stochastic gradient descent with multilayer 2D perceptrons.  Takes an array of 2d points as inputs, a target function, and a value for eta, and runs sgd, ultimately returning the weights.\n",
    "DEFAULT_ETA = .1\n",
    "\n",
    "# https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/\n",
    "    # \" an MLP with two hidden layers is sufficient for creating classification regions of any desired shape. This is instructive, although it should be noted that no indication of how many nodes to use in each layer\"\n",
    "LAYER_STRUCTURE = [2, 3, 2, 1] # Two hidden layers, with 3 neurons and 2 neurons respectively.  Note that these counts do not include the bias neurons, which are indexed at 0.  See the structure in the image above.\n",
    "\n",
    "ACTIVATION_FUNCTION = np.tanh\n",
    "\n",
    "# A single final output is assumed.\n",
    "\n",
    "#TODO: this would be better as a Class, due to var sharing\n",
    "def sgd(inputs, outputs, eta = DEFAULT_ETA, layer_structure=LAYER_STRUCTURE):\n",
    "    total_layers = len(LAYER_STRUCTURE) # Input layer + hidden layers + output layer\n",
    "    # Initialize all neurons x with empty values\n",
    "    x = np.empty(total_layers, dtype=object)\n",
    "    for l, neuron_count in enumerate(LAYER_STRUCTURE):\n",
    "        x[l] = np.empty(neuron_count + 1)\n",
    "        if l < (len(LAYER_STRUCTURE) - 1):\n",
    "            x[l][0] = 1 # 1 for bias\n",
    "        else:\n",
    "            x[l][0] = None # No bias on the final layer.  A placeholder to get 1-based indexing of real neurons.\n",
    "\n",
    "    # Initialize all weights w_{ij}^(l) with Xavier weight initialization..\n",
    "    # Note that our weights include biases.\n",
    "    weights = np.empty(total_layers, dtype=object)\n",
    "\n",
    "    for l, neuron_count in enumerate(LAYER_STRUCTURE):\n",
    "        if l == 0:\n",
    "            prev_neuron_count = neuron_count\n",
    "            continue # skip input layer\n",
    "        weights[l] = np.empty((prev_neuron_count + 1, neuron_count + 1)) \n",
    "        weights[l][0,:] = 0.0 # initialize biases to 0\n",
    "        for i in range(1, prev_neuron_count + 1):\n",
    "            for j in range(1, neuron_count + 1):\n",
    "                weights[l][i][j] = xavier_weight(prev_neuron_count, neuron_count)\n",
    "        prev_neuron_count = neuron_count\n",
    "\n",
    "    # TODO: Proly worth reporting e_in/e_out on our bogus starting weights.\n",
    "\n",
    "    # Initialize the array of signal derivatives\n",
    "    d = np.empty(total_layers, dtype=object)\n",
    "    for l, neuron_count in enumerate(LAYER_STRUCTURE):\n",
    "        if l == 0:\n",
    "            continue # skip input layer\n",
    "        d[l] = np.empty(neuron_count + 1)\n",
    "\n",
    "    epoch_count = 0 \n",
    "    #TODO: ultimately, will want something like out of sample validation to end the loop\n",
    "    while (epoch_count < 100):\n",
    "        # Generate a random permutation for how we pick the inputs\n",
    "        input_perm = np.random.permutation(len(inputs))\n",
    "        for input_id in input_perm:\n",
    "            input = inputs[input_id]\n",
    "\n",
    "            #TODO: extract common feedforward code\n",
    "            # Forward: Compute all x_j^(l)\n",
    "            # First we set layer 1 values\n",
    "            x[0] = input\n",
    "\n",
    "            # Now we forward propagate x\n",
    "            # NOTE: this could be vectorized (but would have to address the empty value in weights)\n",
    "            for l in range(1, total_layers):\n",
    "                for j in range(1, len(x[l])):\n",
    "                    x[l][j] = ACTIVATION_FUNCTION(x[l - 1].dot(weights[l][:,j]))\n",
    "\n",
    "            # Backward: Compute all d_j^(l)\n",
    "            # Begin by computing d_1^(L) for the final layer\n",
    "            d[-1][1] = 2 * (x[-1][1] - outputs[input_id]) * (1 - (x[-1][1] ** 2))\n",
    "\n",
    "            # Now we loop through and calculate the previous d's backward..\n",
    "            for l in reversed(range(1,total_layers - 1)):\n",
    "                for i in range(1, LAYER_STRUCTURE[l]):\n",
    "                    d[l][i] = (1 - ((x[l][i]) ** 2)) * weights[l+1][i][1:].dot(d[l + 1][1:])\n",
    "\n",
    "            # Update the weights: w_{ij}^(l) = w_{ij}^(l) - (eta * x_i^(l-1) * d_j^(l))\n",
    "            # We'd never have j= 1 as an output\n",
    "            \n",
    "            for l in range(1, total_layers):\n",
    "                for i in range(0, LAYER_STRUCTURE[l-1]):\n",
    "                    for j in range(1, LAYER_STRUCTURE[l]):\n",
    "                        weights[l][i][j] = weights[l][i][j] - (eta * x[l-1][i] * d[l][j])\n",
    "            \n",
    "        # Report E_in and e_out for the epoch.  Report current weights too.\n",
    "        print(\"Epoch \" + str(epoch_count))\n",
    "        total_e_in = 0\n",
    "        # First E_in\n",
    "        for input_id, input in enumerate(inputs):\n",
    "            #TODO: extract common feedforward code\n",
    "            x[0] = input\n",
    "\n",
    "            # Now we forward propagate x\n",
    "            # NOTE: this could be vectorized (but would have to address the empty value in weights)\n",
    "            for l in range(1, total_layers):\n",
    "                for j in range(1, len(x[l])):\n",
    "                    x[l][j] = ACTIVATION_FUNCTION(x[l - 1].dot(weights[l][:,j]))\n",
    "            \n",
    "            h = x[-1][1]\n",
    "            # TODO: extract common err code\n",
    "            e_in = (h - outputs[input_id]) ** 2\n",
    "            total_e_in += e_in\n",
    "        \n",
    "        avg_e_in = total_e_in / len(inputs)\n",
    "        print(\"Avg E_in: \" + str(avg_e_in))\n",
    "        \n",
    "        # Now for e_out\n",
    "        total_e_out = 0\n",
    "        # TODO: extract generation of data code\n",
    "        n = 1000 # 1000 training inputs, 1000 testing seems good?\n",
    "        # QSTN: What's a good ratio between training N and testing N?\n",
    "        testing_inputs = rng.uniform(-1,1,(n, 2))\n",
    "        testing_inputs = np.hstack((np.ones((n,1)), testing_inputs)) # Set x_0 = 1 for all xs\n",
    "        testing_outputs = np.sign(.5 - (np.square(testing_inputs[:,1]) + np.square(testing_inputs[:,2])))\n",
    "\n",
    "        for testing_input_id, testing_input in enumerate(testing_inputs):\n",
    "            #TODO: extract common feedforward code\n",
    "            x[0] = testing_input\n",
    "\n",
    "            # Now we forward propagate x\n",
    "            # NOTE: this could be vectorized (but would have to address the empty value in weights)\n",
    "            for l in range(1, total_layers):\n",
    "                for j in range(1, len(x[l])):\n",
    "                    x[l][j] = ACTIVATION_FUNCTION(x[l - 1].dot(weights[l][:,j]))\n",
    "            \n",
    "            h = x[-1][1]\n",
    "            # TODO: extract common err code\n",
    "            e_out = (h - testing_outputs[testing_input_id]) ** 2\n",
    "            total_e_out += e_out\n",
    "        \n",
    "        avg_e_out = total_e_out / len(testing_inputs)\n",
    "        print(\"Avg E_out: \" + str(avg_e_out))        \n",
    "\n",
    "        epoch_count += 1\n",
    "\n",
    "        # Break the loop if it's time to stop [ might be done through an e_out target]\n",
    "        # QSTN: when is the right time to stop in SGD?  how do we identify that?\n",
    "            # Note, overfitting is a concern here https://stats.stackexchange.com/questions/433187/stopping-criteria-for-stochastic-gradient-descent , validation is part of how it's addressed.  Not sure how much I want to postpone my v1 over this..\n",
    "\n",
    "    # Return the weights\n",
    "\n",
    "# looking like https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/, Xavier weight initialization is a good approach. \n",
    "# Note: biases don't count towards count, so subtract them off before passing in\n",
    "def xavier_weight(prev_neuron_count, neuron_count):\n",
    "    return np.random.uniform(-np.sqrt(6)/np.sqrt(prev_neuron_count + neuron_count),np.sqrt(6)/np.sqrt(prev_neuron_count + neuron_count))\n",
    "\n",
    "\n",
    "# Our inputs\n",
    "n = 10\n",
    "rng = np.random.default_rng()\n",
    "inputs = rng.uniform(-1,1,(n, 2))\n",
    "inputs = np.hstack((np.ones((n,1)), inputs)) # Set x_0 = 1 for all xs\n",
    "\n",
    "# Circle function (our target function):\n",
    "# If x_1^2 + x_2^2 < .5, then we are inside our circle (return +1) (which has radius of sqrt(.5) =~ .707).  else return -1.\n",
    "outputs = np.sign(.5 - (np.square(inputs[:,1]) + np.square(inputs[:,2])))\n",
    "\n",
    "# Our target function\n",
    "target_function = None\n",
    "\n",
    "print(sgd(inputs, outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

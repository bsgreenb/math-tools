{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "- Initialize $\\mathbf{w}(0)$\n",
    "- For $t=0,1,2, \\cdots \\quad$ [to termination]\n",
    "$$\n",
    "\\mathbf{w}(t+1)=\\mathbf{w}(t)-\\eta \\nabla E_{\\text {in }}(\\mathbf{w}(t))\n",
    "$$\n",
    "- Return final w\n",
    "\n",
    "With **Stochastic** gradient descent, we pick one $(x_n, y_n)$ at a time, applying Gradient Descent to $e(h(x_n), y_n)$.\n",
    "\n",
    "Rule of thumb: $\\eta = .1$ works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the Network Operates\n",
    "\n",
    "$$\n",
    "w_{i j}^{(l)} \\quad \\begin{cases}1 \\leq l \\leq L & \\text { layers } \\\\ 0 \\leq i \\leq d^{(l-1)} & \\text { inputs } \\\\ 1 \\leq j \\leq d^{(l)} & \\text { outputs }\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta(s)=\\tanh (s)=\\frac{e^s-e^{-s}}{e^s+e^{-s}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_j^{(l)}=\\theta\\left(s_j^{(l)}\\right)=\\theta\\left(\\sum_{i=0}^{d^{(l-1)}} w_{i j}^{(l)} x_i^{(l-1)}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text { Apply } \\mathbf{x} \\text { to } x_1^{(0)} \\cdots x_{d^{(0)}}^{(0)} \\rightarrow \\rightarrow x_1^{(L)}=h(\\mathbf{x})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying SGD\n",
    "\n",
    "Error on example $\\left(\\mathbf{x}_n, y_n\\right)$ is\n",
    "$$\n",
    "\\mathrm{e}\\left(h\\left(\\mathbf{x}_n\\right), y_n\\right)=\\mathrm{e}(\\mathrm{w})\n",
    "$$\n",
    "\n",
    "To implement SGD, we need the gradient: \n",
    "\n",
    "$$\n",
    "\\nabla \\mathrm{e}(\\mathbf{w}): \\frac{\\partial \\mathrm{e}(\\mathrm{w})}{\\partial w_{i j}^{(l)}} \\text { for all } i, j, l\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing $\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial w_{i j}^{(l)}}$\n",
    "\n",
    "A trick for efficient computation:\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial w_{i j}^{(l)}}=\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_j^{(l)}} \\times \\frac{\\partial s_j^{(l)}}{\\partial w_{i j}^{(l)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text { We have } \\frac{\\partial s_j^{(l)}}{\\partial w_{i j}^{(l)}}=x_i^{(l-1)} \\quad \\text { We only need: } \\frac{\\partial \\mathrm{e}(\\mathrm{w})}{\\partial s_j^{(l)}}=\\delta_j^{(l)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\delta$ for the final layer\n",
    "\n",
    "For the final layer $l=L$ and $j=1$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\delta_1^{(L)}=\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_1^{(L)}} \\\\\n",
    "& = \\frac{\\partial(\\theta(s_1^{(L)}) - y_n)^2}{\\partial s_1^{(L)}} \\\\\n",
    "& = 2(\\theta(s_1^{(L)}) - y_n)(\\theta^{\\prime}(s_1^{(L)})) \\\\\n",
    "& = 2(x_1^{(L)} - y_n)(\\theta^{\\prime}(s_1^{(L)})) \\\\\n",
    "& = 2(x_1^{(L)} - y_n)(1 - \\theta^2(s_1^{(L)})) \\\\\n",
    "& = 2(x_1^{(L)} - y_n)(1 - (x_1^{(L)})^2)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back propagation of $\\delta$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta_i^{(l-1)} & =\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_i^{(l-1)}} \\\\\n",
    "& =\\sum_{j=1}^{d^{(l)}} \\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_j^{(l)}} \\times \\frac{\\partial s_j^{(l)}}{\\partial x_i^{(l-1)}} \\times \\frac{\\partial x_i^{(l-1)}}{\\partial s_i^{(l-1)}} \\\\\n",
    "& =\\sum_{j=1}^{d^{(l)}} \\delta_j^{(l)} \\times w_{i j}^{(l)} \\times \\theta^{\\prime}\\left(s_i^{(l-1)}\\right) \\\\\n",
    "\\delta_i^{(l-1)} & =\\left(1-\\left(x_i^{(l-1)}\\right)^2\\right) \\sum_{j=1}^{d^{(l)}} w_{i j}^{(l)} \\delta_j^{(l)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/network-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# QSTN: reuse the same out of sample?  Or generate fresh like we've been doing?\n",
    "\n",
    "class NeuralNetwork:\n",
    "    TRAINING_INPUT_N = 10000\n",
    "    TESTING_INPUT_N = 1000\n",
    "    RNG = np.random.default_rng()\n",
    "    ETA = .1\n",
    "    # https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/\n",
    "    # \"an MLP with two hidden layers is sufficient for creating classification regions of any desired shape. This is instructive, although it should be noted that no indication of how many nodes to use in each layer\"\n",
    "    LAYER_STRUCTURE = [2, 3, 2, 1] # Two hidden layers, with 3 neurons and 2 neurons respectively.  Note that these counts do not include the bias neurons, which are indexed at 0.  See the structure in the image above.\n",
    "    LAYER_COUNT = len(LAYER_STRUCTURE)\n",
    "    ACTIVATION_FUNCTION = np.tanh\n",
    "    EPOCH_LIMIT = 100\n",
    "\n",
    "    def __init__(self):\n",
    "        self.training_inputs = self.generate_inputs(self.TRAINING_INPUT_N)\n",
    "        \n",
    "        self.training_outputs = self.generate_outputs(self.training_inputs)\n",
    "        \n",
    "        # Initialize all neurons x with empty values\n",
    "        self.x = self.initial_neurons()\n",
    "\n",
    "        # Initialize all weights w_{ij}^(l) with Xavier weight initialization..\n",
    "        self.weights = self.initial_weights()\n",
    "\n",
    "        # Initialize the array of signal derivatives\n",
    "        self.d = self.initial_signal_derivatives()\n",
    "\n",
    "        # Keep track of e_ins and e_outs with these\n",
    "        self.e_ins = np.empty(self.EPOCH_LIMIT)\n",
    "        self.e_outs = np.empty(self.EPOCH_LIMIT)\n",
    "\n",
    "    def generate_inputs(self, N):\n",
    "        inputs = self.RNG.uniform(-1,1,(N, 2))\n",
    "        return np.hstack((np.ones((N,1)), inputs)) # Set x_0 = 1 for all xs\n",
    "\n",
    "    def generate_outputs(self, inputs):\n",
    "        # Circle function (our target function):\n",
    "        # If x_1^2 + x_2^2 < .5, then we are inside our circle (return +1) (which has radius of sqrt(.5) =~ .707).  else return -1.\n",
    "        return np.sign(.5 - (np.square(inputs[:,1]) + np.square(inputs[:,2])))\n",
    "\n",
    "    def initial_neurons(self):\n",
    "        x = np.empty(self.LAYER_COUNT, dtype=object)\n",
    "\n",
    "        for l, neuron_count in enumerate(self.LAYER_STRUCTURE):\n",
    "            x[l] = np.empty(neuron_count + 1)\n",
    "            if l < (len(self.LAYER_STRUCTURE) - 1):\n",
    "                x[l][0] = 1 # 1 for bias\n",
    "            else:\n",
    "                x[l][0] = None # No bias on the final layer.  A placeholder to get 1-based indexing of real neurons.\n",
    "\n",
    "        return x\n",
    "\n",
    "    def initial_weights(self):\n",
    "        # Note that our weights include biases.\n",
    "        weights = np.empty(self.LAYER_COUNT, dtype=object)\n",
    "        for l, neuron_count in enumerate(self.LAYER_STRUCTURE):\n",
    "            if l == 0:\n",
    "                prev_neuron_count = neuron_count\n",
    "                continue # skip input layer\n",
    "            weights[l] = np.empty((prev_neuron_count + 1, neuron_count + 1)) \n",
    "            weights[l][0,:] = 0.0 # initialize biases to 0\n",
    "            for i in range(1, prev_neuron_count + 1):\n",
    "                for j in range(1, neuron_count + 1):\n",
    "                    weights[l][i][j] = self.xavier_weight(prev_neuron_count, neuron_count)\n",
    "            prev_neuron_count = neuron_count\n",
    "        \n",
    "        return weights\n",
    "\n",
    "    # looking like https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/, Xavier weight initialization is a good approach. \n",
    "    # Note: biases don't count towards count, so don't include them in the count.\n",
    "    def xavier_weight(self, prev_neuron_count, neuron_count):\n",
    "        return np.random.uniform(-np.sqrt(6)/np.sqrt(prev_neuron_count + neuron_count),np.sqrt(6)/np.sqrt(prev_neuron_count + neuron_count))\n",
    "\n",
    "    def initial_signal_derivatives(self):\n",
    "        d = np.empty(self.LAYER_COUNT, dtype=object)\n",
    "        for l, neuron_count in enumerate(self.LAYER_STRUCTURE):\n",
    "            if l == 0:\n",
    "                continue # skip input layer\n",
    "            d[l] = np.empty(neuron_count + 1)\n",
    "\n",
    "        return d\n",
    "\n",
    "    def sgd(self):\n",
    "        epoch_count = 0 \n",
    "        #TODO: ultimately, will want something like out of sample validation to end the loop\n",
    "        while (epoch_count < self.EPOCH_LIMIT):\n",
    "            # Generate a random permutation for how we pick the inputs\n",
    "            input_perm = np.random.permutation(len(self.training_inputs))\n",
    "            for input_id in input_perm:\n",
    "                input = self.training_inputs[input_id]\n",
    "                self.feed_forward(input)\n",
    "                output = self.training_outputs[input_id]\n",
    "                self.back_propogate(output)\n",
    "                self.update_weights()\n",
    "            \n",
    "            # Report E_in and e_out for the epoch.\n",
    "\n",
    "            avg_e_in = self.calculate_e_in()\n",
    "            self.e_ins[epoch_count] = avg_e_in\n",
    "            avg_e_out = self.calculate_e_out()\n",
    "            self.e_outs[epoch_count] = avg_e_in\n",
    "\n",
    "            epoch_count += 1\n",
    "        \n",
    "        # Show the plot of e_in and e_out by epoch\n",
    "        figure, axis = plt.subplots(1, 2) \n",
    "        axis[0].scatter(range(0, self.EPOCH_LIMIT), self.e_ins, c='blue')\n",
    "        axis[1].scatter(range(0, self.EPOCH_LIMIT), self.e_outs, c='red')\n",
    "        plt.show()\n",
    "\n",
    "    #QSTN: should feed_forward include setting of x?\n",
    "    def feed_forward(self, input):\n",
    "        # We compute all x_j^(l)\n",
    "        # First we set layer 1 values\n",
    "        self.x[0] = input\n",
    "\n",
    "        for l in range(1, self.LAYER_COUNT):\n",
    "            for j in range(1, len(self.x[l])):\n",
    "                self.x[l][j] = self.ACTIVATION_FUNCTION(self.x[l - 1].dot(self.weights[l][:,j]))\n",
    "\n",
    "    def back_propogate(self, output):\n",
    "        # We compute all d_j^(l)\n",
    "        # Begin by computing d_1^(L) for the final layer\n",
    "        self.d[-1][1] = 2 * (self.x[-1][1] - output) * (1 - (self.x[-1][1] ** 2))\n",
    "\n",
    "        # Now we loop through and calculate the previous d's backward..\n",
    "        for l in reversed(range(1,self.LAYER_COUNT - 1)):\n",
    "            for i in range(1, self.LAYER_STRUCTURE[l]):\n",
    "                self.d[l][i] = (1 - ((self.x[l][i]) ** 2)) * self.weights[l+1][i][1:].dot(self.d[l + 1][1:])\n",
    "\n",
    "    def update_weights(self):\n",
    "        # Update the weights: w_{ij}^(l) = w_{ij}^(l) - (eta * x_i^(l-1) * d_j^(l))\n",
    "        # We'd never have j= 1 as an output\n",
    "        for l in range(1, self.LAYER_COUNT):\n",
    "            for i in range(0, self.LAYER_STRUCTURE[l-1]):\n",
    "                for j in range(1, self.LAYER_STRUCTURE[l]):\n",
    "                    self.weights[l][i][j] = self.weights[l][i][j] - (self.ETA * self.x[l-1][i] * self.d[l][j])\n",
    "\n",
    "    def calculate_e_in(self):\n",
    "        total_e_in = 0\n",
    "\n",
    "        for input_id, input in enumerate(self.training_inputs):\n",
    "            self.feed_forward(input)\n",
    "            \n",
    "            h = self.x[-1][1]\n",
    "            e_in = (h - self.training_outputs[input_id]) ** 2 # MSE measure\n",
    "            total_e_in += e_in\n",
    "        \n",
    "        return total_e_in / len(self.training_inputs)\n",
    "\n",
    "    def calculate_e_out(self):\n",
    "        total_e_out = 0\n",
    "\n",
    "        # QSTN: reuse the same out of sample?  Or generate fresh like we've been doing?\n",
    "        testing_inputs = self.generate_inputs(self.TESTING_INPUT_N)\n",
    "        testing_outputs = self.generate_outputs(testing_inputs)\n",
    "\n",
    "        for testing_input_id, testing_input in enumerate(testing_inputs):\n",
    "            self.feed_forward(testing_input)\n",
    "\n",
    "            h = self.x[-1][1]\n",
    "            e_out = (h - testing_outputs[testing_input_id]) ** 2 # MSE measure\n",
    "            total_e_out += e_out\n",
    "\n",
    "        return total_e_out / len(testing_inputs)\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "nn.sgd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

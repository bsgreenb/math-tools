{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "- Initialize $\\mathbf{w}(0)$\n",
    "- For $t=0,1,2, \\cdots \\quad$ [to termination]\n",
    "$$\n",
    "\\mathbf{w}(t+1)=\\mathbf{w}(t)-\\eta \\nabla E_{\\text {in }}(\\mathbf{w}(t))\n",
    "$$\n",
    "- Return final w\n",
    "\n",
    "With **Stochastic** gradient descent, we pick one $(x_n, y_n)$ at a time, applying Gradient Descent to $e(h(x_n), y_n)$.\n",
    "\n",
    "Rule of thumb: $\\eta = .1$ works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the Network Operates\n",
    "\n",
    "$$\n",
    "w_{i j}^{(l)} \\quad \\begin{cases}1 \\leq l \\leq L & \\text { layers } \\\\ 0 \\leq i \\leq d^{(l-1)} & \\text { inputs } \\\\ 1 \\leq j \\leq d^{(l)} & \\text { outputs }\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta(s)=\\tanh (s)=\\frac{e^s-e^{-s}}{e^s+e^{-s}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_j^{(l)}=\\theta\\left(s_j^{(l)}\\right)=\\theta\\left(\\sum_{i=0}^{d^{(l-1)}} w_{i j}^{(l)} x_i^{(l-1)}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text { Apply } \\mathbf{x} \\text { to } x_1^{(0)} \\cdots x_{d^{(0)}}^{(0)} \\rightarrow \\rightarrow x_1^{(L)}=h(\\mathbf{x})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying SGD\n",
    "\n",
    "Error on example $\\left(\\mathbf{x}_n, y_n\\right)$ is\n",
    "$$\n",
    "\\mathrm{e}\\left(h\\left(\\mathbf{x}_n\\right), y_n\\right)=\\mathrm{e}(\\mathrm{w})\n",
    "$$\n",
    "\n",
    "To implement SGD, we need the gradient: \n",
    "\n",
    "$$\n",
    "\\nabla \\mathrm{e}(\\mathbf{w}): \\frac{\\partial \\mathrm{e}(\\mathrm{w})}{\\partial w_{i j}^{(l)}} \\text { for all } i, j, l\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing $\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial w_{i j}^{(l)}}$\n",
    "\n",
    "A trick for efficient computation:\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial w_{i j}^{(l)}}=\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_j^{(l)}} \\times \\frac{\\partial s_j^{(l)}}{\\partial w_{i j}^{(l)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text { We have } \\frac{\\partial s_j^{(l)}}{\\partial w_{i j}^{(l)}}=x_i^{(l-1)} \\quad \\text { We only need: } \\frac{\\partial \\mathrm{e}(\\mathrm{w})}{\\partial s_j^{(l)}}=\\delta_j^{(l)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\delta$ for the final layer\n",
    "\n",
    "For the final layer $l=L$ and $j=1$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\delta_1^{(L)}=\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_1^{(L)}} \\\\\n",
    "& = \\frac{\\partial(\\theta(s_1^{(L)}) - y_n)^2}{\\partial s_1^{(L)}} \\\\\n",
    "& = 2(\\theta(s_1^{(L)}) - y_n)(\\theta^{\\prime}(s_1^{(L)})) \\\\\n",
    "& = 2(x_1^{(L)} - y_n)(\\theta^{\\prime}(s_1^{(L)})) \\\\\n",
    "& = 2(x_1^{(L)} - y_n)(1 - \\theta^2(s_1^{(L)})) \\\\\n",
    "& = 2(x_1^{(L)} - y_n)(1 - (x_1^{(L)})^2)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back propagation of $\\delta$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta_i^{(l-1)} & =\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_i^{(l-1)}} \\\\\n",
    "& =\\sum_{j=1}^{d^{(l)}} \\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_j^{(l)}} \\times \\frac{\\partial s_j^{(l)}}{\\partial x_i^{(l-1)}} \\times \\frac{\\partial x_i^{(l-1)}}{\\partial s_i^{(l-1)}} \\\\\n",
    "& =\\sum_{j=1}^{d^{(l)}} \\delta_j^{(l)} \\times w_{i j}^{(l)} \\times \\theta^{\\prime}\\left(s_i^{(l-1)}\\right) \\\\\n",
    "\\delta_i^{(l-1)} & =\\left(1-\\left(x_i^{(l-1)}\\right)^2\\right) \\sum_{j=1}^{d^{(l)}} w_{i j}^{(l)} \\delta_j^{(l)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/network-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# QSTN: reuse the same out of sample?  Or generate fresh like we've been doing?\n",
    "\n",
    "class NeuralNetwork:\n",
    "    TRAINING_INPUT_N = 10000\n",
    "    TESTING_INPUT_N = 1000\n",
    "    RNG = np.random.default_rng()\n",
    "    ETA = .1\n",
    "    # https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/\n",
    "    # \"an MLP with two hidden layers is sufficient for creating classification regions of any desired shape. This is instructive, although it should be noted that no indication of how many nodes to use in each layer\"\n",
    "    LAYER_STRUCTURE = [2, 3, 2, 1] # Two hidden layers, with 3 neurons and 2 neurons respectively.  Note that these counts do not include the bias neurons, which are indexed at 0.  See the structure in the image above.\n",
    "    LAYER_COUNT = len(LAYER_STRUCTURE)\n",
    "    ACTIVATION_FUNCTION = np.tanh\n",
    "    EPOCH_LIMIT = 1000\n",
    "\n",
    "    def __init__(self):\n",
    "        self.training_inputs = self.generate_inputs(self.TRAINING_INPUT_N)\n",
    "        \n",
    "        self.training_outputs = self.generate_outputs(self.training_inputs)\n",
    "        \n",
    "        # Initialize all neurons x with empty values\n",
    "        self.x = self.initial_neurons()\n",
    "\n",
    "        # Initialize all weights w_{ij}^(l) with Xavier weight initialization..\n",
    "        self.weights = self.initial_weights()\n",
    "\n",
    "        # Initialize the array of signal derivatives\n",
    "        self.d = self.initial_signal_derivatives()\n",
    "\n",
    "    def generate_inputs(self, N):\n",
    "        inputs = self.RNG.uniform(-1,1,(N, 2))\n",
    "        return np.hstack((np.ones((N,1)), inputs)) # Set x_0 = 1 for all xs\n",
    "\n",
    "    def generate_outputs(self, inputs):\n",
    "        # Circle function (our target function):\n",
    "        # If x_1^2 + x_2^2 < .5, then we are inside our circle (return +1) (which has radius of sqrt(.5) =~ .707).  else return -1.\n",
    "        return np.sign(.5 - (np.square(inputs[:,1]) + np.square(inputs[:,2])))\n",
    "\n",
    "    def initial_neurons(self):\n",
    "        x = np.empty(self.LAYER_COUNT, dtype=object)\n",
    "\n",
    "        for l, neuron_count in enumerate(self.LAYER_STRUCTURE):\n",
    "            x[l] = np.empty(neuron_count + 1)\n",
    "            if l < (len(self.LAYER_STRUCTURE) - 1):\n",
    "                x[l][0] = 1 # 1 for bias\n",
    "            else:\n",
    "                x[l][0] = None # No bias on the final layer.  A placeholder to get 1-based indexing of real neurons.\n",
    "\n",
    "        return x\n",
    "\n",
    "    def initial_weights(self):\n",
    "        # Note that our weights include biases.\n",
    "        weights = np.empty(self.LAYER_COUNT, dtype=object)\n",
    "        for l, neuron_count in enumerate(self.LAYER_STRUCTURE):\n",
    "            if l == 0:\n",
    "                prev_neuron_count = neuron_count\n",
    "                continue # skip input layer\n",
    "            weights[l] = np.empty((prev_neuron_count + 1, neuron_count + 1)) \n",
    "            weights[l][0,:] = 0.0 # initialize biases to 0\n",
    "            for i in range(1, prev_neuron_count + 1):\n",
    "                for j in range(1, neuron_count + 1):\n",
    "                    weights[l][i][j] = self.xavier_weight(prev_neuron_count, neuron_count)\n",
    "            prev_neuron_count = neuron_count\n",
    "        \n",
    "        return weights\n",
    "\n",
    "    # looking like https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/, Xavier weight initialization is a good approach. \n",
    "    # Note: biases don't count towards count, so don't include them in the count.\n",
    "    def xavier_weight(self, prev_neuron_count, neuron_count):\n",
    "        return np.random.uniform(-np.sqrt(6)/np.sqrt(prev_neuron_count + neuron_count),np.sqrt(6)/np.sqrt(prev_neuron_count + neuron_count))\n",
    "\n",
    "    def initial_signal_derivatives(self):\n",
    "        d = np.empty(self.LAYER_COUNT, dtype=object)\n",
    "        for l, neuron_count in enumerate(self.LAYER_STRUCTURE):\n",
    "            if l == 0:\n",
    "                continue # skip input layer\n",
    "            d[l] = np.empty(neuron_count + 1)\n",
    "\n",
    "        return d\n",
    "\n",
    "    def sgd(self):\n",
    "        epoch_count = 0 \n",
    "        #TODO: ultimately, will want something like out of sample validation to end the loop\n",
    "        while (epoch_count < self.EPOCH_LIMIT):\n",
    "            # Generate a random permutation for how we pick the inputs\n",
    "            input_perm = np.random.permutation(len(self.training_inputs))\n",
    "            for input_id in input_perm:\n",
    "                input = self.training_inputs[input_id]\n",
    "                self.feed_forward(input)\n",
    "                output = self.training_outputs[input_id]\n",
    "                self.back_propogate(output)\n",
    "                self.update_weights()\n",
    "            \n",
    "            # Report E_in and e_out for the epoch.\n",
    "\n",
    "            print(\"Epoch \" + str(epoch_count))\n",
    "            avg_e_in = self.calculate_e_in()\n",
    "            print(\"Avg E_in: \" + str(avg_e_in))\n",
    "            avg_e_out = self.calculate_e_out()\n",
    "            print(\"Avg E_out: \" + str(avg_e_out))\n",
    "\n",
    "            epoch_count += 1\n",
    "\n",
    "    #QSTN: should feed_forward include setting of x?\n",
    "    def feed_forward(self, input):\n",
    "        # We compute all x_j^(l)\n",
    "        # First we set layer 1 values\n",
    "        self.x[0] = input\n",
    "\n",
    "        for l in range(1, self.LAYER_COUNT):\n",
    "            for j in range(1, len(self.x[l])):\n",
    "                self.x[l][j] = ACTIVATION_FUNCTION(self.x[l - 1].dot(self.weights[l][:,j]))\n",
    "\n",
    "    def back_propogate(self, output):\n",
    "        # We compute all d_j^(l)\n",
    "        # Begin by computing d_1^(L) for the final layer\n",
    "        self.d[-1][1] = 2 * (self.x[-1][1] - output) * (1 - (self.x[-1][1] ** 2))\n",
    "\n",
    "        # Now we loop through and calculate the previous d's backward..\n",
    "        for l in reversed(range(1,self.LAYER_COUNT - 1)):\n",
    "            for i in range(1, self.LAYER_STRUCTURE[l]):\n",
    "                self.d[l][i] = (1 - ((self.x[l][i]) ** 2)) * self.weights[l+1][i][1:].dot(self.d[l + 1][1:])\n",
    "\n",
    "    def update_weights(self):\n",
    "        # Update the weights: w_{ij}^(l) = w_{ij}^(l) - (eta * x_i^(l-1) * d_j^(l))\n",
    "        # We'd never have j= 1 as an output\n",
    "        for l in range(1, self.LAYER_COUNT):\n",
    "            for i in range(0, self.LAYER_STRUCTURE[l-1]):\n",
    "                for j in range(1, self.LAYER_STRUCTURE[l]):\n",
    "                    self.weights[l][i][j] = self.weights[l][i][j] - (self.ETA * self.x[l-1][i] * self.d[l][j])\n",
    "\n",
    "    def calculate_e_in(self):\n",
    "        total_e_in = 0\n",
    "\n",
    "        for input_id, input in enumerate(self.training_inputs):\n",
    "            self.feed_forward(input)\n",
    "            \n",
    "            h = self.x[-1][1]\n",
    "            e_in = (h - self.training_outputs[input_id]) ** 2 # MSE measure\n",
    "            total_e_in += e_in\n",
    "        \n",
    "        return total_e_in / len(self.training_inputs)\n",
    "\n",
    "    def calculate_e_out(self):\n",
    "        total_e_out = 0\n",
    "\n",
    "        # QSTN: reuse the same out of sample?  Or generate fresh like we've been doing?\n",
    "        testing_inputs = self.generate_inputs(self.TESTING_INPUT_N)\n",
    "        testing_outputs = self.generate_outputs(testing_inputs)\n",
    "\n",
    "        for testing_input_id, testing_input in enumerate(testing_inputs):\n",
    "            self.feed_forward(testing_input)\n",
    "\n",
    "            h = self.x[-1][1]\n",
    "            e_out = (h - testing_outputs[testing_input_id]) ** 2 # MSE measure\n",
    "            total_e_out += e_out\n",
    "\n",
    "        return total_e_out / len(testing_inputs)\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "nn.sgd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "- Initialize $\\mathbf{w}(0)$\n",
    "- For $t=0,1,2, \\cdots \\quad$ [to termination]\n",
    "$$\n",
    "\\mathbf{w}(t+1)=\\mathbf{w}(t)-\\eta \\nabla E_{\\text {in }}(\\mathbf{w}(t))\n",
    "$$\n",
    "- Return final w\n",
    "\n",
    "With **Stochastic** gradient descent, we pick one $(x_n, y_n)$ at a time, applying Gradient Descent to $e(h(x_n), y_n)$.\n",
    "\n",
    "Rule of thumb: $\\eta = .1$ works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the Network Operates\n",
    "\n",
    "$$\n",
    "w_{i j}^{(l)} \\quad \\begin{cases}1 \\leq l \\leq L & \\text { layers } \\\\ 0 \\leq i \\leq d^{(l-1)} & \\text { inputs } \\\\ 1 \\leq j \\leq d^{(l)} & \\text { outputs }\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta(s)=\\tanh (s)=\\frac{e^s-e^{-s}}{e^s+e^{-s}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_j^{(l)}=\\theta\\left(s_j^{(l)}\\right)=\\theta\\left(\\sum_{i=0}^{d^{(l-1)}} w_{i j}^{(l)} x_i^{(l-1)}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text { Apply } \\mathbf{x} \\text { to } x_1^{(0)} \\cdots x_{d^{(0)}}^{(0)} \\rightarrow \\rightarrow x_1^{(L)}=h(\\mathbf{x})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying SGD\n",
    "\n",
    "Error on example $\\left(\\mathbf{x}_n, y_n\\right)$ is\n",
    "$$\n",
    "\\mathrm{e}\\left(h\\left(\\mathbf{x}_n\\right), y_n\\right)=\\mathrm{e}(\\mathrm{w})\n",
    "$$\n",
    "\n",
    "To implement SGD, we need the gradient: \n",
    "\n",
    "$$\n",
    "\\nabla \\mathrm{e}(\\mathbf{w}): \\frac{\\partial \\mathrm{e}(\\mathrm{w})}{\\partial w_{i j}^{(l)}} \\text { for all } i, j, l\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing $\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial w_{i j}^{(l)}}$\n",
    "\n",
    "A trick for efficient computation:\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial w_{i j}^{(l)}}=\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_j^{(l)}} \\times \\frac{\\partial s_j^{(l)}}{\\partial w_{i j}^{(l)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text { We have } \\frac{\\partial s_j^{(l)}}{\\partial w_{i j}^{(l)}}=x_i^{(l-1)} \\quad \\text { We only need: } \\frac{\\partial \\mathrm{e}(\\mathrm{w})}{\\partial s_j^{(l)}}=\\delta_j^{(l)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\delta$ for the final layer\n",
    "\n",
    "For the final layer $l=L$ and $j=1$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\delta_1^{(L)}=\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_1^{(L)}} \\\\\n",
    "& = \\frac{\\partial(\\theta(s_1^{(L)}) - y_n)^2}{\\partial s_1^{(L)}} \\\\\n",
    "& = 2(\\theta(s_1^{(L)}) - y_n)(\\theta^{\\prime}(s_1^{(L)})) \\\\\n",
    "& = 2(x_1^{(L)} - y_n)(\\theta^{\\prime}(s_1^{(L)})) \\\\\n",
    "& = 2(x_1^{(L)} - y_n)(1 - \\theta^2(s_1^{(L)})) \\\\\n",
    "& = 2(x_1^{(L)} - y_n)(1 - (x_1^{(L)})^2)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "[Waiting to verify this is correct]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back propagation of $\\delta$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta_i^{(l-1)} & =\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_i^{(l-1)}} \\\\\n",
    "& =\\sum_{j=1}^{d^{(l)}} \\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_j^{(l)}} \\times \\frac{\\partial s_j^{(l)}}{\\partial x_i^{(l-1)}} \\times \\frac{\\partial x_i^{(l-1)}}{\\partial s_i^{(l-1)}} \\\\\n",
    "& =\\sum_{j=1}^{d^{(l)}} \\delta_j^{(l)} \\times w_{i j}^{(l)} \\times \\theta^{\\prime}\\left(s_i^{(l-1)}\\right) \\\\\n",
    "\\delta_i^{(l-1)} & =\\left(1-\\left(x_i^{(l-1)}\\right)^2\\right) \\sum_{j=1}^{d^{(l)}} w_{i j}^{(l)} \\delta_j^{(l)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/network-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 1.49166815e-154, -2.51216048e-001, -8.05322200e-001,\n",
      "         5.05968650e-001])\n",
      " array([ 1.49166815e-154, -8.49652808e-002,  7.65426630e-002,\n",
      "         5.16151804e-001])\n",
      " array([ 1.49166815e-154,  8.82352199e-001,  8.07817532e-001,\n",
      "        -2.64982213e-001])                                   ]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 108\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39m# Our target function\u001b[39;00m\n\u001b[1;32m    106\u001b[0m target_function \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m \u001b[39mprint\u001b[39m(sgd(inputs, target_function))\n",
      "Cell \u001b[0;32mIn[99], line 74\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(inputs, target_function, eta, hidden_layer_structure)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(x[l])):\n\u001b[1;32m     73\u001b[0m         \u001b[39mprint\u001b[39m(weights[l])\n\u001b[0;32m---> 74\u001b[0m         \u001b[39mprint\u001b[39m(weights[l][:,j])\n\u001b[1;32m     75\u001b[0m         x[l][j] \u001b[39m=\u001b[39m ACTIVATION_FUNCTION(x[l \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdot(weights[l][:,j]))\n\u001b[1;32m     77\u001b[0m \u001b[39m# Testing\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "# Stochastic gradient descent with multilayer 2D perceptrons.  Takes an array of 2d points as inputs, a target function, and a value for eta, and runs sgd, ultimately returning the weights.\n",
    "DEFAULT_ETA = .1\n",
    "# QSTN: how many layers / neurons to implement?\n",
    "    # https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/\n",
    "    # \" an MLP with two hidden layers is sufficient for creating classification regions of any desired shape. This is instructive, although it should be noted that no indication of how many nodes to use in each layer\"\n",
    "\n",
    "# 2D input data is assumed (again ignoring the fixed bias input)\n",
    "INPUT_DIMENSION = 2\n",
    "\n",
    "HIDDEN_LAYER_STRUCTURE = [3, 2] # Two layers, with 3 neurons and 2 neurons respectively.  Note that these counts do not include the bias neurons, which are indexed at 0.  See the structure in the image above.\n",
    "\n",
    "ACTIVATION_FUNCTION = np.tanh\n",
    "\n",
    "# A single final output is assumed.\n",
    "\n",
    "#QSTN: initialize bias to 0?\n",
    "\n",
    "#TODO: this would be better as a Class, due to var sharing\n",
    "def sgd(inputs, target_function, eta = DEFAULT_ETA, hidden_layer_structure = HIDDEN_LAYER_STRUCTURE):\n",
    "    total_layers = len(HIDDEN_LAYER_STRUCTURE) + 2 # Input layer + hidden layers + output layer\n",
    "    # Initialize all neurons x with empty values\n",
    "    x = np.empty(total_layers, dtype=object)\n",
    "    x[0] = np.empty(INPUT_DIMENSION + 1) # Input neurons\n",
    "    # Hidden layer neurons\n",
    "    for l, neuron_count in enumerate(HIDDEN_LAYER_STRUCTURE):\n",
    "        l = l + 1\n",
    "        x[l] = np.empty(neuron_count + 1)\n",
    "    # Output neuron\n",
    "    x[-1] = np.empty(2) \n",
    "    x[-1][0] = None # The first entry will be ignored, this is to get 1-indexing.\n",
    "\n",
    "    # Initialize all weights w_{ij}^(l) with Xavier weight initialization..\n",
    "    weights = np.empty(total_layers, dtype=object)\n",
    "    \n",
    "    # Begin with hidden layers\n",
    "    prev_neuron_count = INPUT_DIMENSION + 1\n",
    "    for l, neuron_count in enumerate(HIDDEN_LAYER_STRUCTURE):\n",
    "        l = l + 1\n",
    "        weights[l] = np.empty(prev_neuron_count, dtype=object) \n",
    "        for i in range(prev_neuron_count):\n",
    "            weights[l][i] = np.empty(neuron_count + 1)\n",
    "            for j in range(1, neuron_count + 1):\n",
    "                # QSTN: do biases count toward n?  I'm assuming yes..?\n",
    "                weights[l][i][j] = xavier_weight(prev_neuron_count, neuron_count + 1)\n",
    "        prev_neuron_count = neuron_count + 1\n",
    "\n",
    "    # Set the weights on the final output layer\n",
    "    neuron_count = 1\n",
    "    weights[-1] = np.empty(prev_neuron_count, dtype=object)\n",
    "    for i in range(prev_neuron_count):\n",
    "        weights[-1][i] = np.empty(neuron_count + 1)\n",
    "        # QSTN: same question about bias.. here there is no bias whatsoever so seems a done deal.\n",
    "        weights[-1][i][1] = xavier_weight(prev_neuron_count, neuron_count) \n",
    "\n",
    "    epoch_count = 0 \n",
    "    #TODO: ultimately, will want something like out of sample validation to end the loop\n",
    "    while (epoch_count < 5):\n",
    "        # Generate a random permutation for how we pick the inputs\n",
    "        input_perm = np.random.permutation(len(inputs))\n",
    "        for input_id in input_perm:\n",
    "            input = inputs[input_id]\n",
    "\n",
    "            # Forward: Compute all x_j^(l)\n",
    "            # First we set layer 1 values\n",
    "            x[0] = input\n",
    "\n",
    "            # Now we forward propagate x\n",
    "            for l in range(1, total_layers):\n",
    "                for j in range(1, len(x[l])):\n",
    "                    print(weights[l])\n",
    "                    print(weights[l][:,j])\n",
    "                    x[l][j] = ACTIVATION_FUNCTION(x[l - 1].dot(weights[l][:,j]))\n",
    "\n",
    "            # Testing\n",
    "            return x\n",
    "\n",
    "            # Backward: Compute all d_j^(l)\n",
    "\n",
    "            # Update the weights: w_{ij}^(l) = w_{ij}^(l) - eta * x_i^(l-1) * d_j^(l)\n",
    "        # Report E_in and e_out for the epoch.  Report current weights too.\n",
    "        # Break the loop if it's time to stop [ might be done through an e_out target]\n",
    "        # QSTN: when is the right time to stop in SGD?  how do we identify that?\n",
    "            # Note, overfitting is a concern here https://stats.stackexchange.com/questions/433187/stopping-criteria-for-stochastic-gradient-descent , validation is part of how it's addressed.  Not sure how much I want to postpone my v1 over this..\n",
    "\n",
    "        #TODO: make sure to increment during epoch loop\n",
    "\n",
    "    # Return the weights\n",
    "\n",
    "# looking like https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/, Xavier weight initialization is a good approach. \n",
    "# QSTN: do biases count toward n?  I'm assuming yes..?\n",
    "def xavier_weight(prev_neuron_count, neuron_count):\n",
    "    return np.random.uniform(-np.sqrt(6)/np.sqrt(prev_neuron_count + neuron_count),np.sqrt(6)/np.sqrt(prev_neuron_count + neuron_count))\n",
    "\n",
    "\n",
    "# Our inputs\n",
    "n = 1000\n",
    "rng = np.random.default_rng()\n",
    "inputs = rng.uniform(-1,1,(n, 2))\n",
    "inputs = np.hstack((np.ones((n,1)), inputs)) # Set x_0 = 1 for all xs\n",
    "\n",
    "#TODO: set this up to generate a circle\n",
    "# Our target function\n",
    "target_function = None\n",
    "\n",
    "print(sgd(inputs, target_function))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

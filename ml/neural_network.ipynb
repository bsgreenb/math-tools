{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "- Initialize $\\mathbf{w}(0)$\n",
    "- For $t=0,1,2, \\cdots \\quad$ [to termination]\n",
    "$$\n",
    "\\mathbf{w}(t+1)=\\mathbf{w}(t)-\\eta \\nabla E_{\\text {in }}(\\mathbf{w}(t))\n",
    "$$\n",
    "- Return final w\n",
    "\n",
    "With **Stochastic** gradient descent, we pick one $(x_n, y_n)$ at a time, applying Gradient Descent to $e(h(x_n), y_n)$.\n",
    "\n",
    "Rule of thumb: $\\eta = .1$ works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the Network Operates\n",
    "\n",
    "$$\n",
    "w_{i j}^{(l)} \\quad \\begin{cases}1 \\leq l \\leq L & \\text { layers } \\\\ 0 \\leq i \\leq d^{(l-1)} & \\text { inputs } \\\\ 1 \\leq j \\leq d^{(l)} & \\text { outputs }\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta(s)=\\tanh (s)=\\frac{e^s-e^{-s}}{e^s+e^{-s}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_j^{(l)}=\\theta\\left(s_j^{(l)}\\right)=\\theta\\left(\\sum_{i=0}^{d^{(l-1)}} w_{i j}^{(l)} x_i^{(l-1)}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text { Apply } \\mathbf{x} \\text { to } x_1^{(0)} \\cdots x_{d^{(0)}}^{(0)} \\rightarrow \\rightarrow x_1^{(L)}=h(\\mathbf{x})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying SGD\n",
    "\n",
    "Error on example $\\left(\\mathbf{x}_n, y_n\\right)$ is\n",
    "$$\n",
    "\\mathrm{e}\\left(h\\left(\\mathbf{x}_n\\right), y_n\\right)=\\mathrm{e}(\\mathrm{w})\n",
    "$$\n",
    "\n",
    "To implement SGD, we need the gradient: \n",
    "\n",
    "$$\n",
    "\\nabla \\mathrm{e}(\\mathbf{w}): \\frac{\\partial \\mathrm{e}(\\mathrm{w})}{\\partial w_{i j}^{(l)}} \\text { for all } i, j, l\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing $\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial w_{i j}^{(l)}}$\n",
    "\n",
    "A trick for efficient computation:\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial w_{i j}^{(l)}}=\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_j^{(l)}} \\times \\frac{\\partial s_j^{(l)}}{\\partial w_{i j}^{(l)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text { We have } \\frac{\\partial s_j^{(l)}}{\\partial w_{i j}^{(l)}}=x_i^{(l-1)} \\quad \\text { We only need: } \\frac{\\partial \\mathrm{e}(\\mathrm{w})}{\\partial s_j^{(l)}}=\\delta_j^{(l)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\delta$ for the final layer\n",
    "\n",
    "For the final layer $l=L$ and $j=1$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\delta_1^{(L)}=\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_1^{(L)}} \\\\\n",
    "& = \\frac{\\partial(\\theta(s_1^{(L)}) - y_n)^2}{\\partial s_1^{(L)}} \\\\\n",
    "& = 2(\\theta(s_1^{(L)}) - y_n)(\\theta^{\\prime}(s_1^{(L)})) \\\\\n",
    "& = 2(x_1^{(L)} - y_n)(\\theta^{\\prime}(s_1^{(L)})) \\\\\n",
    "& = 2(x_1^{(L)} - y_n)(1 - \\theta^2(s_1^{(L)})) \\\\\n",
    "& = 2(x_1^{(L)} - y_n)(1 - (x_1^{(L)})^2)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back propagation of $\\delta$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta_i^{(l-1)} & =\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_i^{(l-1)}} \\\\\n",
    "& =\\sum_{j=1}^{d^{(l)}} \\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_j^{(l)}} \\times \\frac{\\partial s_j^{(l)}}{\\partial x_i^{(l-1)}} \\times \\frac{\\partial x_i^{(l-1)}}{\\partial s_i^{(l-1)}} \\\\\n",
    "& =\\sum_{j=1}^{d^{(l)}} \\delta_j^{(l)} \\times w_{i j}^{(l)} \\times \\theta^{\\prime}\\left(s_i^{(l-1)}\\right) \\\\\n",
    "\\delta_i^{(l-1)} & =\\left(1-\\left(x_i^{(l-1)}\\right)^2\\right) \\sum_{j=1}^{d^{(l)}} w_{i j}^{(l)} \\delta_j^{(l)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/network-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 1.        , -0.63618161,  0.69862521])\n",
      " array([ 1.        , -0.18743177,  0.18605569,  0.74468583])\n",
      " array([ 1.        , -0.51298849,  0.19902487])\n",
      " array([       nan, 0.06847504])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "# Stochastic gradient descent with multilayer 2D perceptrons.  Takes an array of 2d points as inputs, a target function, and a value for eta, and runs sgd, ultimately returning the weights.\n",
    "DEFAULT_ETA = .1\n",
    "\n",
    "# 2D input data is assumed (again ignoring the fixed bias input)\n",
    "INPUT_DIMENSION = 2\n",
    "\n",
    "# https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/\n",
    "    # \" an MLP with two hidden layers is sufficient for creating classification regions of any desired shape. This is instructive, although it should be noted that no indication of how many nodes to use in each layer\"\n",
    "HIDDEN_LAYER_STRUCTURE = [3, 2] # Two layers, with 3 neurons and 2 neurons respectively.  Note that these counts do not include the bias neurons, which are indexed at 0.  See the structure in the image above.\n",
    "\n",
    "ACTIVATION_FUNCTION = np.tanh\n",
    "\n",
    "# A single final output is assumed.\n",
    "\n",
    "#TODO: initialize bias to 0\n",
    "#TODO: bias shouldn't count towards xavier\n",
    "\n",
    "#TODO: this would be better as a Class, due to var sharing\n",
    "def sgd(inputs, target_function, eta = DEFAULT_ETA, hidden_layer_structure = HIDDEN_LAYER_STRUCTURE):\n",
    "    total_layers = len(HIDDEN_LAYER_STRUCTURE) + 2 # Input layer + hidden layers + output layer\n",
    "    # Initialize all neurons x with empty values\n",
    "    x = np.empty(total_layers, dtype=object)\n",
    "    x[0] = np.empty(INPUT_DIMENSION + 1) # Input neurons\n",
    "    # Hidden layer neurons\n",
    "    for l, neuron_count in enumerate(HIDDEN_LAYER_STRUCTURE):\n",
    "        l = l + 1\n",
    "        x[l] = np.empty(neuron_count + 1)\n",
    "        x[l][0] = 1\n",
    "    # Output neuron\n",
    "    x[-1] = np.empty(2) \n",
    "    x[-1][0] = None # The first entry will be ignored, this is to get 1-indexing.\n",
    "\n",
    "    # Initialize all weights w_{ij}^(l) with Xavier weight initialization..\n",
    "    weights = np.empty(total_layers, dtype=object)\n",
    "    \n",
    "    # Begin with hidden layers\n",
    "    prev_neuron_count = INPUT_DIMENSION + 1\n",
    "    for l, neuron_count in enumerate(HIDDEN_LAYER_STRUCTURE):\n",
    "        l = l + 1\n",
    "        weights[l] = np.empty((prev_neuron_count, neuron_count + 1)) \n",
    "        weights[l][0,:] = 0.0\n",
    "        for i in range(1, prev_neuron_count):\n",
    "            for j in range(1, neuron_count + 1):\n",
    "                # QSTN: do biases count toward n?  I'm assuming yes..?\n",
    "                weights[l][i][j] = xavier_weight(prev_neuron_count, neuron_count + 1)\n",
    "        prev_neuron_count = neuron_count + 1\n",
    "\n",
    "    # Set the weights on the final output layer\n",
    "    neuron_count = 1\n",
    "    weights[-1] = np.empty((prev_neuron_count, neuron_count + 1))\n",
    "    weights[-1][0,:] = 0.0\n",
    "    for i in range(1, prev_neuron_count):\n",
    "        # QSTN: same question about bias.. here there is no bias whatsoever so seems a done deal.\n",
    "        weights[-1][i][1] = xavier_weight(prev_neuron_count, neuron_count) \n",
    "\n",
    "    epoch_count = 0 \n",
    "    #TODO: ultimately, will want something like out of sample validation to end the loop\n",
    "    while (epoch_count < 5):\n",
    "        # Generate a random permutation for how we pick the inputs\n",
    "        input_perm = np.random.permutation(len(inputs))\n",
    "        for input_id in input_perm:\n",
    "            input = inputs[input_id]\n",
    "\n",
    "            # Forward: Compute all x_j^(l)\n",
    "            # First we set layer 1 values\n",
    "            x[0] = input\n",
    "\n",
    "            # Now we forward propagate x\n",
    "            # NOTE: this could be vectorized (but would have to address the empty value in weights)\n",
    "            for l in range(1, total_layers):\n",
    "                for j in range(1, len(x[l])):\n",
    "                    x[l][j] = ACTIVATION_FUNCTION(x[l - 1].dot(weights[l][:,j]))\n",
    "\n",
    "            # Testing\n",
    "            return x\n",
    "\n",
    "            # Backward: Compute all d_j^(l)\n",
    "\n",
    "            # Update the weights: w_{ij}^(l) = w_{ij}^(l) - eta * x_i^(l-1) * d_j^(l)\n",
    "        # Report E_in and e_out for the epoch.  Report current weights too.\n",
    "        # Break the loop if it's time to stop [ might be done through an e_out target]\n",
    "        # QSTN: when is the right time to stop in SGD?  how do we identify that?\n",
    "            # Note, overfitting is a concern here https://stats.stackexchange.com/questions/433187/stopping-criteria-for-stochastic-gradient-descent , validation is part of how it's addressed.  Not sure how much I want to postpone my v1 over this..\n",
    "\n",
    "        #TODO: make sure to increment during epoch loop\n",
    "\n",
    "    # Return the weights\n",
    "\n",
    "# looking like https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/, Xavier weight initialization is a good approach. \n",
    "# QSTN: do biases count toward n?  I'm assuming yes..?\n",
    "def xavier_weight(prev_neuron_count, neuron_count):\n",
    "    return np.random.uniform(-np.sqrt(6)/np.sqrt(prev_neuron_count + neuron_count),np.sqrt(6)/np.sqrt(prev_neuron_count + neuron_count))\n",
    "\n",
    "\n",
    "# Our inputs\n",
    "n = 1000\n",
    "rng = np.random.default_rng()\n",
    "inputs = rng.uniform(-1,1,(n, 2))\n",
    "inputs = np.hstack((np.ones((n,1)), inputs)) # Set x_0 = 1 for all xs\n",
    "\n",
    "#TODO: set this up to generate a circle\n",
    "# Our target function\n",
    "target_function = None\n",
    "\n",
    "print(sgd(inputs, target_function))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

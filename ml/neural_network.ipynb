{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "- Initialize $\\mathbf{w}(0)$\n",
    "- For $t=0,1,2, \\cdots \\quad$ [to termination]\n",
    "$$\n",
    "\\mathbf{w}(t+1)=\\mathbf{w}(t)-\\eta \\nabla E_{\\text {in }}(\\mathbf{w}(t))\n",
    "$$\n",
    "- Return final w\n",
    "\n",
    "With **Stochastic** gradient descent, we pick one $(x_n, y_n)$ at a time, applying Gradient Descent to $e(h(x_n), y_n)$.\n",
    "\n",
    "Rule of thumb: $\\eta = .1$ works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the Network Operates\n",
    "\n",
    "$$\n",
    "w_{i j}^{(l)} \\quad \\begin{cases}1 \\leq l \\leq L & \\text { layers } \\\\ 0 \\leq i \\leq d^{(l-1)} & \\text { inputs } \\\\ 1 \\leq j \\leq d^{(l)} & \\text { outputs }\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta(s)=\\tanh (s)=\\frac{e^s-e^{-s}}{e^s+e^{-s}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_j^{(l)}=\\theta\\left(s_j^{(l)}\\right)=\\theta\\left(\\sum_{i=0}^{d^{(l-1)}} w_{i j}^{(l)} x_i^{(l-1)}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text { Apply } \\mathbf{x} \\text { to } x_1^{(0)} \\cdots x_{d^{(0)}}^{(0)} \\rightarrow \\rightarrow x_1^{(L)}=h(\\mathbf{x})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying SGD\n",
    "\n",
    "Error on example $\\left(\\mathbf{x}_n, y_n\\right)$ is\n",
    "$$\n",
    "\\mathrm{e}\\left(h\\left(\\mathbf{x}_n\\right), y_n\\right)=\\mathrm{e}(\\mathrm{w})\n",
    "$$\n",
    "\n",
    "To implement SGD, we need the gradient: \n",
    "\n",
    "$$\n",
    "\\nabla \\mathrm{e}(\\mathbf{w}): \\frac{\\partial \\mathrm{e}(\\mathrm{w})}{\\partial w_{i j}^{(l)}} \\text { for all } i, j, l\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing $\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial w_{i j}^{(l)}}$\n",
    "\n",
    "A trick for efficient computation:\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial w_{i j}^{(l)}}=\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_j^{(l)}} \\times \\frac{\\partial s_j^{(l)}}{\\partial w_{i j}^{(l)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text { We have } \\frac{\\partial s_j^{(l)}}{\\partial w_{i j}^{(l)}}=x_i^{(l-1)} \\quad \\text { We only need: } \\frac{\\partial \\mathrm{e}(\\mathrm{w})}{\\partial s_j^{(l)}}=\\delta_j^{(l)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\delta$ for the final layer\n",
    "\n",
    "For the final layer $l=L$ and $j=1$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\delta_1^{(L)}=\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_1^{(L)}} \\\\\n",
    "& = \\frac{\\partial(\\theta(s_1^{(L)}) - y_n)^2}{\\partial s_1^{(L)}} \\\\\n",
    "& = 2(\\theta(s_1^{(L)}) - y_n)(\\theta^{\\prime}(s_1^{(L)})) \\\\\n",
    "& = 2(x_1^{(L)} - y_n)(\\theta^{\\prime}(s_1^{(L)})) \\\\\n",
    "& = 2(x_1^{(L)} - y_n)(1 - \\theta^2(s_1^{(L)})) \\\\\n",
    "& = 2(x_1^{(L)} - y_n)(1 - (x_1^{(L)})^2)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back propagation of $\\delta$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta_i^{(l-1)} & =\\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_i^{(l-1)}} \\\\\n",
    "& =\\sum_{j=1}^{d^{(l)}} \\frac{\\partial \\mathrm{e}(\\mathbf{w})}{\\partial s_j^{(l)}} \\times \\frac{\\partial s_j^{(l)}}{\\partial x_i^{(l-1)}} \\times \\frac{\\partial x_i^{(l-1)}}{\\partial s_i^{(l-1)}} \\\\\n",
    "& =\\sum_{j=1}^{d^{(l)}} \\delta_j^{(l)} \\times w_{i j}^{(l)} \\times \\theta^{\\prime}\\left(s_i^{(l-1)}\\right) \\\\\n",
    "\\delta_i^{(l-1)} & =\\left(1-\\left(x_i^{(l-1)}\\right)^2\\right) \\sum_{j=1}^{d^{(l)}} w_{i j}^{(l)} \\delta_j^{(l)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../images/network-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.        ]\n",
      " [-0.76807485 -0.75910856  0.154013    0.75824958]\n",
      " [ 0.02294212  0.14374331  0.6169705  -0.05720325]]\n",
      "[ 0.         -0.75910856  0.14374331]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Stochastic gradient descent with multilayer 2D perceptrons.  Takes an array of 2d points as inputs, a target function, and a value for eta, and runs sgd, ultimately returning the weights.\n",
    "DEFAULT_ETA = .1\n",
    "\n",
    "# https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/\n",
    "    # \" an MLP with two hidden layers is sufficient for creating classification regions of any desired shape. This is instructive, although it should be noted that no indication of how many nodes to use in each layer\"\n",
    "LAYER_STRUCTURE = [2, 3, 2, 1] # Two hidden layers, with 3 neurons and 2 neurons respectively.  Note that these counts do not include the bias neurons, which are indexed at 0.  See the structure in the image above.\n",
    "\n",
    "ACTIVATION_FUNCTION = np.tanh\n",
    "\n",
    "# A single final output is assumed.\n",
    "\n",
    "#TODO: this would be better as a Class, due to var sharing\n",
    "def sgd(inputs, target_function, eta = DEFAULT_ETA, layer_structure=LAYER_STRUCTURE):\n",
    "    total_layers = len(LAYER_STRUCTURE) # Input layer + hidden layers + output layer\n",
    "    # Initialize all neurons x with empty values\n",
    "    x = np.empty(total_layers, dtype=object)\n",
    "    for l, neuron_count in enumerate(LAYER_STRUCTURE):\n",
    "        x[l] = np.empty(neuron_count + 1)\n",
    "        if l < (len(LAYER_STRUCTURE) - 1):\n",
    "            x[l][0] = 1 # 1 for bias\n",
    "        else:\n",
    "            x[l][0] = None # No bias on the final layer.  A placeholder to get 1-based indexing of real neurons.\n",
    "\n",
    "    # Initialize all weights w_{ij}^(l) with Xavier weight initialization..\n",
    "    # Note that our weights include biases.\n",
    "    weights = np.empty(total_layers, dtype=object)\n",
    "    \n",
    "    # Begin with hidden layers\n",
    "    for l, neuron_count in enumerate(LAYER_STRUCTURE):\n",
    "        if l == 0:\n",
    "            prev_neuron_count = neuron_count\n",
    "            continue # skip input layer\n",
    "        weights[l] = np.empty((prev_neuron_count + 1, neuron_count + 1)) \n",
    "        weights[l][0,:] = 0.0 # initialize biases to 0\n",
    "        for i in range(1, prev_neuron_count + 1):\n",
    "            for j in range(1, neuron_count + 1):\n",
    "                weights[l][i][j] = xavier_weight(prev_neuron_count, neuron_count)\n",
    "        prev_neuron_count = neuron_count\n",
    "\n",
    "    epoch_count = 0 \n",
    "    #TODO: ultimately, will want something like out of sample validation to end the loop\n",
    "    while (epoch_count < 5):\n",
    "        # Generate a random permutation for how we pick the inputs\n",
    "        input_perm = np.random.permutation(len(inputs))\n",
    "        for input_id in input_perm:\n",
    "            input = inputs[input_id]\n",
    "\n",
    "            # Forward: Compute all x_j^(l)\n",
    "            # First we set layer 1 values\n",
    "            x[0] = input\n",
    "\n",
    "            # Now we forward propagate x\n",
    "            # NOTE: this could be vectorized (but would have to address the empty value in weights)\n",
    "            for l in range(1, total_layers):\n",
    "                for j in range(1, len(x[l])):\n",
    "                    x[l][j] = ACTIVATION_FUNCTION(x[l - 1].dot(weights[l][:,j]))\n",
    "\n",
    "            # Testing\n",
    "            return x\n",
    "\n",
    "            # Backward: Compute all d_j^(l)\n",
    "\n",
    "            # Update the weights: w_{ij}^(l) = w_{ij}^(l) - eta * x_i^(l-1) * d_j^(l)\n",
    "        # Report E_in and e_out for the epoch.  Report current weights too.\n",
    "        # Break the loop if it's time to stop [ might be done through an e_out target]\n",
    "        # QSTN: when is the right time to stop in SGD?  how do we identify that?\n",
    "            # Note, overfitting is a concern here https://stats.stackexchange.com/questions/433187/stopping-criteria-for-stochastic-gradient-descent , validation is part of how it's addressed.  Not sure how much I want to postpone my v1 over this..\n",
    "\n",
    "        #TODO: make sure to increment during epoch loop\n",
    "\n",
    "    # Return the weights\n",
    "\n",
    "# looking like https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/, Xavier weight initialization is a good approach. \n",
    "# Note: biases don't count towards count, so subtract them off before passing in\n",
    "def xavier_weight(prev_neuron_count, neuron_count):\n",
    "    return np.random.uniform(-np.sqrt(6)/np.sqrt(prev_neuron_count + neuron_count),np.sqrt(6)/np.sqrt(prev_neuron_count + neuron_count))\n",
    "\n",
    "\n",
    "# Our inputs\n",
    "n = 1000\n",
    "rng = np.random.default_rng()\n",
    "inputs = rng.uniform(-1,1,(n, 2))\n",
    "inputs = np.hstack((np.ones((n,1)), inputs)) # Set x_0 = 1 for all xs\n",
    "\n",
    "#TODO: set this up to generate a circle\n",
    "# Our target function\n",
    "target_function = None\n",
    "\n",
    "print(sgd(inputs, target_function))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

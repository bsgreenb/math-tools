{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72acbd94-93c6-4dd3-8984-78ff1547c5ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Sources\n",
    "\n",
    "- [Gilbert Strang’s Class - MIT Linear Algebra Fall 2011](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/resource-index/)\n",
    " - Uses Introduction to Linear Algebra, 5th Edition\n",
    "- [3 blue 1 brown vids - Essence of Linear Algebra](https://www.youtube.com/watch?v=LyGKycYT2v0&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=10)\n",
    "- May test myself on [Khan](https://www.khanacademy.org/math/linear-algebra) but not planning to use his videos between the textbook and the above vids\n",
    "- Going to distribute [Numpy](https://numpy.org/doc/stable/user/absolute_beginners.html) stuff as I go.  Taking notes separately on that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4c47c69",
   "metadata": {},
   "source": [
    "# Introduction to Vectors (1)\n",
    "Lectures:\n",
    "* [The Geometry of Linear Equations](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/the-geometry-of-linear-equations-1/) - Note, this also covers (2.1)\n",
    "* [An Overview of Linear Algebra](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/an-overview-of-linear-algebra-1/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "672b86a6",
   "metadata": {},
   "source": [
    "## Linear combinations (1.1)\n",
    "\n",
    "$cv + dw$ for linear combinations of vectors $v$ and $w$, where $c$ and $d$ are scalars."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d76a9a2a",
   "metadata": {},
   "source": [
    "\n",
    "## Lengths and Dot Products (1.2)\n",
    "The dot product of vectors $v = \\begin{bmatrix}1 \\\\ 2\\end{bmatrix}$ and $w = \\begin{bmatrix}4 \\\\ 5\\end{bmatrix}$ is $v \\cdot w = (1)(4) + (2)(5) = 4 + 10 = 14$.\n",
    "\n",
    "Some algebraic properties of the dot product:\n",
    "1. Commutative Property: For any two vectors $u$ and $v$, $u \\cdot $v = v \\cdot u$.\n",
    "2. Scalar Multiplication Property: For any two vectors $u$ and $v$ and any real number $c$, $(cu) \\cdot v = u \\cdot (cv) = c(u \\cdot v)$\n",
    "3. Distributive Property: For any 3 vectors $u$, $v$, and $w$, $u \\cdot (v+w) = u \\cdot v + u \\cdot w$.\n",
    "\n",
    "When you multiply two vectors and the dot product is zero, they are perpindicular.  More generally, the angle $\\theta$ between vectors $v$ and $w$ has:\n",
    "$$\n",
    "\\cos \\theta = \\frac{v \\cdot w}{||v||\\;||w||}\n",
    "$$\n",
    "\n",
    "The length $||v||$ of a vector is $\\sqrt{v \\cdot v}$. This follows from the pythagorean theorem.\n",
    "\n",
    "The **unit vector** is a vector with length 1. Divide any vector by its length to get a unit vector."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "132623cc",
   "metadata": {},
   "source": [
    "\n",
    "### Explanation of Angle Between Two Vectors\n",
    "\n",
    "The unit vector that makes an angle $\\theta$ with the x axis is $\\begin{bmatrix}\\cos \\theta \\\\ \\sin \\theta\\end{bmatrix}$, we can see this from the unit circle\n",
    "\n",
    "![image.png](images/unit-circle.png)\n",
    "\n",
    "Let's get a geometric understanding for the rule\n",
    "$$\n",
    "\\cos \\theta = \\frac{v \\cdot w}{||v||\\;||w||}\n",
    "$$\n",
    "\n",
    "Now suppose instead of forming $\\theta$ with the x axis, we have two unit vectors, $U$ and $u$, and they are both rotated from the x axis:\n",
    "\n",
    "![image.png](images/unit-vector-addition.png)\n",
    "\n",
    "$u \\cdot U$ would then be $\\cos{\\alpha}\\cos{\\beta} + \\sin{\\alpha}\\sin{\\beta}$. From the cosine angle addition rule in trignometry, this is equal to $\\cos(\\theta)$.\n",
    "\n",
    "So we have arrived at the preliminary rule that unit vectors $u$ and $U$ at angle $\\theta$ have:\n",
    "\n",
    "$$u \\cdot U = \\cos{\\theta}$$\n",
    "\n",
    "Combine this with our observation before that you can divide any vector by its length to get its unit vector, and we arrive at our **cosine formula** for any vectors $v$ and $w$ by just dividing their lengths:\n",
    "\n",
    "$$\n",
    "\\cos \\theta = \\frac{v \\cdot w}{||v||\\;||w||}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4867f5b",
   "metadata": {},
   "source": [
    "\n",
    "### Schwarz and Triangle Inequalities\n",
    "\n",
    "Because all cosines are between -1 and 1, it follows that the absolute value of the dot product, $|v \\cdot w|$, cannot exceed the product of the lengths, this is the **Schwarz Inequality**:\n",
    "\n",
    "$$|v \\cdot w| \\le ||v||\\: ||w||$$\n",
    "\n",
    "From the Schwarz Inequality [follows](https://math.stackexchange.com/a/91194) the **Triangle Inequality**:\n",
    "\n",
    "$$||u + v|| \\le ||u|| + ||v||$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a034db",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Independence and Dependence\n",
    "\n",
    "Vectors are **independent** if no combination other than 0 multiples gives $b=0$.  Vectors are **dependent** if multiple combinations give $b=0$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2be52981",
   "metadata": {},
   "source": [
    "## Matrices (1.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d2bb8b4",
   "metadata": {},
   "source": [
    "A matrix is **invertible** (aka **non-singular**) if it has independent (see definition above) column vectors, meaning $Ax = 0$ has only one solution between them.\n",
    "\n",
    "A matrix is **singular** if $Ax=0$ has many solutions, or none at all."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56e07f00",
   "metadata": {},
   "source": [
    "\n",
    "# Solving Linear Equations (2)\n",
    "\n",
    "* [Elemination with Matrices](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/elimination-with-matrices-1/) - Lecture covering 2.2 and 2.3\n",
    "## Vectors and Linear Equations (2.1)\n",
    "\n",
    "Geometrically, it's worth noting that the dot product of each row with $x$ gives the equation of a plane.\n",
    "When the number of unknowns matches the number of equations, there is _usually_ one solution.\n",
    "\n",
    "### Matrix, Row, and Column Pictures\n",
    "\n",
    "Lets say we have $n$ equations and $n$ unknowns, and go over:\n",
    "* Matrix Form\n",
    "* Row Picture\n",
    "* Column Picture\n",
    "\n",
    "Let's look specifically at these two equations with two unknowns:\n",
    "$$\n",
    "2x - y = 0 \\\\\n",
    "-x + 2y = 3\n",
    "$$\n",
    "\n",
    "In **matrix form**, with the **coefficient matrix**, followed by the unknowns matrix, equal to solutions/right hand side would be:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2 & -1 \\\\\n",
    "-1 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These three matrices are abstractly referred to as $Ax=b$. When we are solving for $x$ (the inverse), we are abstractly solving $x = A^{-1}b$. And note that only with an invertible matrix (see below) can we solve this.\n",
    "\n",
    "The **row picture** is looking at one equation at a time, it's what we've seen before with systems of equations, or looking for where lines meet when we graph them geometrically.\n",
    "\n",
    "The **column picture** would have us formulate the equations as combinations of the columns, so:\n",
    "\n",
    "$$\n",
    "x \n",
    "\\begin{bmatrix}\n",
    "2 \\\\\n",
    "-1\n",
    "\\end{bmatrix}\n",
    "+ \n",
    "y\n",
    "\\begin{bmatrix}\n",
    "-1 \\\\\n",
    "2\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Geometrically, the column picture can solve these linear equations through vector addition, which we know geometrically means combining the column vectors each a certain number of times to produce the right hand side.\n",
    "\n",
    "### The Identity Matrix\n",
    "\n",
    "Multiplying $Ix$ where $I$ is the identity matrix, you get back the x you started with, $Ix=x$.  An example 3x3 identity matrix:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63361923",
   "metadata": {},
   "source": [
    "## The Idea of Elimination (2.2)\n",
    "\n",
    "**Elimination** is the systematic way of solving linear equations. Elimination proceeds by producing an **upper triangular system** from top to bottom, then solving with **back substitution** from the bottom up.\n",
    "\n",
    "In the first part, where you're producing the upper triangular system, you subtract a multiple of the above equation from the equation below.  This **multiplier** ($l$) is determined from the **pivot** above.  For example, if we have\n",
    "\n",
    "$$\n",
    "4x - 8y = 4 \\\\\n",
    "3x + 2y = 11\n",
    "$$\n",
    "\n",
    "Our multiplier of the first equation would be $l=\\frac{3}{4}$, and we'd then subtract that multiplied equation from the 2nd.  We'd then be left with the pivot of $8$ at the bottom right.  To solve $n$ equations we want $n$ pivots.  If there were a 3rd equation we'd use the $8$ pivot to determine our next multiplier and subtract, and so on.\n",
    "\n",
    "### The breakdown of elimination\n",
    "\n",
    "It's possible for the process of elimination to fail along the way.  Specifically, we might reach a 0 pivot.  In this case, we may be able to rescue this with row exchange, or may not be able to.  It may be that the 0 pivot:\n",
    "\n",
    "* Implies no solutions (e.g. $0y=8$).  Geometrically this would be non-intersecting lines. OR \n",
    "* It may be that it arrives at infinite solutions (e.g. $0y=0$). Geometrically this would be represented by more than one intersection, e.g. two identical lines.\n",
    "* It may be that a row exchange can rescue things, for example:\n",
    "\n",
    "$$\n",
    "0x + 2y = 4 \\\\\n",
    "3x - 2y = 5\n",
    "$$\n",
    "\n",
    "Here would just want to perform **row substitution** to get a triangular system we could then back-substitute on.\n",
    "\n",
    "Recall our terminology from earlier on, when we can complete elimination, we are dealing with a non-singular matrix, whereas the no solutions or infinite solutions cases are singular.\n",
    "\n",
    "### Extending into 3+ equations\n",
    "\n",
    "The process involves clear out columns below the pivots, using multipliers of that pivot, before moving onto the next pivot.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e25d296b",
   "metadata": {},
   "source": [
    "## Elimination Using Matrices (2.3)\n",
    "\n",
    "**Elimination matrices** execute our elimination steps.  An elimination matrix $E_{ij}$ eliminates row $i$, column $j$ by multiplying the $j$th equation by $l_{ij}$ and subtracting it from the $i$th equation.  So for example $E_{21}$ would would be the first elimination step, clearing out row 2, column 1.\n",
    "\n",
    "We need a lot of these $E_{ij}$ matrices to complete elimination, which is why we'll later see they can be combined into one big matrix $E$.  The neatest way to do that is by combining all their inverses $(E_{ij})^{-1}$ into one overall matrix $L = E^{-1}$.  \n",
    "\n",
    "The special property of $L$ is that all the multipliers $l_{ij}$ fall into place.  Those numbers are mixed up in $E$ (forward elimination from A to U).  Inverting puts the steps and their elimination matrices in the opposite order and prevents the mixup.\n",
    "\n",
    "### The Matrix Form of One Elimination Step\n",
    "\n",
    "Suppose we want to subtract two times row 1 from row 2.  The elimination matrix for this step would be:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "-2 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The first and third rows come from the identity matrix $I$. The $-2$ comes from the negative of the multiplier $l$ (2).\n",
    "\n",
    "### Matrix Multiplication\n",
    "\n",
    "Via [MathIsFun](https://www.mathsisfun.com/algebra/matrix-multiplying.html):\n",
    "\n",
    "![image.svg](images/matrix-multiply.svg)\n",
    "\n",
    "It works through the dot product of each row and column.\n",
    "\n",
    "In order to multiply two matrices, the number of columns of A must equal the number of rows of B. The product\n",
    "AB will have the same number of rows as the first matrix and the same number of columns as the second.\n",
    "\n",
    "Algebraic rules for matrix multiplication:\n",
    "* Associative Law is true: $A(BC) = (AB)C$\n",
    "* Commutative Law is false: Often $AB \\ne BA$\n",
    "\n",
    "A note on matrix multiplication order.  When we multiply on the left side vs right side, it's the difference between acting on rows vs columns, which switches based on order.  Multiplying from the left, we're doing row operations.  Multiplying from the right, we're doing column operations.\n",
    "\n",
    "3Blue1Brown emphasized:\n",
    "- Viewing Matrices as transformation of space\n",
    "- Matrix multiplication is just one transformation after another [this may belong in subsequent section]\n",
    "\n",
    "### The Row Exchange Matrix\n",
    "To exchange aka permute rows we use another matrix $P_{ij}$ called the **permutation matrix**.  For example, the permutation matrix $P_{23}$ exchanges rows 2 and 3:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Permutation matrices can swap multiple rows as well, not just one.  We'll see that soon.\n",
    "\n",
    "### The Augmented Matrix\n",
    "\n",
    "We can augment the matrix $A$ in $Ax=b$ to include $b$ as an extra column, and allow it to change through the process of elimination.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62a3c076",
   "metadata": {},
   "source": [
    "## Rules for Matrix Operations (2.4)\n",
    "\n",
    "- Lecture for 2.4 and 2.5: [Multiplication and Inverse Matrices](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/multiplication-and-inverse-matrices/)\n",
    "\n",
    "A matrix with $n$ columns can multiply a matrix with $n$ rows:\n",
    " \n",
    "$$A_{m \\times n}B_{n \\times p} = C_{m \\times p}$$\n",
    "\n",
    "\n",
    "### Multiple ways to multiply matrices\n",
    "\n",
    "1. We went over the typical dot product way of multiplying matrices above, where the entry in row $i$, and column $j$ of $AB$ is (row $i$ of $A$) $\\cdot$ (column $j$ of $B$).\n",
    "\n",
    "Terminology note: A row times a column (a dot product) is also called an **inner product**.  A column times a row is called an **outer product**.\n",
    "\n",
    "Now let's talk about additional ways to multiply matrices..\n",
    "\n",
    "2. Matrix $A$ times every column of $B$: $A\\begin{bmatrix}b_1 \\cdots b_p \\end{bmatrix} = A\\begin{bmatrix}Ab_1 \\cdots Ab_p \\end{bmatrix}$.  Recall from the column picture perspective, that we can therefore see each column of $AB$ as a combination of columns of $A$.\n",
    "\n",
    "3. Every row of matrix $A$ times matrix $B$: \n",
    "$\\begin{bmatrix} \\text{row }i\\text{ of }A\\end{bmatrix}B = \\begin{bmatrix}\\text{row }i \\text{ of }AB\\end{bmatrix}.$\n",
    "\n",
    "4. Multiply columns $1$ to $n$ of $A$ times rows $1$ to $n$ of $B$. Add those matrices. So for example:\n",
    "$$\n",
    "AB = \\begin{bmatrix}a \\\\ c\\end{bmatrix}\\begin{bmatrix}E & F\\end{bmatrix} + \\begin{bmatrix}b \\\\ d\\end{bmatrix}\\begin{bmatrix}G & H\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "You'll find that it works out just like the other methods.\n",
    "\n",
    "### Blocks\n",
    "\n",
    "Matrices can be added and multiplied by **blocks**, so long as the block sizes correspond to the normal rules-- same size for addition, and rows of 1 = cols of 2 for multiplication. \n",
    "\n",
    "Important: Cuts between columns of $A$ must match cuts between rows of $B$.\n",
    "\n",
    "Matrix block multiplication example:\n",
    "\n",
    "$A = \\begin{bmatrix}A_1 & A_2\\end{bmatrix}$ times $B = \\begin{bmatrix}B_1 \\\\ B_2\\end{bmatrix}$ is $A_{1}B_1 + A_{2}B_2$.\n",
    "\n",
    "The blocks must be equal across transposition, so for example you could have:\n",
    "* Two square matrices split up with each corner a block\n",
    "* Block columns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbcd6dc0",
   "metadata": {},
   "source": [
    "## Inverse Matrices (2.5)\n",
    "\n",
    "If the square matrix $A$ has an inverse, then both $A^{-1}A = I$ and $A^{-1}A = I$.  Note that non-square matrices are not invertible.\n",
    "\n",
    "Testing for invertibility:\n",
    "\n",
    "- The _algorithm_ to test invertibility is elimination. $A$ must have $n$ (nonzero) pivots\n",
    "- The _algebra_ test for invertibility is the determinant of $A$. $\\det A$ must not be $0$.\n",
    "- The _equation_ that test for invertibility is $Ax = 0$.  $x = 0$ must be the only solution.\n",
    "\n",
    "A matrix cannot have more than one inverse.  If you found the left-inverse, it must be the same as the right-inverse.\n",
    "\n",
    "### The Inverse of a Product AB\n",
    "\n",
    "If $A$ and $B$ are invertible, then so is $AB$:\n",
    "\n",
    "$$(AB)^{-1} = B^{-1}A^{-1}$$\n",
    "\n",
    "### Gauss-Jordan Elimination\n",
    "Gauss-Jordan eliminates $\\begin{bmatrix}A & I\\end{bmatrix}$ to $\\begin{bmatrix}I & A^{-1}\\end{bmatrix}$.\n",
    "\n",
    "The Gauss-Jordan method is to begin with that augmented matrix, $\\begin{bmatrix}A & I\\end{bmatrix}$, and performing elimination until you get the left block upper triangular.  Then, continue doing elimination upwards, so that you have only a diagonal of pivots on the left.  Finally, divide each row to get **reduced echelon form** ($R=I$) on the left hand side.  Then your inverse will be on the right hand side.\n",
    "\n",
    "This helps explain why the determininant can't be 0 for a matrix with an inverse, you have to divde by the pivots, and you can't divide by 0.\n",
    "\n",
    "**Diagonally dominant** matrices are invertible.  If the absolute value of the diagonal entries are larger than the sum of the absolute values of the rest of their rows, then the matrix is invertible.  This follows from the fact that the other row entires cannot add up to equal those entries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abfb2997",
   "metadata": {},
   "source": [
    "## Elimination = Factorization: A = LU (2.6)\n",
    "Lecture: [Factorization into A=LU](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/factorization-into-a-lu-1/)\n",
    "\n",
    "In the previous section, we went from $A$ to $U$ with elimination.  In this section, we look at elimination in the most useful way.\n",
    "\n",
    "Many key ideas of linear algebra, when you look at them closely, are really _factorizations_ of a matrix. The first factorization we look at comes from elimination.  The factors $L$ and $U$ are triangular matrices. The factorization that comes from elimination is $A=LU$.\n",
    "\n",
    "We already know about $U$, the upper triangular matrix, from producing it during elimination. Reversing those steps, taking $U$ back to $A$ is achieved by a lower triangular $L$.\n",
    "\n",
    "Each elimination step $E_{ij}$ is inverted by $L_{ij}$. The entries of $L$ are exactly the multipliers $l_{ij}$. Every multiplier $l_{ij}$ is in row $i$, column $j$ of $L$.\n",
    "\n",
    "Here's a 2x2 example going forward from $A$ to $U$, then back from $U$ to $A$:\n",
    "\n",
    "$$\n",
    "E_{21}A = \\begin{bmatrix}1 & 0 \\\\ -3 & 1\\end{bmatrix}\\begin{bmatrix}2 & 1 \\\\ 6 & 8\\end{bmatrix} = \\begin{bmatrix}2 & 1 \\\\ 0 & 5\\end{bmatrix} = U \\\\\n",
    "E_{21}^{-1} = \\begin{bmatrix}1 & 0 \\\\ 3 & 1\\end{bmatrix}\\begin{bmatrix}2 & 1 \\\\ 0 & 5\\end{bmatrix} = \\begin{bmatrix}2 & 1 \\\\ 6 & 8\\end{bmatrix} = A\n",
    "$$\n",
    "\n",
    "The second line is our factorization $LU=A$. The whole forward elimination process (with no row exchanges) is inverted by $L$.  Just as $E$ is all eliminations, $L$ is all the inverse eliminations.\n",
    "\n",
    "### Predicting zeroes in L and U\n",
    "\n",
    "We can predict the zeroes in $L$ and $U$ from $A$:\n",
    "\n",
    "- When a row of $A$ starts with zeroes, so does that row of $L$\n",
    "- When a column of $A$ starts with zeroes, so does that column of $U$\n",
    "\n",
    "But note that zeros in the middle of the matrix are likely to be filled in, while elimination sweeps forward.\n",
    "\n",
    "### Better balance from LDU\n",
    "\n",
    "$A=LU$ is not \"symmetric\" in that $A$ has 1s on its pivots while $U$ does not.  This is easy to fix.  Divide $U$ by a diagonal matrix $D$ that contains the pivots. That leaves a new triangular matrix with 1's on the diagonal. E.g:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}1 & 0 \\\\ 3 & 1\\end{bmatrix}\\begin{bmatrix}2 & 8 \\\\ 0 & 5\\end{bmatrix} \\text{ splits further into }\n",
    "\\begin{bmatrix}1 & 0 \\\\ 3 & 1\\end{bmatrix}\\begin{bmatrix}2 & 0 \\\\ 0 & 5\\end{bmatrix}\\begin{bmatrix}1 & 4 \\\\ 0 &1\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### How expensive is elimination\n",
    "\n",
    "The first stage of elimination produces zeros below the first pivot in column 1. To find each entry below the pivot requires one multiplication and one subtraction. We count this first stage as $n^2$ multiplications and $n^2$ subtractions. It is actually less ($n^2 -n$) because row 1 doesn't change.\n",
    "\n",
    "The next stage clears out the second column below the second pivot. The working matrix is now of size $n-1$. We estimate this stage as $(n-1)^2$ multiplications and subtractions.\n",
    "\n",
    "The rough count to reach $U$ is the sum of squares $n^2 + (n-1)^2 + \\cdots + 2^2 + 1^2$. There is an exact formula ([proofs here](https://math.stackexchange.com/questions/48080/sum-of-first-n-squares-equals-fracnn12n16)) $\\frac{1}{3}n(n+\\frac{1}{2})(n + 1)$ for this sum of squares. For considering the cost/complexity here, we can just pay attention to the largest term, and say:\n",
    "\n",
    "Elimination on A requires about $\\frac{1}{3}n^3$ multiplications and $\\frac{1}{3}n^3$ subtractions.\n",
    "\n",
    "What about the right side? Going forward, we subtract multiple of $b_1$ from the components below. This is $n-1$ steps. The second stage takes only $n-2$ steps, because $b_1$ is not involved. The last stage of forward elimination takes one step.\n",
    "\n",
    "Then, for back substitution, $x_n$ takes one step (divide by the last pivot).  The next unknown takes two steps. When we reach $x_1$ it will require $n$ steps ($n-1$ substitutions of the other unknowns, then division by the first pivot). \n",
    "\n",
    "The total count on the right side, from $b$ to $c$ to $x$, forward and backward, is therefore exactly $n^2$, which we can see from:\n",
    "\n",
    "$$\n",
    "[(n - 1) + (n - 2) + \\cdots 1] + [1 + 2 + \\cdots + (n-1) + n] = n^2\n",
    "$$\n",
    "\n",
    "So the right side takes $n^2$ multiplications and $n^2$ subtractions in total."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67d9e6c6",
   "metadata": {},
   "source": [
    "## Transposes and Permutations (2.7)\n",
    "\n",
    "[Lecture - Transposes, Permutations, Vector Spaces](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/transposes-permutations-vector-spaces-1/).  This lecture covers Vector Spaces as well.\n",
    "\n",
    "The **transpose** of $A$ is denoted by $A^T$. The columns of $A^T$ are the rows of $A$:\n",
    "\n",
    "$$(A^T)_{ij} = A_{ji}$$\n",
    "\n",
    "When $A$ is an $m$ by $n$ matrix, the transpose is $n$ by $m$:\n",
    "\n",
    "$$\n",
    "\\text{If }A=\\begin{bmatrix}1 & 2 & 3 \\\\ 0 & 0 & 4\\end{bmatrix} \\text{ then } A^T = \\begin{bmatrix}1 & 0 \\\\ 2 & 0 \\\\ 3 & 4\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The matrix \"flips over\" its main diagonal.\n",
    "\n",
    "### Rules of transposes\n",
    "\n",
    "- Sum: The transpose of $A + B$ is $A^T + B^T$\n",
    "- Product: The transpose of $AB$ is $(AB)^T = B^TA^T$\n",
    "- Inverse: The transpose of $A^{-1}$ is $(A^{-1})^T = (A^T)^{-1}$\n",
    "\n",
    "Notice how $B^TA^T$ comes in reverse order. This follows from how matrix multiplication works.  We get back to the same operations by transposing and flipping the order.\n",
    "\n",
    "The reverse order rule applies to three or more factors, so $(ABC)^T = C^TB^TA^T$.\n",
    "\n",
    "Now let's prove the inverse rule.  Start with $A^{-1}A = I$.  Apply the product rule above, and we get $A^T(A^{-1})^T = I$.  This shows that $(A^{-1})^T = (A^T)^{-1}$. It also follows that $A^T$ is invertible exactly when $A$ is invertible.\n",
    "\n",
    "[there's a section on The meaning of inner products which i dont grok, but maybe will after watching the lecture video.]\n",
    "\n",
    "### Symmetric Matrices\n",
    "\n",
    "For a symmetric matrix, transposing $A$ to $A^T$ produces no change. A symmetric matrix has $S^T = S$, meaning $S_{ji} = S_{ij}$.\n",
    "\n",
    "The inverse of a symmetric matrix is also symmetric, so $(S^{-1})^T = (S^T)^{-1} = S^{-1}$.\n",
    "\n",
    "The product of a matrix and its transpose will always be symmetric.  $A^TA$ is always symmetric.  We can see why this is true from the transpose equaling itself, which is our definition of symmetric: $(AA^T)^T = A^{TT}A^T = AA^T$.\n",
    "\n",
    "### Symmetric Products\n",
    "\n",
    "It follows fromt the product rule that the transpose of $A^TA$ is $A^T(A^T)^T$ which is $A^TA$ again.  \n",
    "\n",
    "Also, a symmetric invertible matrix will have a symmetric factorization, simpler than $S = LDU$, it will have $S=LDL^T$.\n",
    "\n",
    "### Permutation Matrices\n",
    "\n",
    "The transpose plays a special role for a **permutation matrix**.  This matrix P has a single \"1\" in every row and column.  \n",
    "\n",
    "Then $P^T$ is also a permutation matrix, maybe the same as $P$ or maybe different.  Any product $P_1P_2$ is again a permutation matrix.\n",
    "\n",
    "The simplest permutation matrix is $P = I$ (no exchanges).  The next simplest are the row exchanges $P_{ij}$. Other permutations reorder more rows.  By doing all possible row exchanges to $I$, we get all possible permutation matrices.  There are 6 3x3 permtuation matrices:\n",
    "\n",
    "$$\n",
    "\\;\\;I=\\begin{bmatrix}\n",
    "1 & & \\\\\n",
    "& 1 & \\\\\n",
    "& & 1\n",
    "\\end{bmatrix} \\quad \n",
    "P_{21}=\\begin{bmatrix}\n",
    "& 1 & \\\\\n",
    "1 & & \\\\\n",
    "& & 1\n",
    "\\end{bmatrix} \\quad \n",
    "P_{32}P_{21}=\\begin{bmatrix}\n",
    "& 1 & \\\\\n",
    "& & 1 \\\\\n",
    "1 & &\n",
    "\\end{bmatrix} \\\\ \\\\\n",
    "\n",
    "P_{31}=\\begin{bmatrix}\n",
    "& & 1 \\\\\n",
    "& 1 & \\\\\n",
    "1 & &\n",
    "\\end{bmatrix} \\quad \n",
    "P_{32}=\\begin{bmatrix}\n",
    "1 & & \\\\\n",
    "& & 1 \\\\\n",
    "& 1 &\n",
    "\\end{bmatrix} \\quad \n",
    "P_{21}P_{32}=\\begin{bmatrix}\n",
    "& & 1 \\\\\n",
    "1 & & \\\\\n",
    "& 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "There are $n!$ permutation matrices of order $n$.\n",
    "\n",
    "$P^{-1}$ is also a permutation matrix.  Among the P's displayed above, the four matrices on the left are their own inverses.  The two matrices on the right are inverses of each other.  In all cases, a single row exchange is its own inverse.\n",
    "\n",
    "$P^{-1}$ is always the same as $P^T$. So $PP^T = I$.\n",
    "\n",
    "Permutations (row exchanges before elimination) lead to $PA = LU$.\n",
    "\n",
    "### The PA = LU Factorization with Row Exchanges\n",
    "\n",
    "There are multiple ways we could approach permutations during elimination. \n",
    "\n",
    "1. We could do row exchanges in advance.  Then $PA=LU$.\n",
    "2. If we hold row exchanges until after elimination, the pivot rows are in a strange order.  Then $A= LPU$\n",
    "\n",
    "We will focus on the 1st one for our work; it's also the way computers do it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fceefd4",
   "metadata": {},
   "source": [
    "# Vector Spaces and Subspaces (3)\n",
    "## Spaces of Vectors (3.1)\n",
    "* First lecture on it [starts here](https://youtu.be/JibVXBElKL0?t=1246), the second part of the previous lecture\n",
    "* Second lecture on it is [first part here](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/column-space-and-nullspace-1/)\n",
    "\n",
    "The space $R^n$ consists of all column vectors $v$ with $n$ components.\n",
    "\n",
    "The components of $v$ are real numbers, which is the reason for the letter $R$.  A vector with complex numbers lies in the space $C^n$.\n",
    "\n",
    "We can add vectors and multiply by scalars (produce linear combinations) in a space, and the results remain in the space.  Every vector space must include the zero vector.\n",
    "\n",
    "The smallest possible vector space is $Z$, which only includes the zero vector.  Each space has its own zero vector.\n",
    "\n",
    "### Subspaces\n",
    "\n",
    "There are important vector spaces inside $R^n$.  Those are the **subspaces** of $R^n$.\n",
    "\n",
    "An example is a plane through the origin of $R^3$. That plane is a vector space in its own right. If we add two vectors in the plane, their sum is in the plane. If we multiply an in-plane vector by 2 or -5, it is still in the plane.\n",
    "\n",
    "Formal definition: A subspace of a vector space is a set of vectors (including 0) that satisfies two requirements.  If $v$ and $w$ are vectors in the subspace and $c$ is any scalar:\n",
    "1. $v + w$ is in the subspace\n",
    "2. $cv$ is in the subspace\n",
    "\n",
    "Or, to compress both rules into one: a subspace containing $v$ and $w$ must contain all linear combinations $cv + dw$.\n",
    "\n",
    "So a plane that doesn't go through the origin would fail that definition.\n",
    "\n",
    "Note that vector spaces count as subspaces, so $R^3$ etc. are subspaces.  They are subspaces of themselves, really.  Here is a list of all the possible subspaces of $R^3$:\n",
    "- Any line through $(0,0,0)$\n",
    "- The whole space ($R^3$)\n",
    "- Any plane through $(0,0,0)$\n",
    "- The zero vector $(0,0,0)$.\n",
    "\n",
    "An upper quadrant line would not be a subspace, because you could multiply by -1 and end up outside of the subspace.\n",
    "\n",
    "The union of two subspaces will only be a subspace if one of the subspaces contains the other.  The intersection of two subspaces will always be a subspace.\n",
    "\n",
    "### The Column Space of A\n",
    "\n",
    "The **column space** consists of all linear combinations of the columns (all possible $b$s in $Ax=b$). They fill the column space $C(A)$.\n",
    "\n",
    "The system $Ax=b$ is solvable if and only if $b$ is in the column space of $A$.\n",
    "\n",
    "Suppose we have an $m$ by $n$ matrix. The columns belong to $R^m$.  The column space of $A$ is a subspace of $R^m$.\n",
    "\n",
    "The set of all column combinations $Ax$ satisfies the rules for a subspace: when we add linear combinations of them, we still produce combinations of the columns.\n",
    "\n",
    "Instead of columns of $R^n$ we could start with any set $S$ of vectors in a vectors space $V$.  To get a subspace $SS$ of $V$, we take all combinations of the vectors in that set.\n",
    "\n",
    "So column space is an example of a span, of the column vectors.  The columns there \"span\" the column space.\n",
    "\n",
    "The subspace $SS$ is the span of $S$, containing all combinations of vectors in $S$.\n",
    "\n",
    "### 8 rules of vector spaces\n",
    "These rules are also covered [on Wolfram](https://mathworld.wolfram.com/VectorSpace.html).\n",
    "\n",
    "In the definition of a vector space, vector addition $X + Y$ and scalar multiplication $cx$ must oby the following 8 rules:\n",
    "\n",
    "1. $X + Y = Y + X$ (Commutativity of vector addition)\n",
    "2. $X + (Y + Z) = (X + Y) + Z$ (Associativity of vector addition)\n",
    "3. There is a unique zero vector, such that $X + 0 = X$ for all $X$ (Additive identity)\n",
    "4. For each $X$ there is a unique vector $-X$ such that $X + (-X) = 0$ (Existence of additive inverse)\n",
    "5. $1$ times $X$ equals $X$ (Scalar multiplication identity)\n",
    "6. $(c_1c_2)X = c_1(c_2X)$ (Associativity of scalar multiplications)\n",
    "7. $c(X + Y) = cX + cY$ (Distributivity of vector sums)\n",
    "8. $(c_1 + c_2)X = c_1X + c_2X$ (Distributivity of scalar sums)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "334ba622",
   "metadata": {},
   "source": [
    "## The Nullspace of A: Solving Ax = 0 and Rx = 0 \n",
    "* Lecture [begins here](https://youtu.be/8o5Cmfpeo6g?t=1677)\n",
    "\n",
    "The *nullspace*, denoted $N(A)$, consists of all solutions to $Ax=0$.  These vectors $x$ are in $R^n$. \n",
    "\n",
    "For invertible matrices, $x=0$ is the only slution to $Ax=0$. For noninvertible matrices, there are non-zero solutions to $Ax=0$.  Each solution $x$ belongs to the nullspace of $A$.\n",
    "\n",
    "[QSTN: Why is nullspace always a subspace?]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a72789",
   "metadata": {},
   "source": [
    "\n",
    "## The Complete Solution to Ax = b\n",
    "## Independence, Basis and Dimension\n",
    "## Dimensions of the Four Subspaces\n",
    "# Orthogonality \n",
    "## Orthogonality of the Four Subspaces \n",
    "## Projections \n",
    "## Least Squares Approximations \n",
    "## Orthonormal Bases and Gram-Schmidt \n",
    "# Determinants \n",
    "## The Properties of Determinants \n",
    "## Permutations and Cofactors \n",
    "## Cramer’s Rule, Inverses, and Volumes \n",
    "# Eigenvalues and Eigenvectors \n",
    "## Introduction to Eigenvalues \n",
    "## Diagonalizing a Matrix \n",
    "## Systems of Differential Equations \n",
    "## Symmetric Matrices \n",
    "## Positive Definite Matrices "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb58bdd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72acbd94-93c6-4dd3-8984-78ff1547c5ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Sources\n",
    "\n",
    "- [Gilbert Strangâ€™s Class - MIT Linear Algebra Fall 2011](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/resource-index/)\n",
    " - Uses Introduction to Linear Algebra, 5th Edition\n",
    "- [3 blue 1 brown vids - Essence of Linear Algebra](https://www.youtube.com/watch?v=LyGKycYT2v0&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=10)\n",
    "- May test myself on [Khan](https://www.khanacademy.org/math/linear-algebra) but not planning to use his videos between the textbook and the above vids\n",
    "- Going to distribute [Numpy](https://numpy.org/doc/stable/user/absolute_beginners.html) stuff as I go.  Taking notes separately on that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4c47c69",
   "metadata": {},
   "source": [
    "# Introduction to Vectors (1)\n",
    "Lectures:\n",
    "* [The Geometry of Linear Equations](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/the-geometry-of-linear-equations-1/) - Note, this also covers (2.1)\n",
    "* [An Overview of Linear Algebra](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/an-overview-of-linear-algebra-1/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "672b86a6",
   "metadata": {},
   "source": [
    "## Linear combinations (1.1)\n",
    "\n",
    "$cv + dw$ for linear combinations of vectors $v$ and $w$, where $c$ and $d$ are scalars."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d76a9a2a",
   "metadata": {},
   "source": [
    "\n",
    "## Lengths and Dot Products (1.2)\n",
    "The dot product of vectors $v = \\begin{bmatrix}1 \\\\ 2\\end{bmatrix}$ and $w = \\begin{bmatrix}4 \\\\ 5\\end{bmatrix}$ is $v \\cdot w = (1)(4) + (2)(5) = 4 + 10 = 14$.\n",
    "\n",
    "Some algebraic properties of the dot product:\n",
    "1. Commutative Property: For any two vectors $u$ and $v$, $u \\cdot v = v \\cdot u$.\n",
    "2. Scalar Multiplication Property: For any two vectors $u$ and $v$ and any real number $c$, $(cu) \\cdot v = u \\cdot (cv) = c(u \\cdot v)$\n",
    "3. Distributive Property: For any 3 vectors $u$, $v$, and $w$, $u \\cdot (v+w) = u \\cdot v + u \\cdot w$.\n",
    "\n",
    "When you multiply two vectors and the dot product is zero, they are perpindicular.  More generally, the angle $\\theta$ between vectors $v$ and $w$ has:\n",
    "$$\n",
    "\\cos \\theta = \\frac{v \\cdot w}{||v||\\;||w||}\n",
    "$$\n",
    "\n",
    "The length $||v||$ of a vector is $\\sqrt{v \\cdot v}$. This follows from the pythagorean theorem.\n",
    "\n",
    "The **unit vector** is a vector with length 1. Divide any vector by its length to get a unit vector."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "132623cc",
   "metadata": {},
   "source": [
    "\n",
    "### Explanation of Angle Between Two Vectors\n",
    "\n",
    "The unit vector that makes an angle $\\theta$ with the x axis is $\\begin{bmatrix}\\cos \\theta \\\\ \\sin \\theta\\end{bmatrix}$, we can see this from the unit circle\n",
    "\n",
    "![image.png](images/unit-circle.png)\n",
    "\n",
    "Let's get a geometric understanding for the rule\n",
    "$$\n",
    "\\cos \\theta = \\frac{v \\cdot w}{||v||\\;||w||}\n",
    "$$\n",
    "\n",
    "Now suppose instead of forming $\\theta$ with the x axis, we have two unit vectors, $U$ and $u$, and they are both rotated from the x axis:\n",
    "\n",
    "![image.png](images/unit-vector-addition.png)\n",
    "\n",
    "$u \\cdot U$ would then be $\\cos{\\alpha}\\cos{\\beta} + \\sin{\\alpha}\\sin{\\beta}$. From the cosine angle addition rule in trignometry, this is equal to $\\cos(\\theta)$.\n",
    "\n",
    "So we have arrived at the preliminary rule that unit vectors $u$ and $U$ at angle $\\theta$ have:\n",
    "\n",
    "$$u \\cdot U = \\cos{\\theta}$$\n",
    "\n",
    "Combine this with our observation before that you can divide any vector by its length to get its unit vector, and we arrive at our **cosine formula** for any vectors $v$ and $w$ by just dividing their lengths:\n",
    "\n",
    "$$\n",
    "\\cos \\theta = \\frac{v \\cdot w}{||v||\\;||w||}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4867f5b",
   "metadata": {},
   "source": [
    "\n",
    "### Schwarz and Triangle Inequalities\n",
    "\n",
    "Because all cosines are between -1 and 1, it follows that the absolute value of the dot product, $|v \\cdot w|$, cannot exceed the product of the lengths, this is the **Schwarz Inequality**:\n",
    "\n",
    "$$|v \\cdot w| \\le ||v||\\: ||w||$$\n",
    "\n",
    "From the Schwarz Inequality [follows](https://math.stackexchange.com/a/91194) the **Triangle Inequality**:\n",
    "\n",
    "$$||u + v|| \\le ||u|| + ||v||$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19a034db",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Independence and Dependence\n",
    "\n",
    "Vectors are **independent** if no combination other than 0 multiples gives $b=0$.  Vectors are **dependent** if multiple combinations give $b=0$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2be52981",
   "metadata": {},
   "source": [
    "## Cross Products\n",
    "\n",
    "There's not a devoted chapter for this in Strang's book, though I think he does cover it.  Anyhow, these notes are based on the [coverage on Khan](https://www.khanacademy.org/math/multivariable-calculus/thinking-about-multivariable-function/x786f2022:vectors-and-matrices/a/cross-products-mvc).\n",
    "\n",
    "Unlike the dot product, which returns a number, the result of a cross product is another vector.\n",
    "\n",
    "Let's say we have the cross product of $c = a \\times b$.  This vector $c$ has two properties. \n",
    "\n",
    "First, it is perpindicular to both $a$ and $b$, which could be expressed as inner products: $c \\cdot a = 0$ and $c \\cdot b = 0$.  This happens to be why the cross product only works in 3 dimensions and not in 2 or 4+.  In 2 dimensions, there's not always a vector perpindicular to any pair.  In four and more dimensions, there are infinitely many vectors perpindicular to a given pair of other vectors.\n",
    "\n",
    "Second, the length of $c$ is a measure of how far apart $a$ and $b$ are pointing, amplified by their magnitudes:\n",
    "\n",
    "$$||c|| = ||a||||b||\\sin(\\theta)$$\n",
    "\n",
    "This is similar to the dot product formula, but instead of $\\cos(\\theta)$, the cross product uses $\\sin(\\theta)$, where $\\theta$ is the angle between $a$ and $b$.  So when the angle is 90 degrees, the cross product is at its largest.\n",
    "\n",
    "The formula for the cross product is as follows:\n",
    "\n",
    "$$\n",
    "a \\times b = \\begin{bmatrix}a_2b_3 - a_3b_2 \\\\ a_3b_1 - a_1b_3 \\\\ a_1b_2 - a_2b_1\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d2bb8b4",
   "metadata": {},
   "source": [
    "## Matrices (1.3)\n",
    "\n",
    "A matrix is **invertible** (aka **non-singular**) if it has independent (see definition above) column vectors, meaning $Ax = 0$ has only one solution between them.\n",
    "\n",
    "A matrix is **singular** if $Ax=0$ has many solutions, or none at all."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56e07f00",
   "metadata": {},
   "source": [
    "\n",
    "# Solving Linear Equations (2)\n",
    "\n",
    "* [Elimination with Matrices](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/elimination-with-matrices-1/) - Lecture covering 2.2 and 2.3\n",
    "## Vectors and Linear Equations (2.1)\n",
    "\n",
    "Geometrically, it's worth noting that the dot product of each row with $x$ gives the equation of a plane.\n",
    "When the number of unknowns matches the number of equations, there is _usually_ one solution.\n",
    "\n",
    "### Matrix, Row, and Column Pictures\n",
    "\n",
    "Lets say we have $n$ equations and $n$ unknowns, and go over:\n",
    "* Matrix Form\n",
    "* Row Picture\n",
    "* Column Picture\n",
    "\n",
    "Let's look specifically at these two equations with two unknowns:\n",
    "$$\n",
    "2x - y = 0 \\\\\n",
    "-x + 2y = 3\n",
    "$$\n",
    "\n",
    "In **matrix form**, with the **coefficient matrix**, followed by the unknowns matrix, equal to solutions/right hand side would be:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2 & -1 \\\\\n",
    "-1 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These three matrices are abstractly referred to as $Ax=b$. When we are solving for $x$ (the inverse), we are abstractly solving $x = A^{-1}b$. And note that only with an invertible matrix (see below) can we solve this.\n",
    "\n",
    "The **row picture** is looking at one equation at a time, it's what we've seen before with systems of equations, or looking for where lines meet when we graph them geometrically.\n",
    "\n",
    "The **column picture** would have us formulate the equations as combinations of the columns, so:\n",
    "\n",
    "$$\n",
    "x \n",
    "\\begin{bmatrix}\n",
    "2 \\\\\n",
    "-1\n",
    "\\end{bmatrix}\n",
    "+ \n",
    "y\n",
    "\\begin{bmatrix}\n",
    "-1 \\\\\n",
    "2\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Geometrically, the column picture can solve these linear equations through vector addition, which we know geometrically means combining the column vectors each a certain number of times to produce the right hand side.\n",
    "\n",
    "### The Identity Matrix\n",
    "\n",
    "Multiplying $Ix$ where $I$ is the identity matrix, you get back the x you started with, $Ix=x$.  An example 3x3 identity matrix:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63361923",
   "metadata": {},
   "source": [
    "## The Idea of Elimination (2.2)\n",
    "\n",
    "**Elimination** is the systematic way of solving linear equations. Elimination proceeds by producing an **upper triangular system** from top to bottom, then solving with **back substitution** from the bottom up.\n",
    "\n",
    "In the first part, where you're producing the upper triangular system, you subtract a multiple of the above equation from the equation below.  This **multiplier** ($l$) is determined from the **pivot** above.  For example, if we have\n",
    "\n",
    "$$\n",
    "4x - 8y = 4 \\\\\n",
    "3x + 2y = 11\n",
    "$$\n",
    "\n",
    "Our multiplier of the first equation would be $l=\\frac{3}{4}$, and we'd then subtract that multiplied equation from the 2nd.  We'd then be left with the pivot of $8$ at the bottom right.  To solve $n$ equations we want $n$ pivots.  If there were a 3rd equation we'd use the $8$ pivot to determine our next multiplier and subtract, and so on.\n",
    "\n",
    "### The breakdown of elimination\n",
    "\n",
    "It's possible for the process of elimination to fail along the way.  Specifically, we might reach a 0 pivot.  In this case, we may be able to rescue this with row exchange, or may not be able to.  It may be that the 0 pivot:\n",
    "\n",
    "* Implies no solutions (e.g. $0y=8$).  Geometrically this would be non-intersecting lines. OR \n",
    "* It may be that it arrives at infinite solutions (e.g. $0y=0$). Geometrically this would be represented by more than one intersection, e.g. two identical lines.\n",
    "* It may be that a row exchange can rescue things, for example:\n",
    "\n",
    "$$\n",
    "0x + 2y = 4 \\\\\n",
    "3x - 2y = 5\n",
    "$$\n",
    "\n",
    "Here would just want to perform **row substitution** to get a triangular system we could then back-substitute on.\n",
    "\n",
    "Recall our terminology from earlier on, when we can complete elimination, we are dealing with a non-singular matrix, whereas the no solutions or infinite solutions cases are singular.\n",
    "\n",
    "### Extending into 3+ equations\n",
    "\n",
    "The process involves clear out columns below the pivots, using multipliers of that pivot, before moving onto the next pivot.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e25d296b",
   "metadata": {},
   "source": [
    "## Elimination Using Matrices (2.3)\n",
    "\n",
    "**Elimination matrices** execute our elimination steps.  An elimination matrix $E_{ij}$ eliminates row $i$, column $j$ by multiplying the $j$'th equation by $l_{ij}$ and subtracting it from the $i$'th equation.  So for example $E_{21}$ would would be the first elimination step, clearing out row 2, column 1.\n",
    "\n",
    "We need a lot of these $E_{ij}$ matrices to complete elimination, which is why we'll later see they can be combined into one big matrix $E$.  The neatest way to do that is by combining all their inverses $(E_{ij})^{-1}$ into one overall matrix $L = E^{-1}$.  \n",
    "\n",
    "The special property of $L$ is that all the multipliers $l_{ij}$ fall into place.  Those numbers are mixed up in $E$ (forward elimination from A to U).  Inverting puts the steps and their elimination matrices in the opposite order and prevents the mixup.\n",
    "\n",
    "### The Matrix Form of One Elimination Step\n",
    "\n",
    "Suppose we want to subtract two times row 1 from row 2.  The elimination matrix for this step would be:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "-2 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The first and third rows come from the identity matrix $I$. The $-2$ comes from the negative of the multiplier $l$ (2).\n",
    "\n",
    "### Matrix Multiplication\n",
    "\n",
    "Via [MathIsFun](https://www.mathsisfun.com/algebra/matrix-multiplying.html):\n",
    "\n",
    "![image.svg](images/matrix-multiply.svg)\n",
    "\n",
    "It works through the dot product of each row and column.\n",
    "\n",
    "In order to multiply two matrices, the number of columns of A must equal the number of rows of B. The product\n",
    "AB will have the same number of rows as the first matrix and the same number of columns as the second.\n",
    "\n",
    "Algebraic rules for matrix multiplication:\n",
    "* Associative Law is true: $A(BC) = (AB)C$\n",
    "* Commutative Law is false: Often $AB \\ne BA$\n",
    "\n",
    "A note on matrix multiplication order.  When we multiply on the left side vs right side, it's the difference between acting on rows vs columns, which switches based on order.  Multiplying from the left, we're doing row operations.  Multiplying from the right, we're doing column operations.\n",
    "\n",
    "3Blue1Brown emphasized:\n",
    "- Viewing Matrices as transformation of space\n",
    "- Matrix multiplication is just one transformation after another [this may belong in subsequent section]\n",
    "\n",
    "### The Row Exchange Matrix\n",
    "To exchange aka permute rows we use another matrix $P_{ij}$ called the **permutation matrix**.  For example, the permutation matrix $P_{23}$ exchanges rows 2 and 3:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Permutation matrices can swap multiple rows as well, not just one.  We'll see that soon.\n",
    "\n",
    "### The Augmented Matrix\n",
    "\n",
    "We can augment the matrix $A$ in $Ax=b$ to include $b$ as an extra column, and allow it to change through the process of elimination.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62a3c076",
   "metadata": {},
   "source": [
    "## Rules for Matrix Operations (2.4)\n",
    "\n",
    "- Lecture for 2.4 and 2.5: [Multiplication and Inverse Matrices](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/multiplication-and-inverse-matrices/)\n",
    "\n",
    "A matrix with $n$ columns can multiply a matrix with $n$ rows:\n",
    " \n",
    "$$A_{m \\times n}B_{n \\times p} = C_{m \\times p}$$\n",
    "\n",
    "\n",
    "### Multiple ways to multiply matrices\n",
    "\n",
    "1. We went over the typical dot product way of multiplying matrices above, where the entry in row $i$, and column $j$ of $AB$ is (row $i$ of $A$) $\\cdot$ (column $j$ of $B$).\n",
    "\n",
    "Terminology note: A row times a column (a dot product) is also called an **inner product**.  A column times a row is called an **outer product**.\n",
    "\n",
    "Now let's talk about additional ways to multiply matrices..\n",
    "\n",
    "2. Matrix $A$ times every column of $B$: $A\\begin{bmatrix}b_1 \\cdots b_p \\end{bmatrix} = A\\begin{bmatrix}Ab_1 \\cdots Ab_p \\end{bmatrix}$.  Recall from the column picture perspective, that we can therefore see each column of $AB$ as a combination of columns of $A$.\n",
    "\n",
    "3. Every row of matrix $A$ times matrix $B$: \n",
    "$\\begin{bmatrix} \\text{row }i\\text{ of }A\\end{bmatrix}B = \\begin{bmatrix}\\text{row }i \\text{ of }AB\\end{bmatrix}.$\n",
    "\n",
    "4. Multiply columns $1$ to $n$ of $A$ times rows $1$ to $n$ of $B$. Add those matrices. So for example:\n",
    "$$\n",
    "AB = \\begin{bmatrix}a \\\\ c\\end{bmatrix}\\begin{bmatrix}E & F\\end{bmatrix} + \\begin{bmatrix}b \\\\ d\\end{bmatrix}\\begin{bmatrix}G & H\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "You'll find that it works out just like the other methods.\n",
    "\n",
    "### Blocks\n",
    "\n",
    "Matrices can be added and multiplied by **blocks**, so long as the block sizes correspond to the normal rules-- same size for addition, and rows of 1 = cols of 2 for multiplication. \n",
    "\n",
    "Important: Cuts between columns of $A$ must match cuts between rows of $B$.\n",
    "\n",
    "Matrix block multiplication example:\n",
    "\n",
    "$A = \\begin{bmatrix}A_1 & A_2\\end{bmatrix}$ times $B = \\begin{bmatrix}B_1 \\\\ B_2\\end{bmatrix}$ is $A_{1}B_1 + A_{2}B_2$.\n",
    "\n",
    "The blocks must be equal across transposition, so for example you could have:\n",
    "* Two square matrices split up with each corner a block\n",
    "* Block columns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbcd6dc0",
   "metadata": {},
   "source": [
    "## Inverse Matrices (2.5)\n",
    "\n",
    "If the square matrix $A$ has an inverse, then both $A^{-1}A = I$ and $A^{-1}A = I$.  Note that non-square matrices are not invertible.\n",
    "\n",
    "Testing for invertibility:\n",
    "\n",
    "- The _algorithm_ to test invertibility is elimination. $A$ must have $n$ (nonzero) pivots\n",
    "- The _algebra_ test for invertibility is the determinant of $A$. $\\det A$ must not be $0$.\n",
    "- The _equation_ that test for invertibility is $Ax = 0$.  $x = 0$ must be the only solution.\n",
    "\n",
    "A matrix cannot have more than one inverse.  If you found the left-inverse, it must be the same as the right-inverse.\n",
    "\n",
    "### The Inverse of a Product AB\n",
    "\n",
    "If $A$ and $B$ are invertible, then so is $AB$:\n",
    "\n",
    "$$(AB)^{-1} = B^{-1}A^{-1}$$\n",
    "\n",
    "### Gauss-Jordan Elimination\n",
    "Gauss-Jordan eliminates $\\begin{bmatrix}A & I\\end{bmatrix}$ to $\\begin{bmatrix}I & A^{-1}\\end{bmatrix}$.\n",
    "\n",
    "The Gauss-Jordan method is to begin with that augmented matrix, $\\begin{bmatrix}A & I\\end{bmatrix}$, and performing elimination until you get the left block upper triangular.  Then, continue doing elimination upwards, so that you have only a diagonal of pivots on the left.  Finally, divide each row to get **reduced echelon form** ($R=I$) on the left hand side.  Then your inverse will be on the right hand side.\n",
    "\n",
    "This helps explain why the determininant can't be 0 for a matrix with an inverse, you have to divde by the pivots, and you can't divide by 0.\n",
    "\n",
    "**Diagonally dominant** matrices are invertible.  If the absolute value of the diagonal entries are larger than the sum of the absolute values of the rest of their rows, then the matrix is invertible.  This follows from the fact that the other row entires cannot add up to equal those entries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abfb2997",
   "metadata": {},
   "source": [
    "## Elimination = Factorization: A = LU (2.6)\n",
    "Lecture: [Factorization into A=LU](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/factorization-into-a-lu-1/)\n",
    "\n",
    "In the previous section, we went from $A$ to $U$ with elimination.  In this section, we look at elimination in the most useful way.\n",
    "\n",
    "Many key ideas of linear algebra, when you look at them closely, are really _factorizations_ of a matrix. The first factorization we look at comes from elimination.  The factors $L$ and $U$ are triangular matrices. The factorization that comes from elimination is $A=LU$.\n",
    "\n",
    "We already know about $U$, the upper triangular matrix, from producing it during elimination. Reversing those steps, taking $U$ back to $A$ is achieved by a lower triangular $L$.\n",
    "\n",
    "Each elimination step $E_{ij}$ is inverted by $L_{ij}$. The entries of $L$ are exactly the multipliers $l_{ij}$. Every multiplier $l_{ij}$ is in row $i$, column $j$ of $L$.\n",
    "\n",
    "Here's a 2x2 example going forward from $A$ to $U$, then back from $U$ to $A$:\n",
    "\n",
    "$$\n",
    "E_{21}A = \\begin{bmatrix}1 & 0 \\\\ -3 & 1\\end{bmatrix}\\begin{bmatrix}2 & 1 \\\\ 6 & 8\\end{bmatrix} = \\begin{bmatrix}2 & 1 \\\\ 0 & 5\\end{bmatrix} = U \\\\\n",
    "E_{21}^{-1}U = \\begin{bmatrix}1 & 0 \\\\ 3 & 1\\end{bmatrix}\\begin{bmatrix}2 & 1 \\\\ 0 & 5\\end{bmatrix} = \\begin{bmatrix}2 & 1 \\\\ 6 & 8\\end{bmatrix} = A\n",
    "$$\n",
    "\n",
    "The second line is our factorization $LU=A$. The whole forward elimination process (with no row exchanges) is inverted by $L$.  Just as $E$ is all eliminations, $L$ is all the inverse eliminations.\n",
    "\n",
    "### Predicting zeroes in L and U\n",
    "\n",
    "We can predict the zeroes in $L$ and $U$ from $A$:\n",
    "\n",
    "- When a row of $A$ starts with zeroes, so does that row of $L$\n",
    "- When a column of $A$ starts with zeroes, so does that column of $U$\n",
    "\n",
    "But note that zeros in the middle of the matrix are likely to be filled in, while elimination sweeps forward.\n",
    "\n",
    "### Better balance from LDU\n",
    "\n",
    "$A=LU$ is not \"symmetric\" in that $A$ has 1s on its pivots while $U$ does not.  This is easy to fix.  Divide $U$ by a diagonal matrix $D$ that contains the pivots. That leaves a new triangular matrix with 1's on the diagonal. E.g:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}1 & 0 \\\\ 3 & 1\\end{bmatrix}\\begin{bmatrix}2 & 8 \\\\ 0 & 5\\end{bmatrix} \\text{ splits further into }\n",
    "\\begin{bmatrix}1 & 0 \\\\ 3 & 1\\end{bmatrix}\\begin{bmatrix}2 & 0 \\\\ 0 & 5\\end{bmatrix}\\begin{bmatrix}1 & 4 \\\\ 0 &1\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### How expensive is elimination\n",
    "\n",
    "The first stage of elimination produces zeros below the first pivot in column 1. To find each entry below the pivot requires one multiplication and one subtraction. We count this first stage as $n^2$ multiplications and $n^2$ subtractions. It is actually less ($n^2 -n$) because row 1 doesn't change.\n",
    "\n",
    "The next stage clears out the second column below the second pivot. The working matrix is now of size $n-1$. We estimate this stage as $(n-1)^2$ multiplications and subtractions.\n",
    "\n",
    "The rough count to reach $U$ is the sum of squares $n^2 + (n-1)^2 + \\cdots + 2^2 + 1^2$. There is an exact formula ([proofs here](https://math.stackexchange.com/questions/48080/sum-of-first-n-squares-equals-fracnn12n16)) $\\frac{1}{3}n(n+\\frac{1}{2})(n + 1)$ for this sum of squares. For considering the cost/complexity here, we can just pay attention to the largest term, and say:\n",
    "\n",
    "Elimination on A requires about $\\frac{1}{3}n^3$ multiplications and $\\frac{1}{3}n^3$ subtractions.\n",
    "\n",
    "What about the right side? Going forward, we subtract multiple of $b_1$ from the components below. This is $n-1$ steps. The second stage takes only $n-2$ steps, because $b_1$ is not involved. The last stage of forward elimination takes one step.\n",
    "\n",
    "Then, for back substitution, $x_n$ takes one step (divide by the last pivot).  The next unknown takes two steps. When we reach $x_1$ it will require $n$ steps ($n-1$ substitutions of the other unknowns, then division by the first pivot). \n",
    "\n",
    "The total count on the right side, from $b$ to $c$ to $x$, forward and backward, is therefore exactly $n^2$, which we can see from:\n",
    "\n",
    "$$\n",
    "[(n - 1) + (n - 2) + \\cdots 1] + [1 + 2 + \\cdots + (n-1) + n] = n^2\n",
    "$$\n",
    "\n",
    "So the right side takes $n^2$ multiplications and $n^2$ subtractions in total."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67d9e6c6",
   "metadata": {},
   "source": [
    "## Transposes and Permutations (2.7)\n",
    "\n",
    "[Lecture - Transposes, Permutations, Vector Spaces](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/transposes-permutations-vector-spaces-1/).  This lecture covers Vector Spaces as well.\n",
    "\n",
    "The **transpose** of $A$ is denoted by $A^T$. The columns of $A^T$ are the rows of $A$:\n",
    "\n",
    "$$(A^T)_{ij} = A_{ji}$$\n",
    "\n",
    "When $A$ is an $m$ by $n$ matrix, the transpose is $n$ by $m$:\n",
    "\n",
    "$$\n",
    "\\text{If }A=\\begin{bmatrix}1 & 2 & 3 \\\\ 0 & 0 & 4\\end{bmatrix} \\text{ then } A^T = \\begin{bmatrix}1 & 0 \\\\ 2 & 0 \\\\ 3 & 4\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The matrix \"flips over\" its main diagonal.\n",
    "\n",
    "### Rules of transposes\n",
    "\n",
    "- Sum: The transpose of $A + B$ is $A^T + B^T$\n",
    "- Product: The transpose of $AB$ is $(AB)^T = B^TA^T$\n",
    "- Inverse: The transpose of $A^{-1}$ is $(A^{-1})^T = (A^T)^{-1}$\n",
    "\n",
    "Notice how $B^TA^T$ comes in reverse order. This follows from how matrix multiplication works.  We get back to the same operations by transposing and flipping the order.\n",
    "\n",
    "The reverse order rule applies to three or more factors, so $(ABC)^T = C^TB^TA^T$.\n",
    "\n",
    "Now let's prove the inverse rule.  Start with $A^{-1}A = I$.  Apply the product rule above, and we get $A^T(A^{-1})^T = I$.  This shows that $(A^{-1})^T = (A^T)^{-1}$. It also follows that $A^T$ is invertible exactly when $A$ is invertible.\n",
    "\n",
    "[there's a section on The meaning of inner products which i dont grok, but maybe will after watching the lecture video.]\n",
    "\n",
    "### Symmetric Matrices\n",
    "\n",
    "For a symmetric matrix, transposing $A$ to $A^T$ produces no change. A symmetric matrix has $S^T = S$, meaning $S_{ji} = S_{ij}$.\n",
    "\n",
    "The inverse of a symmetric matrix is also symmetric, so $(S^{-1})^T = (S^T)^{-1} = S^{-1}$.\n",
    "\n",
    "The product of a matrix and its transpose will always be symmetric.  $A^TA$ is always symmetric.  We can see why this is true from the transpose equaling itself, which is our definition of symmetric: $(AA^T)^T = A^{TT}A^T = AA^T$.\n",
    "\n",
    "### Symmetric Products\n",
    "\n",
    "It follows from the product rule that the transpose of $A^TA$ is $A^T(A^T)^T$ which is $A^TA$ again.  \n",
    "\n",
    "Also, a symmetric invertible matrix will have a symmetric factorization, simpler than $S = LDU$, it will have $S=LDL^T$.\n",
    "\n",
    "### Permutation Matrices\n",
    "\n",
    "The transpose plays a special role for a **permutation matrix**.  This matrix P has a single \"1\" in every row and column.  \n",
    "\n",
    "Then $P^T$ is also a permutation matrix, maybe the same as $P$ or maybe different.  Any product $P_1P_2$ is again a permutation matrix.\n",
    "\n",
    "The simplest permutation matrix is $P = I$ (no exchanges).  The next simplest are the row exchanges $P_{ij}$. Other permutations reorder more rows.  By doing all possible row exchanges to $I$, we get all possible permutation matrices.  There are 6 3x3 permtuation matrices:\n",
    "\n",
    "$$\n",
    "\\;\\;I=\\begin{bmatrix}\n",
    "1 & & \\\\\n",
    "& 1 & \\\\\n",
    "& & 1\n",
    "\\end{bmatrix} \\quad \n",
    "P_{21}=\\begin{bmatrix}\n",
    "& 1 & \\\\\n",
    "1 & & \\\\\n",
    "& & 1\n",
    "\\end{bmatrix} \\quad \n",
    "P_{32}P_{21}=\\begin{bmatrix}\n",
    "& 1 & \\\\\n",
    "& & 1 \\\\\n",
    "1 & &\n",
    "\\end{bmatrix} \\\\ \\\\\n",
    "\n",
    "P_{31}=\\begin{bmatrix}\n",
    "& & 1 \\\\\n",
    "& 1 & \\\\\n",
    "1 & &\n",
    "\\end{bmatrix} \\quad \n",
    "P_{32}=\\begin{bmatrix}\n",
    "1 & & \\\\\n",
    "& & 1 \\\\\n",
    "& 1 &\n",
    "\\end{bmatrix} \\quad \n",
    "P_{21}P_{32}=\\begin{bmatrix}\n",
    "& & 1 \\\\\n",
    "1 & & \\\\\n",
    "& 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "There are $n!$ permutation matrices of order $n$.\n",
    "\n",
    "$P^{-1}$ is also a permutation matrix.  Among the P's displayed above, the four matrices on the left are their own inverses.  The two matrices on the right are inverses of each other.  In all cases, a single row exchange is its own inverse.\n",
    "\n",
    "$P^{-1}$ is always the same as $P^T$. So $PP^T = I$.\n",
    "\n",
    "Permutations (row exchanges before elimination) lead to $PA = LU$.\n",
    "\n",
    "### The PA = LU Factorization with Row Exchanges\n",
    "\n",
    "There are multiple ways we could approach permutations during elimination. \n",
    "\n",
    "1. We could do row exchanges in advance.  Then $PA=LU$.\n",
    "2. If we hold row exchanges until after elimination, the pivot rows are in a strange order.  Then $A= LPU$\n",
    "\n",
    "We will focus on the 1st one for our work; it's also the way computers do it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fceefd4",
   "metadata": {},
   "source": [
    "# Vector Spaces and Subspaces (3)\n",
    "## Spaces of Vectors (3.1)\n",
    "* First lecture on it [starts here](https://youtu.be/JibVXBElKL0?t=1246), the second part of the previous lecture\n",
    "* Second lecture on it is [first part here](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/column-space-and-nullspace-1/)\n",
    "\n",
    "The space $R^n$ consists of all column vectors $v$ with $n$ components.\n",
    "\n",
    "The components of $v$ are real numbers, which is the reason for the letter $R$.  A vector with complex numbers lies in the space $C^n$.\n",
    "\n",
    "We can add vectors and multiply by scalars (produce linear combinations) in a space, and the results remain in the space.  Every vector space must include the zero vector.\n",
    "\n",
    "The smallest possible vector space is $Z$, which only includes the zero vector.  Each space has its own zero vector.\n",
    "\n",
    "### Subspaces\n",
    "\n",
    "There are important vector spaces inside $R^n$.  Those are the **subspaces** of $R^n$.\n",
    "\n",
    "An example is a plane through the origin of $R^3$. That plane is a vector space in its own right. If we add two vectors in the plane, their sum is in the plane. If we multiply an in-plane vector by 2 or -5, it is still in the plane.\n",
    "\n",
    "Formal definition: A subspace of a vector space is a set of vectors (including 0) that satisfies two requirements.  If $v$ and $w$ are vectors in the subspace and $c$ is any scalar:\n",
    "1. $v + w$ is in the subspace\n",
    "2. $cv$ is in the subspace\n",
    "\n",
    "Or, to compress both rules into one: a subspace containing $v$ and $w$ must contain all linear combinations $cv + dw$.\n",
    "\n",
    "So a plane that doesn't go through the origin would fail that definition.\n",
    "\n",
    "Note that vector spaces count as subspaces, so $R^3$ etc. are subspaces.  They are subspaces of themselves, really.  Here is a list of all the possible subspaces of $R^3$:\n",
    "- Any line through $(0,0,0)$\n",
    "- The whole space ($R^3$)\n",
    "- Any plane through $(0,0,0)$\n",
    "- The zero vector $(0,0,0)$.\n",
    "\n",
    "An upper quadrant line would not be a subspace, because you could multiply by -1 and end up outside of the subspace.\n",
    "\n",
    "The union of two subspaces will only be a subspace if one of the subspaces contains the other.  The intersection of two subspaces will always be a subspace.\n",
    "\n",
    "### The Column Space of A\n",
    "\n",
    "The **column space** consists of all linear combinations of the columns (all possible $b$'s in $Ax=b$). They fill the column space $C(A)$.\n",
    "\n",
    "The system $Ax=b$ is solvable if and only if $b$ is in the column space of $A$.\n",
    "\n",
    "Suppose we have an $m$ by $n$ matrix. The columns belong to $R^m$.  The column space of $A$ is a subspace of $R^m$.\n",
    "\n",
    "The set of all column combinations $Ax$ satisfies the rules for a subspace: when we add linear combinations of them, we still produce combinations of the columns.\n",
    "\n",
    "Instead of columns of $R^n$ we could start with any set $S$ of vectors in a vectors space $V$.  To get a subspace $SS$ of $V$, we take all combinations of the vectors in that set.\n",
    "\n",
    "So column space is an example of a span, of the column vectors.  The columns there \"span\" the column space.\n",
    "\n",
    "The subspace $SS$ is the span of $S$, containing all combinations of vectors in $S$.\n",
    "\n",
    "### 8 rules of vector spaces\n",
    "These rules are also covered [on Wolfram](https://mathworld.wolfram.com/VectorSpace.html).\n",
    "\n",
    "In the definition of a vector space, vector addition $X + Y$ and scalar multiplication $cx$ must obey the following 8 rules:\n",
    "\n",
    "1. $X + Y = Y + X$ (Commutativity of vector addition)\n",
    "2. $X + (Y + Z) = (X + Y) + Z$ (Associativity of vector addition)\n",
    "3. There is a unique zero vector, such that $X + 0 = X$ for all $X$ (Additive identity)\n",
    "4. For each $X$ there is a unique vector $-X$ such that $X + (-X) = 0$ (Existence of additive inverse)\n",
    "5. $1$ times $X$ equals $X$ (Scalar multiplication identity)\n",
    "6. $(c_1c_2)X = c_1(c_2X)$ (Associativity of scalar multiplications)\n",
    "7. $c(X + Y) = cX + cY$ (Distributivity of vector sums)\n",
    "8. $(c_1 + c_2)X = c_1X + c_2X$ (Distributivity of scalar sums)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "334ba622",
   "metadata": {},
   "source": [
    "## The Nullspace of A: Solving Ax = 0 and Rx = 0 (3.2)\n",
    "* Lecture [begins here](https://youtu.be/8o5Cmfpeo6g?t=1677)\n",
    "\n",
    "The *nullspace*, denoted $N(A)$, consists of all solutions to $Ax=0$.  These vectors $x$ are in $R^n$. \n",
    "\n",
    "For invertible matrices, $x=0$ is the only solution to $Ax=0$. For noninvertible matrices, there are non-zero solutions to $Ax=0$.  Each solution $x$ belongs to the nullspace of $A$.\n",
    "\n",
    "The solution vectors, the null space, forms a subspace.  Suppose $x$ and $y$ are in the nullspace (meaning $Ax=0$ and $Ay=0$).  Then $A(x + y) = 0 + 0$ and $A(cx) = c0$, meaning that both adding vectors in the nullspace, and scalar multiplying vectors in the nullspace, produces more vectors within the nullspace.  Since we can add and multiply without leaving the nullspace, it's a subspace.\n",
    "\n",
    "### Special solutions\n",
    "\n",
    "To describe the solutions to $Ax=0$, an efficient way is to choose one point on the line (one *special solution*).  Then all points on the line are multiples of this one.\n",
    "\n",
    "Example with a 2x2 matrix: The nullspace of $A = \\begin{bmatrix}1 & 2 \\\\ 3 & 6\\end{bmatrix}$ contains all multiples of $s = \\begin{bmatrix}-2 \\\\ 1\\end{bmatrix}$.  This solution is special because we set the free variable to $x_2 = 1$.\n",
    "\n",
    "The nullspace consists of all combinations of the special solutions to $Ax = 0$.\n",
    "\n",
    "Example with two free variables: $x + 2y + 3z = 0$ comes from the 1x3 matrix $A = \\begin{bmatrix}1 & 2 & 3\\end{bmatrix}$.  Then $Ax=0$ produces a plane, which is the nullspace of $A$.  There are two free variables, $y$ and $z$, which we alternately set to 0 and 1:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}x \\\\ y \\\\ z\\end{bmatrix} \\text{ has two special solutions }s_1 = \\begin{bmatrix}-2 \\\\ \\textbf{1} \\\\ \\textbf{0}\\end{bmatrix} \\text{ and } s_2 = \\begin{bmatrix}-3 \\\\ \\textbf{0} \\\\ \\textbf{1}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Those vectors $s_1$ and $s_2$ lie on the plane $x + 2y + 3z = 0$.  All vectors on the plane are combinations of $s_1$ and $s_2$.\n",
    "\n",
    "The last two components are \"free\" and we choose them specially as 1,0 and 0,1.  Then the first components -2 and -3 are determined by the equation $Ax=0$.\n",
    "\n",
    "What about when dealing with more than two free components?  Then each special solution will have a 1 in a free spot and 0s in the rest.\n",
    "\n",
    "The solutions to $x + 2y + 3z = 6$ also lie on a plane, but that plane is not a subspace.  We will explore the solutions for these types of equations later.\n",
    "\n",
    "### Pivots, and (reduced) row echelon form in rectangular matrices\n",
    "Previously we've dealt with square matrices where the pivots would always be clean across the diagonal.  More broadly, the pivots are the leading non-zero value of each row when the matrix is in [row echelon form](https://en.wikipedia.org/wiki/Row_echelon_form), which is defined as:\n",
    "- All rows consisting of only zeroes are at the bottom\n",
    "- The leading entry of every nonzero row is to the right of the leading entry of every row above\n",
    "\n",
    "Here's an example in row echelon form:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & a_0 & a_1 & a_2 & a_3 \\\\\n",
    "0 & 0 & 2 & a_4 & a_5 \\\\\n",
    "0 & 0 & 0 & 1 & a_6 \\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note: Strang doesn't really use \"row echelon form\", instead referring to that stage as upper-triangular U, which terminologically I think is only supposed to apply to square matrices, but whatever.\n",
    "\n",
    "A matrix is in reduced row echelon form if:\n",
    "- It is in row echelon form (see above)\n",
    "- The leading entry (pivot) in each non-zero row is a 1\n",
    "- Each column containing a leading 1 has zeros in all other entries\n",
    "\n",
    "Here is a matrix in reduced row echelon form:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & a_1 & 0 & b_1 \\\\\n",
    "0 & 1 & a_2 & 0 & b_2 \\\\\n",
    "0 & 0 & 0 & 1 & b_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "### Pivot columns and free columns\n",
    "\n",
    "The first column of $A = \\begin{bmatrix}1 & 2 & 3\\end{bmatrix}$ contains the only pivot, so the first component of $x$ is not free.  The **free components** correspond to columns with no pivots.  The special choice (one or zero) is only for the free variables in the special solutions.\n",
    "\n",
    "### Reduced Row Echelon Form\n",
    "\n",
    "The Nullspace stays the same as we go from the starting matrix to upper triangular to reduced row echelon form.  But the nullspace / special solutions are easiest to calculate from the reduced row echelon form.\n",
    "\n",
    "### Matrix Shape and Free Variables\n",
    "\n",
    "Suppose $Ax=0$ has more unknowns than equations ($n > m$, more columns than rows).  There must be at least one free column.  Then $Ax = 0$ has nonzero solutions.\n",
    "\n",
    "This follows from the fact that free variables can be set to 1 (special solutions), which negates x being a zero a solution.\n",
    "\n",
    "The nullspace is a subspace. Its \"dimension\" is the number of free variables.  Let's explore this further..\n",
    "\n",
    "### The Rank of a Matrix\n",
    "\n",
    "The numbers $m$ and $n$ give the size of a matrix, but not necessarily the true size of a linear system.  An equation like $0=0$ shouldn't count.  If there are two identical rows in $A$, the second one dissapears in elimination. Also if row 3 is a combination of rows 1 and 2, then row 3 will become zeros in row echelon form.  The true size of $A$ is given by its rank.\n",
    "\n",
    "The **rank** ($r$) of $A$ is the number of pivots.\n",
    "\n",
    "### Rank one, more on ranks\n",
    "\n",
    "Matrices of rank one have only one pivot.  With these matrices, when elimination produces zero in the first column, it produces zero in all the columns.  As a result, every row is a multiple of the pivot row, and every column is a multiple of the pivot column:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "1 & 3 & 10 \\\\\n",
    "2 & 6 & 20 \\\\\n",
    "3 & 9 & 30\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "R = \\begin{bmatrix}\n",
    "1 & 3 & 10 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The column space of a rank one matrix is \"one-dimensional\".  In the example above, all the columns are on the line through $u = (1,2,3)$. The columns of $A$ above are $u$ and $3u$ and $10u$.  Put those numbers into the row $v^T = \\begin{bmatrix} 1 & 3 & 10 \\end{bmatrix}$ and you have the special rank one from $A = uv^T$, which is $A = \\text{column times row} = uv^T$:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 3 & 10 \\\\\n",
    "2 & 6 & 20 \\\\\n",
    "3 & 9 & 30\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}1 \\\\ 2 \\\\ 3\\end{bmatrix}\n",
    "\\begin{bmatrix}1 & 3 & 10\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Our second definition of rank will be at a higher level.  It deals with entire rows and entire columns-- vectors and not just numbers.  We can define rank in terms of number of independent rows/cols.  \n",
    "\n",
    "Lastly, we can define rank in terms of spaces of vectors.  The rank is the \"dimension\" of the column space.  It is also the dimension of the row space.  And the great fact which we're saving for last: $n - r$ is the dimension of the nullspace (this follows from free columns producing the dimensions of the nullspace)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8a72789",
   "metadata": {},
   "source": [
    "## The Complete Solution to Ax = b (3.3)\n",
    "[Lecture for 3.3 and 3.4](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/independence-basis-and-dimension-1/)\n",
    "\n",
    "We're going to expand on the previous section by also considering cases where $b$ is not 0.  \n",
    "\n",
    "This means we'll have to keep track of it as we perform row operations on the left.  One way to organize this is to use the augmented matrix $\\begin{bmatrix}A b\\end{bmatrix}$, and go from $\\begin{bmatrix}A b\\end{bmatrix}$ to $\\begin{bmatrix}R d\\end{bmatrix}$ once in reduced row echelon form.\n",
    "\n",
    "### One particular solution $Ax_p = b$\n",
    "\n",
    "We denote our particular solution with $x_p$. For an easy particular solution $x_p$, choose the free variables to be zero, then we can solve for a particular solution using the multiples of the pivot varables that reach $d$.\n",
    "\n",
    "$x_n$ is the symbol for the nullspace solutions.  Unlike the particular, there can be multiple of these as we saw before, one for each special solution to the nullspace.\n",
    "\n",
    "Here's an example where we write out the **complete solution** $x_p + x_n$ to $Ax=b$:\n",
    "\n",
    "Let's start with $Ax=b$ as:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 3 & 0 & 2 \\\\\n",
    "0 & 0 & 1 & 4 \\\\\n",
    "1 & 3 & 1 & 6\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "x_4\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "6 \\\\\n",
    "7\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We perform elimination on the augmented matrix (subtract row 1 from row 3, then subtract row 2 from row 3):\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 3 & 0 & 2 & 1 \\\\\n",
    "0 & 0 & 1 & 4 & 6 \\\\\n",
    "1 & 3 & 1 & 6 & 7\n",
    "\\end{bmatrix} \n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 3 & 0 & 2 & 1 \\\\\n",
    "0 & 0 & 1 & 4 & 6 \\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "We find the particular solution by setting the free variables in columns 2 and 4 to 0:\n",
    "\n",
    "$$\n",
    "Rx_p = \n",
    "\\begin{bmatrix}\n",
    "1 & 3 & 0 & 2 \\\\\n",
    "0 & 0 & 1 & 4 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "1 \\\\ 0 \\\\ 6 \\\\ 0\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\ 6 \\\\ 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Next we incorporate our special solutions for the nullspace, i.e. Rx = **0**.  Bringing it all together we have:\n",
    "\n",
    "$$\n",
    "x = x_p + x_n = \\begin{bmatrix}1 \\\\ 0 \\\\ 6 \\\\ 0\\end{bmatrix} + x_2\\begin{bmatrix}-3 \\\\ 1 \\\\ 0 \\\\ 0\\end{bmatrix} + x_4\\begin{bmatrix}-2 \\\\ 0 \\\\ -4 \\\\ 1\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Suppose we had a square invertible matrix, then with no free variables, and a nullspace of only $0$, there's only one answer: $x = x_p + x_n = A^{-1}b + 0$.\n",
    "\n",
    "### Full column rank\n",
    "\n",
    "With **full column rank** ($r = n$), every column has a pivot.  Reducing these matrices puts $I$ at the top:\n",
    "\n",
    "$$\n",
    "R = \\begin{bmatrix}I \\\\ 0\\end{bmatrix} = \\begin{bmatrix}\\text{$n$ by $n$ identity matrix} \\\\ \\text{$m - n$ rows of zeros}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "There are no free columns or variables with these.  So the nullspace only contains the zero vector.\n",
    "\n",
    "If $A$ has full column rank, then $Ax=b$ has either 0 or 1 solutions, depending on whether $b$ is reachable.\n",
    "\n",
    "There will be $m - n$ rows of zeros in $R$.  Only $b$'s on the right side that follow the conditions of those zero rows will be solvable.\n",
    "\n",
    "As we'll see in a following section, full column rank matrices have **independent columns**.\n",
    "\n",
    "### Full row rank\n",
    "\n",
    "The other extreme case is **full row rank** ($r=m$).  Now $Ax=b$ will have either one (for the square invertible case) or infinitely many (for $m=r<n$) solutions.\n",
    "\n",
    "Every full row rank matrix will:\n",
    "1. Have pivots in every row\n",
    "2. Have a solution for every right side $b$\n",
    "3. Fill all of the column space $R^m$.\n",
    "4. Has $n -r = n - m$ special solutions in the nullspace of $A$.\n",
    "\n",
    "In this case we have **independent rows**.\n",
    "\n",
    "### Summing it up\n",
    "\n",
    "The four possibilities for linear equations depend on the rank $r$:\n",
    "\n",
    "\n",
    "|  Rank| Description| Ax=b|\n",
    "| --- | --- | --- |\n",
    "| $r=m$ and $r=n$ | Square and invertible | 1 solution |\n",
    "| $r=m$ and $r<n$ | Short and wide | $\\infty$ solutions |\n",
    "| $r<m$ and $r=n$ | Tall and thin | 0 or 1 solution |\n",
    "| $r<m$ and $r<n$ | Not full rank | 0 or $\\infty$  solutions |\n",
    "\n",
    "And we'll get four types of $R$ after reduction:\n",
    "\n",
    "|  | | | |\n",
    "| --- | --- | --- | --- |\n",
    "| $\\begin{bmatrix}I\\end{bmatrix}$ | $\\begin{bmatrix}I & F\\end{bmatrix}$ | $\\begin{bmatrix}I \\\\ 0\\end{bmatrix}$ | $\\begin{bmatrix}I & F \\\\ 0 & 0\\end{bmatrix}$ |\n",
    "| $r = m = n$ | $r = m < n$ | $r = n < m$ | $r < m, r < n$ |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d4134dc",
   "metadata": {},
   "source": [
    "## Independence, Basis and Dimension (3.4)\n",
    "[Lecture for 3.3 and 3.4](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/independence-basis-and-dimension-1/)\n",
    "\n",
    "The true dimension of the column space is the rank $r$.\n",
    "\n",
    "The **basis** for a space are independent vectors that span the space.  Every vector in the space is a unique combination of the basis vectors.\n",
    "\n",
    "The **dimension** of a space is the number of vectors in a basis.\n",
    "\n",
    "### Linear Independence\n",
    "\n",
    "The columns of $A$ are **linearly independent** when the only solution to $Ax=0$ is $x=0$.  Or put another way, the columns are independent when the nullspace $N(A)$ contains only the zero vector.  \n",
    "\n",
    "Geometrically, if three vectors are not in the same plane, they are independent.  Conversely, if three vectors are in the same plane, they are dependent.\n",
    "\n",
    "The formal definition of linear independence covers more broadly combinations of vectors not in a matrix $A$, i.e sequences of vectors.  Same idea though; the only linear combination of them that equals 0 should occur when you take 0 times each one.\n",
    "\n",
    "Note that a sequence containing the zero vector will always be dependent.  \n",
    "\n",
    "Three vectors in $R^2$ cannot be independent.  A couple of ways of seeing this. One is that the matrix $A$ with those three columns must have a free variable and then a special solution to $Ax=0$.  Another way: if the two vectors are independent, some combination of them will produce the third vector, because they fill up $R^2$.\n",
    "\n",
    "Let's say we get a three vectors in $R^3$, and we're asked to determine if they're dependent.  We could see this by plugging them into a 3x3 matrix $A$ and seeing if $Ax=0$ has a non-zero solution.  If it does, they're dependent.\n",
    "\n",
    "In a square matrix, dependent columns imply dependent rows.\n",
    "\n",
    "When vectors are independent, the matrix of independent columns will be of full column rank. Whereas any set of vectors in $R^m$ must be linearly dependent if $n > m$. \n",
    "\n",
    "The columns might be dependent or independent, if $n \\le m$.  Elimination will reveal the pivot columns, which are the independent ones.\n",
    "\n",
    "### Vectors that span a subspace\n",
    "\n",
    "A set of vectors **spans** a space if their linear combinations fill the space.  For example, $\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$ and  $\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$ span all of $R^2$. $\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$ and  $\\begin{bmatrix}-1 \\\\ -1\\end{bmatrix}$ only span a line in $R^2$.\n",
    "\n",
    "It's fine to \"overkill\" on a span, like have another vector that doesn't add anything, so long as it's spanning.\n",
    "\n",
    "Let's introduce a new subspace, which is spanned by the rows.  The combination of the rows produces the **row space** in $R^n$.  The row space of $A$ is the column space of $A^T$.\n",
    "\n",
    "### A basis for a vector space\n",
    "\n",
    "We want enough independent vectors to span a space, and not more. A **basis** for a vector space, defined as a sequence of vectors which are linearly independent and span the space, provides this. \n",
    "\n",
    "The columns of the $n$ by $n$ identity matrix give the **standard basis** for $R^n$.\n",
    "\n",
    "The columns for every invertible $n$ by $n$ matrix give a basis for $R^n$.\n",
    "\n",
    "The vectors $v_1, \\dots, v_n$ are a basis for $R^n$ precisely when they are the columns of an $n$ by $n$ invertible matrix. Thus $R^n$ has infinitely many different bases.\n",
    "\n",
    "Note that going from $A$ to $R$, the column spaces/bases change, while retaining the same dimension.  But row space however does not change between $A$ and $R$.\n",
    "\n",
    "### Dimension of a vector space\n",
    "\n",
    "The number of vectors in every basis is the **dimension** of the space. A line has dimension 1, a plane dimension 2.\n",
    "\n",
    "Column Space of $A$ has dimension $r$, and the nullspace of $A$ has dimension $n-r$.\n",
    "\n",
    "### Bases for Matrix Spaces and Function Spaces\n",
    "\n",
    "Independence/basis/dimension is not limited to column vectors.  We can apply these concepts to matrices and functions as well.\n",
    "\n",
    "#### Matrix spaces\n",
    "\n",
    "We can ask whether matrices are dependent by asking whether some combination of them produces the zero matrix.  And we can ask the dimenion, for example the dimension of a 3 by 4 matrix space is 12.\n",
    "\n",
    "- The dimension of the whole $n$ by $n$ matrix space is $n^2$\n",
    "- The dimension of the subspace upper triangular matrices is $\\frac{1}{2}n^2 + \\frac{1}{2}n$.\n",
    "- The dimension of the subspace of diagonal matrices is $n$.\n",
    "- The dimension of the subspace of symmetric matrices is $\\frac{1}{2}n^2 + \\frac{1}{2}n$.\n",
    "\n",
    "#### Function spaces\n",
    "\n",
    "In differential equations $d^2y/dx^2 = y$ has a space of solutions. One basis is $y = e^x$ and $y = e^{-x}$.  The dimension there is 2, because of the second derivative.\n",
    "\n",
    "- $y'' =0$ is solved by any linear function $y = cx + d$\n",
    "- $y'' = -y$ is solved by any combination $y = c\\sin{x} + d\\cos{x}$\n",
    "- $y'' = y$ is solved by any combination $y = ce^x + de^{-x}$\n",
    "\n",
    "That solution space for $y'' = -y$ has two basis functions: $\\sin{x}$ and $\\cos{x}$. The space for $y'' = 0$ has $x$ and $1$.  It is the \"nullspace\" of the second derivative.  The dimension in each case is 2.\n",
    "\n",
    "The solutions of $y'' = 2$ don't form a subspace, because the rightside $b=2$ is not zero.  A particular solution is $x^2$.  The complete solution is $y(x) = x^2 + cx + d$.  All those functions satisfy $y'' = 2$.  Notice the particular solution plus any function $cx + d$ in the nullspace.  A linear differential equation is like a linear matrix equation $Ax=b$, but we solve it by calculus instead of linear algebra.\n",
    "\n",
    "### Basis of space Z\n",
    "\n",
    "The space $Z$ contains only the zero vector. The dimension of this space is zero. The empty set (containing no vectors) is a basis for $Z$. We can never allow the zero vector into a basis, because then linear independence is lost.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "528876fa",
   "metadata": {},
   "source": [
    "## Dimensions of the Four Subspaces (3.5)\n",
    "\n",
    "[Lecture here](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/the-four-fundamental-subspaces-1/)\n",
    "\n",
    "The rank of $A$ reveals the dimensions of all four fundamental subspaces.  We're introducing a new one here:\n",
    "\n",
    "1. The row space is $C(A^T)$, a subspace of $R^n$\n",
    "2. The column space is $C(A)$, a subspace of $R^m$\n",
    "3. The nullspace is $N(A)$, a subspace of $R^n$\n",
    "4. The **left nullspace** is $N(A^T)$, a subspace of $R^m$.\n",
    "\n",
    "For the left nullspace we solve $A^Ty = 0$. The vectors $y$ go on the left side of $A$ when the equation is written $y^TA = 0^T$.\n",
    "\n",
    "Whereas the row space and the column space have the same dimension $r$,  $N(A)$ and $N(A^T)$ have dimensions $n-r$ and $m-r$, to make up the full $n$ and $m$.\n",
    "\n",
    "### The four subspaces for R\n",
    "\n",
    "Suppose $A$ is reduced to its reduced row echelon form $R$. Two of the subspaces will remain the same, and two will change-- Row space and Null space are the same between $A$ and $R$, but Column Space and Left Nullspace change.   But all of them will retain the same dimension for both $A$ and $R$.\n",
    "\n",
    "The left nullspace looks for combinations of rows that equal to zero.  In reduced echelon form, this will always be multiples of the zero rows at the bottom, with the previous rows set to 0 because they are linearly independent and can't add up to zero together.\n",
    "\n",
    "### Rank one matrices\n",
    "\n",
    "Every rank one matrix is one column times one row. $A=uv^T$.  Example:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2 & 3 & 7 & 8 \\\\\n",
    "2a & 3a & 7a & 8a \\\\\n",
    "2b & 3b & 7b & 8b \n",
    "\\end{bmatrix} = \\begin{bmatrix}1 \\\\ a \\\\ b\\end{bmatrix}\\begin{bmatrix}2 & 3 & 7 & 8\\end{bmatrix} = uv^T\n",
    "$$\n",
    "\n",
    "### Rank two matrices = Rank one plus rank one\n",
    "\n",
    "Every rank $r$ matrix is a sum of $r$ rank one matrices.\n",
    "\n",
    "If $EA=R$, the last $m-r$ rows of $E$ are a basis for the left nullspace of $A$.\n",
    "\n",
    "### Products and Rank\n",
    "\n",
    "All the rows of $AB$ are combinations of the rows of $B$.  So the row space of $AB$ is contained in or equal to the row space of $B$. Rank(AB) $\\le$ Rank(B).\n",
    "\n",
    "All the columns of $AB$ are combinations of the columns of $A$.  So the column space of $AB$ is contained in or equal to the column space of $A$. Rank(AB) $\\le$ Rank(A).\n",
    "\n",
    "If we multiply by an invertible matrix, the rank will not change.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c07195b6",
   "metadata": {},
   "source": [
    "# Orthogonality (4)\n",
    "## Orthogonality of the Four Subspaces (4.1)\n",
    "\n",
    "[Lecture video here](https://youtu.be/YzZUIYRCE38)\n",
    "\n",
    "Two vectors are orthogonal when their dot product is zero: $v \\cdot w = v^Tw = 0$. \n",
    "\n",
    "$||v||^2 + ||w||^2 = ||v + w||^2$ by the pythagorean theorem.\n",
    "\n",
    "The row space is perpindicular to the nullspace.  Every row of $A$ is perpindicular to every solution of $Ax=0$.\n",
    "\n",
    "The column space is perpindicular to the nullspace of $A^T$.\n",
    "\n",
    "Two subspaces $V$ and $W$ of the a vector space are **orthogonal** if every vector $v$ in $V$ is perpindicular to every vector $w$ in $W$.\n",
    "\n",
    "Examples for clarification: The floor of your room is a subspace $V$. The line where two walls meet is a one-dimensional subspace $W$.  Those subspaces are orthogonal.  Every vector up the meeting line of the walls is perpindicular to every vector in the floor.\n",
    "\n",
    "In contrast, two walls are not orthogonal.  Their meeting line is in both $V$ and $W$, and the line is not perpindicular to itself.  Two planes (dimensions 2 and 2 in $R^3$) cannot be orthogonal subspaces.\n",
    "\n",
    "When a vector is in two orthogonal subspaces, it must be zero, which is perpindicular to itself.\n",
    "\n",
    "Orthogonality is impossible when dim $V$ + dim $W \\gt$ dim (whole space).\n",
    "\n",
    "This graphic explains why the row space being perpindicular to the nullspace follows from $Ax = 0$:\n",
    "\n",
    "![image.png](images/nullspace-rowspace.png)\n",
    "\n",
    "Every row has a zero dot product with $x$.  So $x$ is also perpindicular to every combination of the rows.  So the whole row space is orthogonal to the nullspace.\n",
    "\n",
    "The same logic we applied to showing why the row space is perpindicular to the nullspace can be applied to recognize the column space as perpindicular to the left null space:\n",
    "\n",
    "![image.png](images/leftnullspace-colspace.png)\n",
    "\n",
    "### Orthogonal Complements\n",
    "\n",
    "The **orthogonal complement** of a subspace $V$ contains every vector that is perpindicular to $V$.  The orthogonal complement is denoted by $V^\\perp$.  \n",
    "\n",
    "The orthogonal complement is the largest dimension orthogonal subspace.\n",
    "\n",
    "Part 2 of the fundamental theorem of linear algebra is that the nullspace is the orthogonal complement of the row space in $R^n$, and the left null space is the orthogonal complement of the column space in $R^m$.\n",
    "\n",
    "The point of complements is that every $x$ can be split into a row space component, $x_r$, and a nullspace component $x_n$.  When $A$ multiplies $x = x_r + x_n$, what happens to $Ax = Ax_r + Ax_n$ is the null space component goes to zero, $Ax_n = 0$, and the row space component goes to the column space $Ax_r = Ax$.\n",
    "\n",
    "If a matrix is of full rank, every vector $b$ in the column space comes from one and only one vector $x_r$ in the row space.\n",
    "\n",
    "There is an $r$ by $r$ invertible matrix hiding inside $A$, if we throw away the two nullspaces.  From the row space to the column space, $A$ is invertible.  Example:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "3 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 5 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\quad\\text{ contains the submatrix }\n",
    "\\begin{bmatrix}\n",
    "3 & 0 \\\\\n",
    "0 & 5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The other eleven zeroes are responsible for the nullspaces.\n",
    "\n",
    "### Combining Bases from Subspaces\n",
    "\n",
    "With bases for the rowspace and nullspace, we have $r + (n -r) = n$ vectors.  Those $n$ vectors are independent.  Therefore they span $R^n$.\n",
    "\n",
    "Each $x$ is the sum $x_r$ + $x_n$ of a rowspace vector $x_r$ and a nullspace vector $x_n$.  For example:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}1 & 2 \\\\ 3 & 6\\end{bmatrix} \\text{ split }x = \\begin{bmatrix}4 \\\\ 3\\end{bmatrix} \\text{ into } x_r + x_n = \\begin{bmatrix}2 \\\\ 4\\end{bmatrix} + \\begin{bmatrix}2 \\\\ -1\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The next section will compute this splitting for any $A$ and $x$, by a projection.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44d6834d",
   "metadata": {},
   "source": [
    "## Projections (4.2)\n",
    "[Lecture here](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/lecture-15-projections-onto-subspaces-1/)\n",
    "\n",
    "Projection matrices are symmetric matrices with $P^2 = P$. The **projection** of $b$ is $Pb$.\n",
    "\n",
    "When a vector $b$ is projected onto a line, it's projection $p$ is the part of $b$ on that line.  When $b$ is projected onto a plane, $p$ is the part in that plane.\n",
    "\n",
    "The projection $p$ is $Pb$, where $P$ is the projection matrix that multiplies $b$ to give $p$.\n",
    "\n",
    "Suppose we have $b = (2,3,4)$. If we wanted to project it onto the $z$ axis, we'd have projection $(0,0,4)$.  If we projected it onto the xy plane, we'd have projection $(2,3,0)$.  Those are the parts of $b$ along the z axis and the xy plane.\n",
    "\n",
    "The projection matrices $P_1$ and $P_2$ are 3 by 3. They multiply $b$ with 3 components to produce $p$ with 3 components. Projection onto a line comes from a rank one matrix.  Projection onto a plane comes from a rank 2 matrix.\n",
    "\n",
    "$$\n",
    "P_1 = \n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "P_2 = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The line and the plane we're projecting onto are orthogonal complements.  Every vector $b$ in the whole space is the sum of its parts in the two subspaces.  The projections $p_1$ and $p_2$ are exactly those two parts of $b$.\n",
    "\n",
    "The vectors give $p_1 + p_2 = b$.  The matrices give $P_1 + P_2 = I$.\n",
    "\n",
    "We can project any vector $b$ onto the column space of of any matrix, which will be our focus.\n",
    "\n",
    "### Projection onto a Line\n",
    "\n",
    "A line goes through the origin in the direction of $a = (a_1, \\dots, a_m)$.  Along that line, we want the point $p$ closest to $b = (b_1, \\dots, b_m)$.  The key to projection is orthogonality.  The line from $b$ to $p$ is perpindicular to the vector $a$.  This is the dotted line marked $e = b - p$ on the left side of the figure below:\n",
    "\n",
    "![image.png](images/projection-error.png)\n",
    "\n",
    "The projection $p$ will be some multiple of $a$.  Call it $p = \\hat{x}a$.  Computing this number $\\hat{x}$ will give the vector $p$.  Then, from formula for $p$, we will read off the projection matrix $P$.  These three steps will lead to all projection matrices:\n",
    "\n",
    "1. Find $\\hat{x}$\n",
    "2. Find the vector $p$\n",
    "3. Find the matrix $P$\n",
    "\n",
    "The dotted line $b - p$ is the **error** $e = b - \\hat{x}a$.  It is perpindicular to $a$-- this will determine $\\hat{x}$.  Use the fact that $b - \\hat{x}a$ is perpindicular to $a$ when their dot product is zero:\n",
    "\n",
    "$$\n",
    "a \\cdot (b - \\hat{x}a) = 0 \\text{ or } a \\cdot b - \\hat{x}a \\cdot a = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{x} = \\frac{a \\cdot b}{a \\cdot a} = \\frac{a^Tb}{a^Ta}\n",
    "$$\n",
    "\n",
    "The multiplication $a^Tb$ is the same as $a \\cdot b$.  Using the transpose is better, because it also applies to matrices. Our formula $\\hat{x} = \\frac{a^Tb}{a^Ta}$ gives the projection $p = \\hat{x}a$.\n",
    "\n",
    "So the projection of $b$ onto the line through $a$ is the vector $p = \\hat{x}a =\\frac{a^Tb}{a^Ta}a$\n",
    "\n",
    "Special cases pop out from this: \n",
    "\n",
    "1. If $b = a$, then $\\hat{x} = 1$.  The projection of $a$ onto itself is itself. $Pa =a$.\n",
    "2. If $b$ is perpindicular to $a$, then $a^Tb = 0$. The projection is $p = 0$.\n",
    "\n",
    "$p$ has length $||p|| = ||b||\\cos{\\theta}$. $e$ has length $||e|| = b\\sin{\\theta}$.\n",
    "\n",
    "The formula for the projection matrix $P$ follows from $p = \\hat{x}a =\\frac{a^Tb}{a^Ta}a$ and $p=Pb$:\n",
    "\n",
    "$$P = \\frac{aa^T}{a^Ta}$$\n",
    "\n",
    "The line we project into is in the column space of $P$.\n",
    "\n",
    "Since $P^2 = P$, projecting a second time changes nothing.\n",
    "\n",
    "The matrix $I - P$ should be a projection too. It produces $e$, the perpindicular part of $b$. Note that $(I - P)b = b - p = e$.  When $P$ projects onto one subspace, $I-P$ projects onto the perpindicular subspace.\n",
    "\n",
    "### Projection onto a subspace\n",
    "\n",
    "Start with vectors $a_1, \\dots, a_n$ in $R^m$.  Assume that these $a$'s are linearly independent. \n",
    "\n",
    "Our problem is to find the combination $p = \\hat{x_1}a_1 + \\cdots + \\hat{x_n}a_n$ closest to a given vector $b$. We are projecting each $b$ in $R^m$ onto the subspace spanned by the $a$'s.\n",
    "\n",
    "With $n=1$ (one vector $a_1$) this is projection onto a line.  The line is the column space of $A$, which has just one column.  In general the matrix $A$ has $n$ columns, $a_1, \\dots, a_n$.\n",
    "\n",
    "The combinations in $R^m$ are the vectors $Ax$ in the column space.  We are looking for the particular combination $p = A\\hat{x}$ (the projection) that is closest to $b$.  The hat over $\\hat{x}$ indicates the best choice $\\hat{x}$, to give the closest vector in the column space. When $n=1$, we saw that choice was $\\hat{x} = \\frac{a^Tb}{a^Ta}$.  For $n>1$, the best $\\hat{x} = (\\hat{x1}, \\dots\\, \\hat{x_n})$ is to be found now.\n",
    "\n",
    "We compute projections onto n-dimensional subspaces in the same three steps as before: find the vector $\\hat{x}$, find the projection $p=A\\hat{x}$, then find the projection matrix $P$.\n",
    "\n",
    "The key to solving this is in the fact that the error vector $e = b - A\\hat{x}$ is perpindicular to the subspace we're projecting upon.  The error $b - A\\hat{x}$ makes a right angle with the vectors $a_1, \\dots, a_n$. The $n$ right angles give the $n$ equations for $\\hat{x}$\n",
    "\n",
    "\n",
    "![image.png](images/perp-subspace.png)\n",
    "\n",
    "The matrix with those rows $a_i^T$ is $a^T$.  The $n$ equations are exactly $A^T(b - A\\hat{x}) = 0$.\n",
    "\n",
    "We rewrite $A^T(b - A\\hat{x}) = 0$ into its famous form $A^TA\\hat{x} = A^Tb$.  This is the equation for $\\hat{x}$, and the coefficient matrix is $A^TA$.  Now we can find $\\hat{x}$ and $p$ and $P$, in that order..\n",
    "\n",
    "The solution for $\\hat{x} = (A^TA)^{-1}A^Tb$. The solution for $p = A\\hat{x}$.  The solution for $P$ comes from $P = A(A^TA)^{-1}A^T$, which is a modification of the equation $p = Pb$.\n",
    "\n",
    "### Invertibility of $A^TA$\n",
    "\n",
    "We will prove that $A^TA$ is invertible only if $A$ has linearly independent columns.\n",
    "\n",
    "We have to show that for every matrix $A$, $A^TA$ has the same nullspace as $A$.  When the columns of $A$ are linearly independent, its nullspace contains only the zero vector.  Then $A^TA$, with the same nullspace, is invertible.\n",
    "\n",
    "Let $A$ be any matrix.  If $x$ is in its nullspace, then $Ax=0$.  Multiplying by $A^T$ gives $A^TAx = 0$. So this $x$ is also in the nullspace of $A^TA$.\n",
    "\n",
    "Now we start with the nullspace of $A^TA$.  From $Ax=0$ we must prove that $Ax=0$.  We multiply by $x^T$:\n",
    "\n",
    "$$(x^T)A^TAx = 0 \\text{ or } (Ax)^T(Ax) = 0 \\text{ or } ||Ax^2|| =0$$\n",
    "\n",
    "We have shown that if $A^TAx=0$ then $Ax$ has length 0.  Therefore $Ax=0$.  Meaning every vector $x$ in one nullspace is in the other nullspace. If $A^TA$ has dependent columns, so has $A$.  If $A^TA$ has independent columns, so has $A$.\n",
    "\n",
    "When $A$ has independent columns, $A^TA$ is square, symmetric, and invertible.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77d1f890",
   "metadata": {},
   "source": [
    "## Least Squares Approximations (4.3)\n",
    "\n",
    "[Lecture](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/lecture-16-projection-matrices-and-least-squares-1/).\n",
    "\n",
    "When the length of $e$ (the error) is as small as possible, $\\hat{x}$ is a **least squares solution**.\n",
    "\n",
    "To fit points $(t_1, b_1), \\dots (t_m, b_m)$ by a straight line, we project onto $A$ with columns $(1,\\dots,1)$ and $t_1,\\dots,t_m$.  When there's no solution to this $Ax=b$, the $\\hat{x}$ provides the best fitting line.\n",
    "\n",
    "### Minimizing the error\n",
    "\n",
    "The best $x$ can be found by geometry, algebra, or calculus.  In geometry, the error space of $e$ meets the column space at $90\\degree$. In algebra, $A^TA\\hat{x} = A^Tb$.  And in Calculus, the derivative of the error $||Ax -b||$ is zero at $\\hat{x}$.\n",
    "\n",
    "The squared length for any $x$: $||Ax - b||^2 = ||Ax - p||^2 + ||e||^2$.  We reduce $Ax - p$ to zero by choosing $x = \\hat{x}$.  This leaves the smallest possible error which we can't reduce.  Notice what \"smallest\" means.  The squared length of $Ax - b$ is minimized.  The least squares solution $\\hat{x}$ makes $E = ||Ax - b||^2$ as small as possible.\n",
    "\n",
    "The errors will add to zero, because they're perpindicular to the 1's in the first column of $A$.\n",
    "\n",
    "### Fitting a straight line\n",
    "\n",
    "$$\n",
    "A \\boldsymbol{x}=\\boldsymbol{b} \\quad \\text { is } \\begin{gathered}\n",
    "C+D t_1=b_1 \\\\\n",
    "C+D t_2=b_2 \\\\\n",
    "\\vdots \\\\\n",
    "C+D t_m=b_m\n",
    "\\end{gathered} \\quad \\text { with } \\quad A\\left[\\begin{array}{cc}\n",
    "1 & t_1 \\\\\n",
    "1 & t_2 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & t_m\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "The closest line $C + Dt$ has heights $p_1, \\dots, p_m$ with errors $e_1, \\dots, e_m$.  We solve $A^TA\\hat{x} = A^Tb$ for $\\hat{x} = (C, D)$.  The errors are $e_i = b_i - C - Dt_i$.\n",
    "\n",
    "The dot-product matrix $A^TA$:\n",
    "\n",
    "$$\n",
    "A^TA = \\begin{bmatrix}1 & \\dots & 1 \\\\ t_1 & \\dots & t_m\\end{bmatrix}\\begin{bmatrix}1 & t_1 \\\\ \\vdots & \\vdots \\\\ 1 & t_m\\end{bmatrix} = \\begin{bmatrix}m & \\sum t_i \\\\ \\sum t_i & \\sum t_i^2\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The right side of the $A^TA\\hat{x} = A^Tb$, i.e. $A^Tb$:\n",
    "\n",
    "$$\n",
    "A^Tb = \\begin{bmatrix}1 & \\dots & 1 \\\\ t_1 & \\dots & t_m\\end{bmatrix}\\begin{bmatrix}b_1 \\\\ \\vdots \\\\ b_m\\end{bmatrix} = \\begin{bmatrix}\\sum b_i \\\\ \\sum t_ib_i\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Putting $A^TA\\hat{x} = A^Tb$ all together we can solve for our best-fit line $C + Dt$ with:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}m & \\sum t_i \\\\ \\sum t_i & \\sum t_i^2\\end{bmatrix}\\begin{bmatrix}C \\\\ D\\end{bmatrix} = \\begin{bmatrix}\\sum b_i \\\\ \\sum t_ib_i\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Projecting to orthogonal $A$\n",
    "\n",
    "$A$ has orthogonal columns when the measurement times $t_i$ add to zero.  For example, suppose $b = (1,2,4)$ at times $t=(-2,0,2)$.  Note how the times add to zero.  So the columns of $A$ have 0 dot product: $(1,1,1)$ is orthogonal to $(-2,0,2)$.  So equation-wise we'd have:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& C+D(-2)=1 \\\\\n",
    "& C+D(0)=2 \\\\\n",
    "& C+D(2)=4\n",
    "\\end{aligned} \\quad \\text { or } \\quad A x=\\left[\\begin{array}{rr}\n",
    "1 & -2 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 2\n",
    "\\end{array}\\right]\\left[\\begin{array}{l}\n",
    "C \\\\\n",
    "D\n",
    "\\end{array}\\right]=\\left[\\begin{array}{l}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "4\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Per our formula above, we have $A^TA = \\begin{bmatrix}3 & 0 \\\\ 0 & 8\\end{bmatrix}$ and $A^Tb = \\begin{bmatrix}7 \\\\ 6\\end{bmatrix}$.  Solving $A^TA\\hat{x} = A^Tb$ we'd do:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}3 & 0 \\\\ 0 & 8\\end{bmatrix}\\begin{bmatrix}C \\\\ D\\end{bmatrix} = \\begin{bmatrix}7 \\\\ 6\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Notice how easily we can solve for $C$ and $D$ here.  The diagonal matrix we get from orthogonal columns is almost as good as the identity matrix in terms of solving.\n",
    "\n",
    "Orthogonal columns are so helpful that it is worth shifting the times by substracting average time, $\\hat{t}$.  If the original times were $1,3,5$, then they'd have an average of $\\hat{t} = 3$.  The shifted times $T = t - \\hat{t} = t - 3$ add up to zero. Now we have the convenient matrix which we projected onto in the beginning of this section.  We'll make use of this \"make the columns orthogonal in advance\" technique later on.\n",
    "\n",
    "### Dependent columns in $A$: What is $\\hat{x}$?\n",
    "\n",
    "So far we've assumed that $A$ has independent columns, allowing us to solve $A^TA\\hat{x} = A^Tb$ for the least squares solution. If $A$ has dependent columns, we will find infinite numbers of ways to project that have the same least squares distance.  In this situation, we should go with the shortest possible length vector as an answer.  This is covered more later in Section 7.4 wrt \"pseudoinverses\".\n",
    "\n",
    "### Fitting by a parabola\n",
    "\n",
    "A parabola is represented by $b = C + Dt + Et^2$.\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}\n",
    "C+D t_1+E t_1^2=b_1 & \\\\\n",
    "\\vdots & \\text { is } A \\boldsymbol{x}=\\boldsymbol{b} \\text { with } \\\\\n",
    "C+D t_m+E t_m^2=b_m & \\text { the } m \\text { by } 3 \\text { matrix }\n",
    "\\end{array} \\quad A=\\left[\\begin{array}{ccc}\n",
    "1 & t_1 & t_1^2 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "1 & t_m & t_m^2\n",
    "\\end{array}\\right]\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "519e86c5",
   "metadata": {},
   "source": [
    "## Orthonormal Bases and Gram-Schmidt (4.4)\n",
    "\n",
    "[Lecture](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/lecture-17-orthogonal-matrices-and-gram-schmidt-1/)\n",
    "\n",
    "The vectors $q_1, \\dots, q_n$ are **orthogonal** when their dot products $q_i \\cdot q_j$ are zero.  More exactly $q_i^Tq_j = 0$ whenever $i \\ne j$. \n",
    "\n",
    "With one more step-- just divide each vector by its length-- the vectors become **orthogonal unit vectors**.  Their lengths are all 1 (normal).  Then the basis is called **orthonormal**.  A matrix with orthonormal columns is assigned the special letter $Q$.\n",
    "\n",
    "The matrix $Q$ is easy to work with because $Q^TQ = I$.  $Q$ is not required to be square.  When $Q$ is square, $Q^TQ = I$ means that $Q^T = Q^{-1}$, that the transpose is the inverse.\n",
    "\n",
    "If the columns are only orthogonal, but not unit vectors, dot products still give a diagonal matrix, but not the identity matrix.  This diagonal matrix is almost as good as $I$.  The important thing is orthogonality-- then it is easy to produce unit vectors.\n",
    "\n",
    "$Q^TQ = I$ even when $Q$ is rectangular.  In that case $Q^T$ is only an inverse from the left. For square matrices we also have $QQ^T = I$, so $Q^T$ is the two-sided inverse of $Q$.  The rows of a square $Q$ are orthonormal like the columns.  The inverse is the transpose.  In the square case we call $Q$ an **orthogonal matrix**. \n",
    "\n",
    "### Example orthogonal matrices\n",
    "\n",
    "$$\n",
    "Q = \\begin{bmatrix}\n",
    "\\cos{\\theta} & -\\sin{\\theta} \\\\  \n",
    "\\sin{\\theta} & \\cos{\\theta}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The columns of $Q$ are orthogonal, which you can see from taking their dot product.  They are unit vectors because $\\sin^2{\\theta} + \\cos^2{\\theta} = 1$. Those columns give an **orthonormal basis** for the plane $R^2$.  $Q^TQ = I$ and $QQ^T = I$.\n",
    "\n",
    "Next example would be any permutation matrix, such as:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "All of the columns of a permutation matrix are unit vectors, as their lengths are obviously 1.  They are also orthogonal because the 1s appear in different places.  The inverse of a permutation matrix is its transpose: $Q^{-1} = Q^T$.  Every permutation matrix is an orthogonal matrix.\n",
    "\n",
    "We can also get an orthogonal matrix by reflection.  Take any unit vector $u$, and set $Q = I - 2uu^T$. As an example, choose the direction $u = (\\frac{-1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}})$.  We compute $2uu^T$ and subtract from $I$ to get the reflection matrix $Q$ in the direction of $u$:\n",
    "\n",
    "$$Q = I - 2\\begin{bmatrix}.5 & -.5 \\\\ -.5 & .5\\end{bmatrix} = \\begin{bmatrix}0 & 1 \\\\ 1 & 0\\end{bmatrix}$$\n",
    "\n",
    "### Multiplication by Q doesnt change length or angle\n",
    "\n",
    "Multiplication by any orthogonal matrix $Q$ does not change length or angle.\n",
    "\n",
    "$||Qx|| = ||x||$ for every vector $x$.  $Q$ also preserves dot products: $(Qx)^T(Qy) = x^Ty$.\n",
    "\n",
    "### Projections using Orthonormal Bases: $Q$ replaces $A$\n",
    "\n",
    "Orthogonal matrices are great for computations-- numbers can never grow too large when lengths of vectors are fixed.  Computers make use of $Q$'s as much as possible.\n",
    "\n",
    "When you're doing projection onto a matrix Q, where the basis vectors are orthonormal, then the $a$'s become the $q$'s.  Then $A^TA$ simplifies to $Q^TQ = I$.  So now $\\hat{x}  = Q^Tb$, $p = Q\\hat{x}$, and $P = QQ^T$.\n",
    "\n",
    "With $Q$ As our basis, there are no matrices to invert.\n",
    "\n",
    "When $Q$ is square, things are even simpler.  In this case $p = b$ and $P=I$.\n",
    "\n",
    "[there's some complex discussion here which I may want to revisit and summarize]\n",
    "\n",
    "### The Gram-Schmidt Process\n",
    "\n",
    "Because orthonormal vectors are so good, we want to work with them.  This section covers the \"**Gram-Schmidt** way\" of creating orthonormal vectors.\n",
    "\n",
    "Start with three independent vectors $a,b,c$.  We intend to construct three orthogonal vectors $A,B,C$.  Then we will divide $A,B,C$ by their lengths.  That produces three orthonormal vectors $q_1 = \\frac{A}{||A||}, q_2 = \\frac{B}{||B||}, q_3 = \\frac{C}{||C||}$.\n",
    "\n",
    "We begin by choosing $A=a$.  This first direction is just accepted as it comes.  The next direction $B$ must be perpindicular to $A$.  Start with $b$ and subtract its projection along $A$.  This leaves the perpindicular part which is the orthogonal vector $B$:\n",
    "\n",
    "$$B = b - \\frac{A^Tb}{A^TA}A$$\n",
    "\n",
    "The third direction starts with $c$.  This is not a combination of $A$ and $B$ because $C$ is not a combination of $a$ and $b$.  But most likely $c$ is not perpindicular to $A$ and $B$.  So subtract off its components in those two directions to get a perpindicular direction $C$:\n",
    "\n",
    "$$C = c - \\frac{A^Tc}{A^TA}A - \\frac{B^Tc}{B^TB}B$$\n",
    "\n",
    "This is the one and only idea of the Gram-Schmidt process.  Subtract from every new vector its projections in the directions already set.  That idea is repeated at every step.  If we had a fourth vector $d$, we would subtract three projections onto $A,B,C$ to get $D$.\n",
    "\n",
    "At the end, or immediately when each one is found, divide the orthogonal vectors $A, B, C$ by their lengths.  The resulting vectors $q_1, q_2, q_3$ are orthonormal.\n",
    "\n",
    "### The Factorization $A = QR$\n",
    "\n",
    "We started with a matrix $A$, whose columns were $a,b,c$.  We ended with a matrix $Q$, whose columns are $q_1, q_2, q_3$.  How are these matrices related?  Since the vectors $a,b,c$ are combinations of the $q$'s and vice versa, there must be a third matrix connecting $A$ to $Q$.  This third matrix is the rectangular $R$ in $A=QR$.\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{lll}\n",
    "\\boldsymbol{a} & \\boldsymbol{b} & \\boldsymbol{c}\n",
    "\\end{array}\\right]=\\left[\\begin{array}{lll}\n",
    "\\boldsymbol{q}_1 & \\boldsymbol{q}_2 & \\boldsymbol{q}_3\n",
    "\\end{array}\\right]\\left[\\begin{array}{lll}\n",
    "\\boldsymbol{q}_1^{\\mathrm{T}} \\boldsymbol{a} & \\boldsymbol{q}_1^{\\mathrm{T}} \\boldsymbol{b} & \\boldsymbol{q}_1^{\\mathrm{T}} \\boldsymbol{c} \\\\\n",
    "& \\boldsymbol{q}_2^{\\mathrm{T}} \\boldsymbol{b} & \\boldsymbol{q}_2^{\\mathrm{T}} \\boldsymbol{c} \\\\\n",
    "& & \\boldsymbol{q}_3^{\\mathrm{T}} \\boldsymbol{c}\n",
    "\\end{array}\\right] \\quad \\text { or } \\quad \\boldsymbol{A}=\\boldsymbol{Q} \\boldsymbol{R} .\n",
    "$$\n",
    "\n",
    "\n",
    "$A=QR$ is Gram-Schmidt in a nutshell.  Multiply by $Q^T$ to recognize $R=Q^TA$.\n",
    "\n",
    "[there's a good amt i'm not groking in this section, hopefully the lecture clears things up.  notes here are incomplete.]\n",
    "\n",
    "Any $m$ by $n$ matrix $A$ with independent columns can be factored into $A=QR$.  the $m$ by $n$ matrix $Q$ has orthonormal columns, and the square matrix $R$ is upper triangular with positive diagonal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f752e13a",
   "metadata": {},
   "source": [
    "# Determinants (5)\n",
    "## The Properties of Determinants (5.1)\n",
    "\n",
    "[Lecture](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/lecture-18-properties-of-determinants-1/)\n",
    "\n",
    "The **determinant** of a square matrix is a single number.  This number contains an enormous amount of information about the matrix.\n",
    "\n",
    "It tells immediately whether the matrix is invertible.  The determinant is zero when the matrix has no inverse.  When $A$ is invertible, the determinant of $A^{-1}$ is $\\frac{1}{\\det A}$. In fact the determinant leads to a formula for every entry in $A^{-1}$.\n",
    "\n",
    "This is one use for determinants-- to find formulas for inverse matrices and pivots and solution $A^{-1}b$.  For a large matrix we seldom use those formulas, because elimination is faster.\n",
    "\n",
    "For a 2 by 2 matrix with entries $a,b,c,d$ its determinant $ad - bc$ shows how $A^{-1}$ changes as $A$ changes.\n",
    "\n",
    "$$A=\\left[\\begin{array}{ll}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{array}\\right] \\quad \\text { has inverse } \\quad A^{-1}=\\frac{1}{\\boldsymbol{a d}-\\boldsymbol{b c}}\\left[\\begin{array}{rr}\n",
    "d & -b \\\\\n",
    "-c & a\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Multiply those matrices to get $I$.  When the determinant is $ad-bc = 0$, we are asked to divide by zero and we can't-- then $A$ has no inverse.  The rows are parallel when $\\frac{a}{c} = \\frac{b}{d}$.  This gives $ad=bc$ and $\\det A = 0$.  Dependent rows always lead to $\\det A = 0$.\n",
    "\n",
    "The determinant is also connected to the pivots.  For a 2 by 2 matrix, the pivots are $a$ and $d - \\frac{c}{a}b$.  The product of the pivots is the determinant:\n",
    "\n",
    "$$a(d - \\frac{c}{a}b) = ad - bc = \\det A$$\n",
    "\n",
    "After a row exchange the pivots change to $c$ and $b - \\frac{a}{c}d$.  Those new pivots multiply to give $bc - ad$.  The row exchange to $\\begin{bmatrix}c & d \\\\ a & b\\end{bmatrix}$ reversed the sign of the determinant.\n",
    "\n",
    "The determinant of an $n$ by $n$ matrix can be found in three ways:\n",
    "\n",
    "1. The **pivot formula** - Multiply the $n$ pivots (times 1 or -1)\n",
    "2. The **\"big formula\"** - Add up $n!$ terms (times 1 or -1)\n",
    "3. The **cofactor formula** -- Combine $n$ smaller determinants (times 1 or -1)\n",
    "\n",
    "You can see from that list that plus or minus signs-- the decisions between 1 and -1-- play a big part in determinants.  This comes from this rule for $n$ by $n$ matrices: The determinant changes sign when two rows (or two columns) are exchanged.\n",
    "\n",
    "The identity matrix has determinant +1.  Exchange two rows and $\\det P = -1$.  Exchange two more rows and the new permuation has $\\det P = +1$.  Half of all permutations are _even_ ($\\det P = 1$) and half are _odd_ ($\\det P = -1$).  Starting from $I$, half of the $P$'s involve an even number of exchanges and half require an odd number.\n",
    "\n",
    "$$\n",
    "\\det\\begin{bmatrix}1 & 0 \\\\ 0 & 1\\end{bmatrix} = 1 \n",
    "\\quad\\text{and}\\quad\n",
    "\\det\\begin{bmatrix}0 & 1 \\\\ 1 & 0\\end{bmatrix} = -1 \n",
    "$$\n",
    "\n",
    "The other essential rule is linearity.  Linearity does not mean $\\det{A + B} = \\det{A} + \\det{B}$.  The true rule is $\\det 2I = 2^n$.  Determinants are multiplied by $2^n$ (not just 2) when matrices are multiplied by 2.\n",
    "\n",
    "### Properties of the determinant\n",
    "\n",
    "Determinants have three basic properties.  By using these rules we can compute the determinant of any square matrix $A$.  This number is written in two ways, $\\det A$ and $|A|$.\n",
    "\n",
    "We will be going through these rules on the simple case of a 2x2 matrix, but remember that these apply to to any $n$ by $n$ matrix $A$.  Rules 4-10 follow from rules 1-3.\n",
    "\n",
    "1\\. The determinant of the $n$ by $n$ identity matrix is $1$:\n",
    "\n",
    "$$\n",
    "\\left|\\begin{array}{ll}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{array}\\right|=1 \\quad \\text { and } \\quad\\left|\\begin{array}{lll}\n",
    "1 & & \\\\\n",
    "& \\ddots & \\\\\n",
    "& & 1\n",
    "\\end{array}\\right|\n",
    "$$\n",
    "\n",
    "2\\. The determinant changes sign when two rows are exchanged (sign reversal):\n",
    "\n",
    "$$\n",
    "\\left|\\begin{array}{ll}\n",
    "c & d \\\\\n",
    "a & b\n",
    "\\end{array}\\right|=-\\left|\\begin{array}{ll}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{array}\\right| \\quad \\text { (both sides equal } b c-a d \\text { ). }\n",
    "$$\n",
    "\n",
    "Because of this rule, we can find $\\det P$ for any permutation matrix.  Just exchange rows of $I$ until you reach $P$.  Then $\\det P = +1$ for an even number of row exchanges, and $\\det P = -1$ for an odd number.  \n",
    "\n",
    "The third rule makes the big jump to determinants of all matrices.\n",
    "\n",
    "3\\. The determinant is a linear function of each row separately (all other rows stay fixed). If the first row is multiplied by $t$, the determinant is multiplied by $t$.  If first rows are added, determinants are added.  This rule only applies when the other rows do not change!  Notice how $c$ and $d$ stay the same:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\left|\\begin{array}{cc}\n",
    "t a & t b \\\\\n",
    "c & d\n",
    "\\end{array}\\right|=t\\left|\\begin{array}{ll}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{array}\\right| \\\\\n",
    "& \\left|\\begin{array}{cc}\n",
    "a+a^{\\prime} & b+b^{\\prime} \\\\\n",
    "c & d\n",
    "\\end{array}\\right|=\\left|\\begin{array}{ll}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{array}\\right|+\\left|\\begin{array}{cc}\n",
    "a^{\\prime} & b^{\\prime} \\\\\n",
    "c & d\n",
    "\\end{array}\\right|\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "You can see these rules are true. In the first case, both sides are $tad - tbc$.  Then the $t$ factors out.  In the second case, both sides are $ad + a^{\\prime}d - bc - b^{\\prime}c$.  These rules still apply when $A$ is $n$ by $n$, and one row changes.\n",
    "\n",
    "$$\n",
    "A=\\left|\\begin{array}{lll}\n",
    "\\mathbf{4} & \\mathbf{8} & \\mathbf{8} \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{array}\\right|=\\mathbf{4}\\left|\\begin{array}{lll}\n",
    "\\mathbf{1} & \\mathbf{2} & \\mathbf{2} \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{array}\\right| \\text { and }\\left|\\begin{array}{lll}\n",
    "\\mathbf{4} & \\mathbf{8} & \\mathbf{8} \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{array}\\right|=\\left|\\begin{array}{lll}\n",
    "\\mathbf{4} & \\mathbf{0} & \\mathbf{0} \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{array}\\right|+\\left|\\begin{array}{lll}\n",
    "\\mathbf{0} & \\mathbf{8} & \\mathbf{8} \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{array}\\right|\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cace507d",
   "metadata": {},
   "source": [
    "Combining multiplication and addition, we can get any linear combination in one row.  Rule 2 for row exchanges can put that row into the first row and back again.\n",
    "\n",
    "4\\. If two rows of $A$ are equal, then $\\det A = 0$.\n",
    "\n",
    "$$\n",
    "\\left|\\begin{array}{ll}\n",
    "a & b \\\\\n",
    "a & b\n",
    "\\end{array}\\right|=0\n",
    "$$\n",
    "\n",
    "Rule 4 follows from rule 2. Exchanging the two equal rows, the determinant is supposed to change sign.  But it also has to stya the same, because the matrix hasn't change. The only number $D = -D$ is $0$.\n",
    "\n",
    "5\\. Subtracting a multiple of one row from another row leaves $\\det A$ unchanged.\n",
    "\n",
    "$$\n",
    "\\left|\\begin{array}{cc}\n",
    "a & b \\\\\n",
    "c-l a & d-\\ell b\n",
    "\\end{array}\\right|=\\left|\\begin{array}{cc}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{array}\\right|\n",
    "$$\n",
    "\n",
    "Rule 3 (linearity) split the left side into the right side plus another term $-l\\left|\\begin{array}{cc}a & b \\\\ a & b\\end{array}\\right|$.  This extra term is zero by rule 4: equal rows.  Therefore rule 5 is correct.\n",
    "\n",
    "We can conclude then that the determinant is not changed by the usual elimination steps form $A$ to $U$. Thus $\\det A = \\det U$.  Every row exchange reverses the sign, so always $\\det A = \\pm \\det U$.  So this rule has narrowed our problem of finding determinants to triangular matrices.\n",
    "\n",
    "6\\. A matrix with a row of 0s has $\\det A = 0$.\n",
    "\n",
    "$$\n",
    "\\left|\\begin{array}{ll}\n",
    "0 & 0 \\\\\n",
    "c & d\n",
    "\\end{array}\\right|=0 \\quad \\text { and } \\quad\\left|\\begin{array}{ll}\n",
    "a & b \\\\\n",
    "0 & 0\n",
    "\\end{array}\\right|=0\n",
    "$$\n",
    "\n",
    "For an easy proof, add some other row to the zero row.  The determinant is not changed (rule 5).  But the matrix now has two equal rows. So $\\det A = 0$ by rule 4.\n",
    "\n",
    "7\\. If $A$ is triangular then $\\det A = a_{11}a_{22}\\dots a_{nn} =$ product of diagonal entries.\n",
    "\n",
    "$$\n",
    "\\left|\\begin{array}{ll}\n",
    "a & b \\\\\n",
    "0 & d\n",
    "\\end{array}\\right|=a d \\quad \\text { and also } \\quad\\left|\\begin{array}{ll}\n",
    "a & 0 \\\\\n",
    "c & d\n",
    "\\end{array}\\right|=a d\n",
    "$$\n",
    "\n",
    "Suppose all diagonal entries are nonzero.  remove the off-diagonal entries by elimination!  If $A$ is lower triangular, subtract multiples of each row from lower rows.  If $A$ is upper triangular, subtract from higher rows.  By Rule 5 the determinant is not changed, and now the matrix is diagonal.\n",
    "\n",
    "Factor $a_{11}$ from the first row by rule 3.  Then factor $a_{22}$ from the second row.  Eventually factor $a_{nn}$ from the last row.  The determinant is now all those diagonal factors eventually times $\\det I$.  Finally, per rule 1, $\\det I = $, proving our rule here.\n",
    "\n",
    "8\\. If $A$ is singular then $\\det A = 0$.  If $A$ is invertible then $\\det A \\ne 0$\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{ll}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{array}\\right] \\quad \\text { is singular if and only if } \\quad a d-b c=0\n",
    "$$\n",
    "\n",
    "Let's go over the proof.  Elimination goes from $A$ to $U$.  If $A$ is singular then $U$ has a zero row.  The rules give $\\det A = \\det U = 0$.  If $A$ is invertible then $U$ has pivots along its diagonal.  The product of nonzero pivots (using rule 7) gives a nonzero determinant:\n",
    "\n",
    "$$\n",
    "\\operatorname{det} A= \\pm \\operatorname{det} U= \\pm \\text { (product of the pivots) }\n",
    "$$\n",
    "\n",
    "The pivots of a 2 by 2 matrix (if $a \\ne 0$) are $a$ and $d - (\\frac{c}{a})b$:\n",
    "\n",
    "$$\n",
    "\\text { The determinant is }\\left|\\begin{array}{ll}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{array}\\right|=\\left|\\begin{array}{cc}\n",
    "a & b \\\\\n",
    "0 & d-(c / a) b\n",
    "\\end{array}\\right|=a d-b c\n",
    "$$\n",
    "\n",
    "This is the first formula for the determinant.  The sign in $\\pm \\det U$ dependents on whether the number of row exchanges is even or odd: +1 or -1 is the determinant of the permutation $P$ that exchanges rows.\n",
    "\n",
    "With no row exchanges, $P = I$ and $\\det A = \\det U =$ product of pivots. And $\\det L = 1$:\n",
    "\n",
    "If $PA = LU$ then $\\det P \\det A = \\det L \\det U$ and $\\det A = \\pm \\det U$.\n",
    "\n",
    "9\\. The determinant of $AB$ is $\\det A$ times $\\det B$: $|AB| = |A||B|$.\n",
    "\n",
    "When the matrix $B$ is $A^{-1}$, this rule says that the determinant of $A^{-1}$ is $\\frac{1}{\\det A}$.\n",
    "\n",
    "$$\n",
    "A A^{-1}=I \\text { so }(\\operatorname{det} A)\\left(\\operatorname{det} A^{-1}\\right)=\\operatorname{det} I=1 .\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78c9d8b6",
   "metadata": {},
   "source": [
    "10\\. The transpose $A^T$ has the same determinant as $A$.\n",
    "\n",
    "$$\n",
    "\\left|\\begin{array}{ll}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{array}\\right|=\\left|\\begin{array}{ll}\n",
    "a & c \\\\\n",
    "b & d\n",
    "\\end{array}\\right| \\quad \\text { since both sides equal } a d-b c\n",
    "$$\n",
    "\n",
    "Because of this rule, every rule for the rows can apply to the columns, just by transposing since $|A| = |A^T|$.  The determinant changes sign when two columns are exchanged. A zero column or two equal columns will make the determinant zero. If a column is multiplied by $t$, so is the determinant.  The determinant is a linear function of each column separately.  In the next section we'll find an explicit formula for the determinant. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab8b285c",
   "metadata": {},
   "source": [
    "## Permutations and Cofactors  (5.2)\n",
    "\n",
    "[Lecture](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/lecture-19-determinant-formulas-and-cofactors-1/)\n",
    "\n",
    "A computer finds the determinant from the pivots.  This sections explains to other ways to do it.  There is a **big formula** using all $n!$ permutations.  There is a **cofactor formula** using determinants of size $n - 1$.\n",
    "\n",
    "### The Pivot Formula\n",
    "\n",
    "When elimination leads to $A=LU$, the pivots are on the diagonal of the upper triangular $U$.  If no row exchanges are involved, we multiply those pivots to find the determinant.\n",
    "\n",
    "$$\n",
    "\\operatorname{det} A=(\\operatorname{det} L)(\\operatorname{det} U)=(1)\\left(d_1 d_2 \\cdots d_n\\right)\n",
    "$$\n",
    "\n",
    "This formula for $\\det A$ appeared in the previous section, with the further possibility of row exchanges.  Then a permutation enters $PA = LU$.  The determinant of $P$ is $-1$ or $+1$.\n",
    "\n",
    "$$\n",
    "(\\operatorname{det} \\boldsymbol{P})(\\operatorname{det} \\boldsymbol{A})=(\\operatorname{det} \\boldsymbol{L})(\\operatorname{det} \\boldsymbol{U}) \\quad \\text { gives } \\quad \\operatorname{det} A= \\pm\\left(d_1 d_2 \\cdots d_n\\right) \\text {. }\n",
    "$$\n",
    "\n",
    "An example:\n",
    "\n",
    "$$\n",
    "A=\\left[\\begin{array}{lll}\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 2 & 3 \\\\\n",
    "4 & 5 & 6\n",
    "\\end{array}\\right] \\quad P A=\\left[\\begin{array}{lll}\n",
    "4 & 5 & 6 \\\\\n",
    "0 & 2 & 3 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{array}\\right] \\quad \\operatorname{det} A=-(4)(2)(1)=-8\n",
    "$$\n",
    "\n",
    "The odd number of row exchanges (1) means that $\\det P = -1$.\n",
    "\n",
    "For matrices without row exchanges, the first $k$ pivots come from the $k$ by $k$ matrix $A_k$ in the top left corner of $A$.  The determinant of that corner submatrix $A_k$ is $d_1 d_2 \\dots d_k$ (first $k$ pivots).\n",
    "\n",
    "Assuming no row exhcnages, then $A = LU$ and $A_k = L_kU_k$.  Dividing one determinant by the previous determinant ($\\det A_k$ divided by $\\det A_{k-1}$ cancels everything but the latest pivot $d_k$.  Each pivot is a ratio of determinants:\n",
    "\n",
    "$$\n",
    "\\text { The } k \\text { th pivot is } d_k=\\frac{d_1 d_2 \\cdots d_k}{d_1 d_2 \\cdots d_{k-1}}=\\frac{\\operatorname{det} A_k}{\\operatorname{det} A_{k-1}}\n",
    "$$\n",
    "\n",
    "### The Big Formuila for Determinants\n",
    "\n",
    "Pivots are good for computing.  They concentrate a lot of information, enough to find the determinant.  But it is hard to connect them to the original $a_{ij}$.  that part will be clearer if we go back to rules 1-3, linearity and sign reversal and $\\det I = 1$.  We want to derivate a single explicit formula for the determinant, directly from the entries $a_{ij}$.\n",
    "\n",
    "The formula has $n!$ terms.  Its size grows pretty fast then because, $n! = 1,2,6,24,120,\\dots$.  By $n=11$, there are about forty million terms.  For $n=2$, the two terms are $ad$ and $bc$.\n",
    "\n",
    "Half the terms have minus signs (as in $-bc$).  The other half have plus signs (as in $ad$).  For $n=3$ there are $3! = 6$ terms.  Here are those six terms:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& 3 \\text { by } 3 \\\\\n",
    "& \\text { determinant }\n",
    "\\end{aligned}\\left|\\begin{array}{lll}\n",
    "a_{11} & a_{12} & \\boldsymbol{a}_{\\mathbf{1 3}} \\\\\n",
    "\\boldsymbol{a}_{\\mathbf{2 1}} & a_{22} & a_{23} \\\\\n",
    "a_{31} & \\boldsymbol{a}_{\\mathbf{3}} & a_{33}\n",
    "\\end{array}\\right|=\\begin{aligned}\n",
    "& +a_{11} a_{22} a_{33}+a_{12} a_{23} a_{31}+\\boldsymbol{a}_{\\mathbf{1 3}} \\boldsymbol{a}_{\\mathbf{2 1}} \\boldsymbol{a}_{\\mathbf{3 2}} \\\\\n",
    "& -a_{11} a_{23} a_{32}-a_{12} a_{21} a_{33}-a_{13} a_{22} a_{31}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Notice the pattern.  Each product like $a_{11}a_{23}a_{32}$ has one entry from each row.  It also has one entry from each column.  It will be \"permutations\" which tell us the sign of each term.\n",
    "\n",
    "The next step ($n = 4$) brings $4! = 24$ terms.  There are 24 ways to choose one entry from each row and column.  Down the main diagonal, $a_{11}a_{22}a_{33}a_{44} with column order 1,2,3,4 always has a plus sign.  That is the \"identity permutation.\"\n",
    "\n",
    "To derive the big formula, let's start with $n=2$.  The goal is to reach $ad - bc$ in a systematic way.  Break each row into two simpler rows:\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{ll}\n",
    "a & b\n",
    "\\end{array}\\right]=\\left[\\begin{array}{ll}\n",
    "a & 0\n",
    "\\end{array}\\right]+\\left[\\begin{array}{ll}\n",
    "0 & b\n",
    "\\end{array}\\right] \\quad \\text { and } \\quad\\left[\\begin{array}{ll}\n",
    "c & d\n",
    "\\end{array}\\right]=\\left[\\begin{array}{ll}\n",
    "c & 0\n",
    "\\end{array}\\right]+\\left[\\begin{array}{ll}\n",
    "0 & d\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Now apply linearity, first in row 1 (with row 2 fixed) and then in row 2 (with row 1 fixed):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left|\\begin{array}{ll}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{array}\\right| & =\\left|\\begin{array}{ll}\n",
    "a & 0 \\\\\n",
    "c & d\n",
    "\\end{array}\\right|+\\left|\\begin{array}{ll}\n",
    "0 & b \\\\\n",
    "c & d\n",
    "\\end{array}\\right| \\\\\n",
    "& =\\left|\\begin{array}{ll}\n",
    "a & 0 \\\\\n",
    "c & 0\n",
    "\\end{array}\\right|+\\left|\\begin{array}{ll}\n",
    "a & 0 \\\\\n",
    "0 & d\n",
    "\\end{array}\\right|+\\left|\\begin{array}{ll}\n",
    "0 & b \\\\\n",
    "c & 0\n",
    "\\end{array}\\right|+\\left|\\begin{array}{ll}\n",
    "0 & b \\\\\n",
    "0 & d\n",
    "\\end{array}\\right|\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The last line has $2^2 = 4$ determinants.  The first and fourht are zero because one row is a multiple of the other row.  We are left with $2! = 2$ determinants to compute:\n",
    "\n",
    "$$\n",
    "\\left|\\begin{array}{ll}\n",
    "a & 0 \\\\\n",
    "0 & d\n",
    "\\end{array}\\right|+\\left|\\begin{array}{ll}\n",
    "0 & b \\\\\n",
    "c & 0\n",
    "\\end{array}\\right|=a d\\left|\\begin{array}{ll}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{array}\\right|+b c\\left|\\begin{array}{ll}\n",
    "0 & 1 \\\\\n",
    "1 & 0\n",
    "\\end{array}\\right|=a d-b c\n",
    "$$\n",
    "\n",
    "The splitting led to permutation matrices.  Their determinants give a plus or minus sign.  The permutation tells the column sequence. In this case the column order if $(1,2)$ or $(2,1)$.\n",
    "\n",
    "Now try $n=3$.  Each row splits into 3 simpler rows like $\\begin{bmatrix}a_{11} & 0 & 0\\end{bmatrix}$.  Using linearity in each row, $\\det A$ splits into $3^3 = 27$ simple determinants.  If a column choice is repeated-- for example if we also choose the row  $\\begin{bmatrix}a_{21} & 0 & 0\\end{bmatrix}$-- then the simple determinant is zero.\n",
    "\n",
    "We pay attention only when the entries $a_{ij}$ come from different columns like (3,1,2):\n",
    "\n",
    "![image.png](images/det-six-terms.png)\n",
    "\n",
    "There are $3! = 6$ ways to order the columns, so six determinants.  The six permutations of $(1,2,3)$ include the identity permutation $(1,2,3)$ from $P=I$.\n",
    "\n",
    "$$\n",
    "\\text { Column numbers }=(1,2,3),(2,3,1),(\\mathbf{3}, \\mathbf{1}, \\mathbf{2}),(1,3,2),(2,1,3),(3,2,1)\n",
    "$$\n",
    "\n",
    "The last three are odd permutations (one exchange).  The first three are even permutations (0 or 2 exchanges).  When the column sequence is (3,1,2) we have chosen the entries $a_{13}a_{21}a_{32}$-- that particular column sequence comes with a plus sign (2 exchanges).  The determinant of $A$ is now split into six simple terms.  We factor out the $a_{ij}$:\n",
    "\n",
    "![image.png](images/det-factored.png)\n",
    "\n",
    "The first three (even) permutations have $\\det P = +1$, the last three (odd) permutations have $\\det P = -1$.  We have proved the 3 by 3 formula in a systematic way.\n",
    "\n",
    "Now you can see the $n$ by $n$ formula. There are $n!$ orderings of the columns.  The columns $(1,2,\\dots,n)$ go in each possible order.  Eventually we have the determinant, with half the column orderings having sign -1.\n",
    "\n",
    "The determinant of $A$ is the sum of these $n!$ simple determinants, times 1 or -1.  The simple determinants choose one entry from every row and column.  For 5 by 5, the term $a_{15}a_{22}a_{33}a_{44}a_{51}$ would have $\\det P = -1$ from exchanging 5 and 1.\n",
    "\n",
    "When $U$ is upper triangular, only one of the $n!$ products can be nonzero.  This one term comes from the diagonal $\\det U = +u_{11}u_{22}\\dots u_{nn}$.  All other column orderings pick at least one entry below the diagonal, where $U$ has zeros.  As soon as we pick a number like $u_{21} = 0$, that term is sure to be zero.\n",
    "\n",
    "### Determinant by Cofactors\n",
    "\n",
    "If we a separate the 6 terms from a 3x3 matrix into 3 pairs we get:\n",
    "\n",
    "$$\n",
    "\\operatorname{det} A=a_{11}\\left(a_{22} a_{33}-a_{23} a_{32}\\right)+\\boldsymbol{a}_{\\mathbf{1 2}}\\left(a_{23} a_{31}-a_{21} a_{33}\\right)+\\boldsymbol{a}_{\\mathbf{1 3}}\\left(a_{21} a_{32}-a_{22} a_{31}\\right) \\text {. }\n",
    "$$\n",
    "\n",
    "Those 3 quantities in parentheses are called **cofactors**.  They are 2 by 2 determinants, from rows 2 and 3.  The first row contributes the factors $a_{11},a_{12},a_{13}$.  The lower rows contribute the cofactors $C_{11},C_{12},C_{13}$.\n",
    "\n",
    "The cofactor of $a_{11}$ is $C_{11} = a_{22}a_{33} - a_{23}a_{32}$.  You can see it in this splitting:\n",
    "\n",
    "$$\n",
    "\\left|\\begin{array}{lll}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{array}\\right|=\\left|\\begin{array}{lll}\n",
    "a_{11} & & \\\\\n",
    "& a_{22} & a_{23} \\\\\n",
    "& a_{32} & a_{33}\n",
    "\\end{array}\\right|+\\left|\\begin{array}{lll} \n",
    "& a_{12} & \\\\\n",
    "a_{21} & & a_{23} \\\\\n",
    "a_{31} & & a_{33}\n",
    "\\end{array}\\right|+\\left|\\begin{array}{lll} \n",
    "& & a_{13} \\\\\n",
    "a_{21} & a_{22} & \\\\\n",
    "a_{31} & a_{32} &\n",
    "\\end{array}\\right|\n",
    "$$\n",
    "\n",
    "We are still choosing one entry from each row and column.  Since $a_{11}$ uses up row 1 and column 1, that leaves a 2 by 2 determinant as its cofactor.\n",
    "\n",
    "As always, we have to watch signs.  The sign pattern for cofactors along the first row is plus-minus-plus-minus.  The cofactors along row 1 are $C_{1 j}=(-1)^{1+j} \\operatorname{det} M_{1 j}$. The cofactor expansion is det $A=a_{11} C_{11}+a_{12} C_{12}+\\cdots+a_{1 n} C_{1 n}$.\n",
    "\n",
    "Note that what we're doing with the first row, could be done with subequent rows, so long as we follow the checkerboard sign pattern of multiplying by $(-1)^{i + j}$.\n",
    "\n",
    "Cofactors are useful when matrices have many zeros. For example:\n",
    "\n",
    "$$\n",
    "\\left|\\begin{array}{rrrr}\n",
    "2 & -1 & & \\\\\n",
    "-\\mathbf{1} & 2 & -\\mathbf{1} & \\\\\n",
    "& -1 & \\mathbf{2} & -\\mathbf{1} \\\\\n",
    "& & -\\mathbf{1} & \\mathbf{2}\n",
    "\\end{array}\\right|=2\\left|\\begin{array}{rrr}\n",
    "2 & -1 & \\\\\n",
    "-1 & 2 & -1 \\\\\n",
    "& -1 & 2\n",
    "\\end{array}\\right|-(-1)\\left|\\begin{array}{rrr}\n",
    "-1 & -\\mathbf{1} & \\\\\n",
    "& \\mathbf{2} & \\mathbf{- 1} \\\\\n",
    "& -\\mathbf{1} & \\mathbf{2}\n",
    "\\end{array}\\right|\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1801beb4",
   "metadata": {},
   "source": [
    "## Cramerâ€™s Rule, Inverses, and Volumes (5.3)\n",
    "\n",
    "[Lecture](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/lecture-20-cramers-rule-inverse-matrix-and-volume-1/)\n",
    "\n",
    "This chapter solves $Ax=b$ and also finds $A^{-1}$ by algebra, rather than elimination like previously.  In all the formulas you will see division by $\\det A$.  Each entry in $A^{-1}$ and $A^{-1}b$ is a determinant divided by the determinant of $A$.  Let's start with Cramer's Rule.\n",
    "\n",
    "Cramer's Rule solves $Ax=b$.  A neat idea gives the first component $x_1$.  If we replace the first column of $I$ by $x$, we get a matrix with determinant $x_1$.  When we multiply it by $A$, the first column becomes $Ax$ which is $b$.  The other columns of $B_1$ are copied from $A$:\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{ll}\n",
    "A & \\\\\n",
    "&\n",
    "\\end{array}\\right]\\left[\\begin{array}{lll}\n",
    "\\boldsymbol{x}_{\\mathbf{1}} & 0 & 0 \\\\\n",
    "\\boldsymbol{x}_{\\mathbf{2}} & 1 & 0 \\\\\n",
    "\\boldsymbol{x}_{\\mathbf{3}} & 0 & 1\n",
    "\\end{array}\\right]=\\left[\\begin{array}{lll}\n",
    "\\boldsymbol{b}_{\\mathbf{1}} & a_{12} & a_{13} \\\\\n",
    "\\boldsymbol{b}_{\\mathbf{2}} & a_{22} & a_{23} \\\\\n",
    "\\boldsymbol{b}_{\\mathbf{3}} & a_{32} & a_{33}\n",
    "\\end{array}\\right]=B_1\n",
    "$$\n",
    "\n",
    "We multiplied a column at a time.  We can take determinants of the three matrices to find $x_1$, because by the product rule:\n",
    "\n",
    "$$\n",
    "(\\operatorname{det} A)\\left(x_1\\right)=\\operatorname{det} B_1 \\quad \\text { or } \\quad x_1=\\frac{\\operatorname{det} B_1}{\\operatorname{det} A}\n",
    "$$\n",
    "\n",
    "This is the first component of $x$ in Cramer's Rule.  Changing a column of $A$ gave $B_1$.  To find $x_2$ and $B_2$, put the vectors $x$ and $b$ into the 2nd columns of $I$ and $A$:\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{ccc}\n",
    "a_1 & a_2 & a_3\n",
    "\\end{array}\\right]\\left[\\begin{array}{lll}\n",
    "1 & x_1 & 0 \\\\\n",
    "0 & x_2 & 0 \\\\\n",
    "0 & x_3 & 1\n",
    "\\end{array}\\right]=\\left[\\begin{array}{lll}\n",
    "a_1 & b & a_3 \\\\\n",
    "& &\n",
    "\\end{array}\\right]=B_2\n",
    "$$\n",
    "\n",
    "We again take determinants, to find $(\\det A)(x_2) = \\det B_2$.  This gives $x_2 = \\frac{\\det B_2}{\\det A}$.\n",
    "\n",
    "With that established, let's formally state **Cramer's Rule**.  If $\\det A$ is not zero, $Ax=b$ is solved by determinants:\n",
    "\n",
    "$$\n",
    "x_1 = \\frac{\\det B_1}{\\det A} \\quad x_2 = \\frac{\\det B_2}{\\det A} \\quad \\dots \\quad x_n = \\frac{\\det B_n}{\\det A}\n",
    "$$\n",
    "\n",
    "To solve an $n$ by $n$ system, Cramer's Rule evaluates $n+1$ determinants (of $A$ and the $n$ different $B$'s).  When each one is the sum of $n!$ terms-- by the \"big formula\" with all permutations-- this makes a total of $(n + 1)!$ terms.  It would be crazy to solve equations that way.  But we do finally have an explicit formula for the solution $x$.\n",
    "\n",
    "Let's look at an instructive example.  For $n=2$, find the columns of $A^{-1} = \\begin{bmatrix}x & y\\end{bmatrix}$ by solving $AA^{-1} = I$:\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{ll}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{array}\\right\\rfloor\\left\\lfloor\\begin{array}{l}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{array}\\right]=\\left[\\begin{array}{l}\n",
    "\\mathbf{1} \\\\\n",
    "\\mathbf{0}\n",
    "\\end{array}\\right] \\quad\\left[\\begin{array}{ll}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{array}\\right]\\left[\\begin{array}{l}\n",
    "y_1 \\\\\n",
    "y_2\n",
    "\\end{array}\\right]=\\left[\\begin{array}{l}\n",
    "\\mathbf{0} \\\\\n",
    "\\mathbf{1}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Those share the same matrix $A$.  We need $|A|$ and four determinants for $x_1, x_2, y_1, y_2$:\n",
    "\n",
    "$$\n",
    "\\left|\\begin{array}{ll}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{array}\\right| \\text { and }\\left|\\begin{array}{ll}\n",
    "\\mathbf{1} & b \\\\\n",
    "\\mathbf{0} & d\n",
    "\\end{array}\\right| \\quad\\left|\\begin{array}{ll}\n",
    "a & \\mathbb{1} \\\\\n",
    "c & \\mathbf{0}\n",
    "\\end{array}\\right| \\quad\\left|\\begin{array}{ll}\n",
    "\\mathbf{0} & b \\\\\n",
    "\\mathbf{1} & d\n",
    "\\end{array}\\right| \\quad\\left|\\begin{array}{ll}\n",
    "a & \\mathbf{0} \\\\\n",
    "c & \\mathbf{1}\n",
    "\\end{array}\\right|\n",
    "$$\n",
    "\n",
    "The last four determinants are $d, -c, -b,$ and $a$.  They are the cofactors!  Here is $A^{-1}$:\n",
    "\n",
    "$$\n",
    "x_1=\\frac{d}{|A|}, x_2=\\frac{-c}{|A|}, y_1=\\frac{-b}{|A|}, y_2=\\frac{a}{|A|} \\text { and then } A^{-1}=\\frac{1}{a d-b c}\\left[\\begin{array}{rr}\n",
    "d & -b \\\\\n",
    "-c & a\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "We chose a 2x2 matrix here so that the main point could come through clearly.  The key idea here is that $A^{-1}$ involves the cofactors. When the right side is a column of the identity matrix $I$, as in $AA^{-1} = I$, the determinant of each $B_j$ in Cramer's Rule is a cofactor of $A$.\n",
    "\n",
    "Let's look at the cofactors for $n=3$.  We solve $Ax = (1,0,0)$ to find column 1 of $A^{-1}$.  Our determinants of our $B$'s will be cofactors of $A$:\n",
    "\n",
    "$$\n",
    "\\left|\\begin{array}{lll}\n",
    "\\mathbf{1} & a_{12} & a_{13} \\\\\n",
    "\\mathbf{0} & a_{22} & a_{23} \\\\\n",
    "\\mathbf{0} & a_{32} & a_{33}\n",
    "\\end{array}\\right| \\quad\\left|\\begin{array}{lll}\n",
    "a_{11} & \\mathbf{1} & a_{13} \\\\\n",
    "a_{21} & \\mathbf{0} & a_{23} \\\\\n",
    "a_{31} & \\mathbf{0} & a_{33}\n",
    "\\end{array}\\right| \\quad\\left|\\begin{array}{lll}\n",
    "a_{11} & a_{12} & \\mathbf{1} \\\\\n",
    "a_{21} & a_{22} & \\mathbf{0} \\\\\n",
    "a_{31} & a_{32} & \\mathbf{0}\n",
    "\\end{array}\\right|\n",
    "$$\n",
    "\n",
    "Don't forget the sign flips based on where the $1$ is at (checkerboard pattern).\n",
    "\n",
    "Putting this all together, we have a formula for $A^{-1}$.  Notice that the $i, j$ entry of $A^{-1} is the cofactor $C_{ji}$ (not $C_{ij}$) divided by $\\det A$:\n",
    "\n",
    "$$\n",
    "\\left(A^{-1}\\right)_{i j}=\\frac{C_{j i}}{\\operatorname{det} A} \\quad \\text { and } \\quad A^{-1}=\\frac{C^{\\mathrm{T}}}{\\operatorname{det} A}\n",
    "$$\n",
    "\n",
    "Those cofactors $C_{ij}$ go into the \"cofactor matrix\" $C$.  The transpose of $C$ leads to $A^{-1}$.\n",
    "\n",
    "We can prove this formula $A^{-1} = \\frac{C^T}{\\det A}$ by showing that the equivalent $AC^T = (\\det A)I$ is true:\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{lll}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{array}\\right]\\left[\\begin{array}{lll}\n",
    "C_{11} & C_{21} & C_{31} \\\\\n",
    "C_{12} & C_{22} & C_{32} \\\\\n",
    "C_{13} & C_{23} & C_{33}\n",
    "\\end{array}\\right]=\\left[\\begin{array}{ccc}\n",
    "\\operatorname{det} A & 0 & 0 \\\\\n",
    "0 & \\operatorname{det} A & 0 \\\\\n",
    "0 & 0 & \\operatorname{det} A\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Row 1 of $A$ times column 1 of $C^T$ yields the first $\\det A$ on the right: $a_{11}C_{11} + a_{12}C_{12} + a_{13}C_{13} = \\det A$, which is exactly the cofactor rule.\n",
    "\n",
    "How about the 0's off the main diagonal on the right?  There, the rows of $A$ are multiplying cofactors from different rows.  Why is the answer 0? \n",
    "\n",
    "$$a_{21}C_{11} + a_{22}C_{12} + A_{23}C_{13} = 0$$\n",
    "\n",
    "The answer is that this is the cofactor rule for a new matrix, when the second row of $A$ is copied into its first row.  The new matrix $A*$ has two equal rows, so $\\det A* = 0$ as we see in the equation above. Notice thatr $A*$ has the same cofactors $C_{11},C_{12},C_{13}$ as $A$, because all rows agree after the first row.  Thus our equation works.\n",
    "\n",
    "A couple final comments on Cramer's Rule:\n",
    "- The inverse of a triangular matrix is triangular, and cofactors help explain why\n",
    "- If all cofactors are nonzero, A could still be non-invertible.\n",
    "\n",
    "### Area of a Triangle\n",
    "\n",
    "Everybody knows a triangle's area is half the base times the height.  But what if we knew the corners?  Using the corners to find the base and height first isn't a good way to compute the area then.\n",
    "\n",
    "Determinants are the best way to find area.  The area of a triangle is half of a 3 by 3 determinant.  The square roots in the base and height cancel out in the good formula.  If one corner is at the origin, the determinant is only 2 by 2.\n",
    "\n",
    "The triangle with corners $(x_1, y_1)$, $(x_2, y_2)$, and $(x_3,y_3)$ has $\\text{area} = \\frac{\\text{determinant}}{2}$:\n",
    "\n",
    "$$\n",
    "\\text { Area of triangle } \\frac{1}{2}\\left|\\begin{array}{lll}\n",
    "x_1 & y_1 & 1 \\\\\n",
    "x_2 & y_2 & 1 \\\\\n",
    "x_3 & y_3 & 1\n",
    "\\end{array}\\right| \\quad \\text { Area }=\\frac{1}{2}\\left|\\begin{array}{ll}\n",
    "x_1 & y_1 \\\\\n",
    "x_2 & y_2\n",
    "\\end{array}\\right| \\quad \\text { when }\\left(x_3, y_3\\right)=(0,0)\n",
    "$$\n",
    "\n",
    "When you set $x_3 = y_3 = 0$ in the 3 by 3 determinant, you get the 2 by 2 determinant.  The area of this triangle with the (0,0) coordinate is $\\frac{1}{2}|x_1y_2 - x_2y_1|$.  We can see why this is by looking at a parallelogram, which is twice as big being the combination of two equal triangles.\n",
    "\n",
    "![image.png](images/parallelogram-area.png)\n",
    "\n",
    "There are many possible proofs for this, but a fitting one here is to show that the area has the same 1-2-3 properties as the determinant.  Then area will = determinant:\n",
    "\n",
    "1. When $A=I$, the parallelogram becomes the unit square.  It's area is $\\det I = 1$.\n",
    "2. When rows are exchanged, the determinant reverses sign.  The absolute value (positive area) stays the same-- it is the same parallelogram.\n",
    "3. If row 1 is multiplied by $t$, the area is also multiplied by $t$.  Suppose a new row $(x^{prime}_1, y^{prime}_1)$ is added to $(x_1, y_1)$ (keeping row 2 fixed).  Then the total area will equal A + A'.\n",
    "\n",
    "For a 3 dimensional box, the volume equals the absolute value of $\\det A$.  Our proof for this similarly can be based on properties 1-3 of determinants.  When an edge is stretched by a factor $t$, the volume is multiplied by $t$.  When edge 1 is added to edge 1', the volume is the sum of the original two volumes.\n",
    "\n",
    "![image.png](images/volume-of-box.png)\n",
    "\n",
    "The unit cube has volume = 1, which is $\\det I$.  Row exchanges or edge exchanges leave the same box and the same absolute volume.  The determinant changes sign, to indicate whether the the edges are a right-handed triple ($\\det A > 0$) or a left-handed triple ($\\det A < 0$).  The box volume follows the rules of determinants, so volume of $\\det A = absolute value$.\n",
    "\n",
    "### The Cross Product\n",
    "\n",
    "Unlike the determinant, the cross product is a vector rather than a number.  Let's first go over the formula for cross product.  The cross product of $u = (u_1, u_2, u_3)$ and $v = (v_1, v_2, v_3)$ is the vector:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{u} \\times \\boldsymbol{v}=\\left|\\begin{array}{ccc}\n",
    "\\boldsymbol{i} & \\boldsymbol{j} & \\boldsymbol{k} \\\\\n",
    "u_1 & u_2 & u_3 \\\\\n",
    "v_1 & v_2 & v_3\n",
    "\\end{array}\\right|=\\left(u_2 v_3-u_3 v_2\\right) \\boldsymbol{i}+\\left(u_3 v_1-u_1 v_3\\right) \\boldsymbol{j}+\\left(u_1 v_2-u_2 v_1\\right) \\boldsymbol{k}\n",
    "$$\n",
    "\n",
    "This vector $u \\times v$ is perpindicular to $u$ and $v$.  The cross product $v \\times u$ is $-(u \\times v)$\n",
    "\n",
    "This 3x3 determinant is the easiest way to remember how to calculate a cross product. In the determinant, the vector $i = (1,0,0)$ multiplies $u_2v_3$ and $-u_3v_2$.  The result is $(u_2v_3, u_3v_2, 0, 0)$, which is the first component of the cross product.\n",
    "\n",
    "Notice the cyclic pattern of the subscripts. 2 and 3 give component 1 of $u \\times v$, then 3 and 1 give component 2, then 1 and 2 give component 3.  This completes our definition of $u \\times v$.  Now let's list the properties of the cross-product:\n",
    "\n",
    "1. $v \\times u$ reverses rows 2 and 3 in the determinant so it equals $-(u \\times v)$.\n",
    "2. The cross product $u \\times v$ is perpindicular to $u$ (and also to $v$).  The direct proof is to watch terms cancel, producing a zero dot product: $\\boldsymbol{\\iota} \\cdot(\\boldsymbol{u} \\times \\boldsymbol{v})=u_1\\left(u_2 v_3-u_3 v_2\\right)+u_2\\left(u_3 v_1-u_1 v_3\\right)+u_3\\left(u_1 v_2-u_2 v_1\\right)=0$.  The determinant for $u \\cdot (u \\times v)$ has rows $u, u$, and $v$ (2 equal rows) so it is zero.\n",
    "3. The cross product of any vector with itself (two equal rows) is $u \\times u = 0$.  When $u$ and $v$ are parallel, the cross product is zero.  When $u$ and $v$ are perpindicular, the dot product is zero.\n",
    "\n",
    "The length of $u \\times v$ equals the area of the parallelogram with sides $u$ and $v$.\n",
    "\n",
    "### Triple product = Determinant = Volume\n",
    "\n",
    "Since $u \\times v$ is a vectort, we can take its dot product with a third vector $w$.  That produces the **triple product** $(u \\times v) \\cdot w$.  It is called a \"scalar\" triple product, because it is a number.  In fact it is a determinant-- it gives the volume of the $u, v, w$ box:\n",
    "\n",
    "$$\n",
    "(\\boldsymbol{u} \\times \\boldsymbol{v}) \\cdot \\boldsymbol{w}=\\left|\\begin{array}{lll}\n",
    "w_1 & w_2 & w_3 \\\\\n",
    "u_1 & u_2 & u_3 \\\\\n",
    "v_1 & v_2 & v_3\n",
    "\\end{array}\\right|=\\left|\\begin{array}{ccc}\n",
    "u_1 & u_2 & u_3 \\\\\n",
    "v_1 & v_2 & v_3 \\\\\n",
    "w_1 & w_2 & w_3\n",
    "\\end{array}\\right|\n",
    "$$\n",
    "\n",
    "We can put $w$ in the top row or the bottom row.  The determinants are the same because two row exchanges go from one to the other. $(u \\times v) \\cdot w = 0$ exactly when the vectors lie in the same plane.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2feccbb8",
   "metadata": {},
   "source": [
    "# Eigenvalues and Eigenvectors (6)\n",
    "## Introduction to Eigenvalues (6.1)\n",
    "\n",
    "[Lecture](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/lecture-21-eigenvalues-and-eigenvectors-1/)\n",
    "\n",
    "This chapter enters a new part of linear algebra.  The first part was about $Ax=b$: balance and equilibrium and steady state.  Now the second part is about change.  Time enters the picture-- continuous time in a differential equation $\\frac{du}{dt} = Au$, or time steps in a difference equation $u_{k+1} = Au_k$. Those equations are Not solved by elimination.\n",
    "\n",
    "The key idea is to avoid all the complications presented by the matrix $A$.  Suppose the solution vector $u(t)$ stays in the direction of a fixed vector $x$.  Then we only need to find the number (changing with time) that multiplies $x$.  A number is easier than a vector.  We want eigenvectors $x$ that don't change direction when you multiply by $A$.\n",
    "\n",
    "A good model comes from the powers $A, A^2, A^3, \\dots$ of a matrix.  Suppose you need the hundredth power $A^{100}$.  Its columns are very close to the eigenvector $(.6, .4)$:\n",
    "\n",
    "$$\n",
    "A, A^2, A^3=\\left[\\begin{array}{ll}\n",
    ".8 & .3 \\\\\n",
    ".2 & .7\n",
    "\\end{array}\\right] \\quad\\left[\\begin{array}{ll}\n",
    ".70 & .45 \\\\\n",
    ".30 & .55\n",
    "\\end{array}\\right]\\left[\\begin{array}{ll}\n",
    ".650 & .525 \\\\\n",
    ".350 & .475\n",
    "\\end{array}\\right] \\quad \\boldsymbol{A}^{\\mathbf{1 0 0}} \\approx\\left[\\begin{array}{ll}\n",
    "\\mathbf{6 0 0 0} & \\mathbf{. 6 0 0 0} \\\\\n",
    "\\mathbf{4 0 0 0} & \\mathbf{. 4 0 0 0}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$A^{100}$ was found by using the eigenvalues of $A$, not by multiplying 100 matrices.  Those eigenvalues (here they are $\\lambda = 1$ and $\\frac{1}{2}$) are a new way to see into the heart of a matrix.\n",
    "\n",
    "To explain eigenvalues, we first explain **eigenvectors**.  Almost all vectors change direction when they are multiplied by $A$.  Certain exceptional vectors $x$ are in the same direction as $Ax$.  Those are the eigenvectors.  Multiply an eigenvector by $A$, and the vector $Ax$ is a number $\\lambda$ times the original $x$.\n",
    "\n",
    "The basic equation is $Ax = \\lambda x$.  The number $\\lambda$ is an eigenvalue of $A$.\n",
    "\n",
    "The eigenvalue $\\lambda$ tells whether the special vector $x$ is stretched or shrunk or reversed or left unchanged--when it is multiplied by $A$.  We may find $\\lambda = 2$ or $\\frac{1}{2}$ or $-1$ or $1$.  The eigenvalue $\\lambda$ could be zero! Then $Ax = 0x$ means that this eigenvector $x$ is in the nullspace.\n",
    "\n",
    "If $A$ is the identity matrix, every vector has $Ax = x$.  All vectors are eigenvectors of $I$.  All eigenvalues are $\\lambda = 1$.  This is unusual to say the least. Most 2 by 2 matrices have two eigenvector directions and two eigenvalues.  We will show that $\\det(A - \\lambda I) = 0$.\n",
    "\n",
    "This section will explain how to compute the $x$'s and the $\\lambda$'s.  Before we go over/derive the official formulas, let's go ahead and use $\\det(A - \\lambda I) = 0$ to find the eigenvalues in this example.  This example has two eigenvalues, $\\lambda = 1$ and $\\lambda = \\frac{1}{2}$.  Look at $\\det(A - \\lambda I)$:\n",
    "\n",
    "$$\n",
    "A=\\left[\\begin{array}{ll}\n",
    ".8 & .3 \\\\\n",
    ".2 & .7\n",
    "\\end{array}\\right] \\quad \\operatorname{det}\\left[\\begin{array}{ll}\n",
    ".8-\\lambda & .3 \\\\\n",
    ".2 & .7-\\lambda\n",
    "\\end{array}\\right]=\\lambda^2-\\frac{3}{2} \\lambda+\\frac{1}{2}=(\\lambda-1)\\left(\\lambda-\\frac{1}{2}\\right)\n",
    "$$\n",
    "\n",
    "We factored the quadratic into $\\lambda - 1$ times $\\lambda - \\frac{1}{2}$, to see the two eigenvalues $\\lambda = 1$ and $\\lambda = \\frac{1}{2}$.  For those numbers, the matrix $A - \\lambda I$ becomes singular (zero determinant).  The eigenvectors $x_1$ and $x_2$ are in the nullspaces of $A - I$ and $A - \\frac{1}{2}I$.\n",
    "\n",
    "$(A-I)x_1 = 0$ is $Ax_1 = x_1$ and the first eigenvector is $(.6, .4)$.\n",
    "$(A- \\frac{1}{2}I)x_2 = 0$ is $Ax_2 = \\frac{1}{2}x_2$ and the second eigenvector is $(1,-1)$.\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\boldsymbol{x}_1=\\left[\\begin{array}{l}\n",
    ".6 \\\\\n",
    ".4\n",
    "\\end{array}\\right] \\quad \\text { and } & A \\boldsymbol{x}_1=\\left[\\begin{array}{ll}\n",
    ".8 & .3 \\\\\n",
    ".2 & .7\n",
    "\\end{array}\\right]\\left[\\begin{array}{l}\n",
    ".6 \\\\\n",
    ".4\n",
    "\\end{array}\\right]=\\boldsymbol{x}_1 \\quad\\left(A \\boldsymbol{x}=\\boldsymbol{x} \\text { means that } \\lambda_1=1\\right) \\\\\n",
    "\\boldsymbol{x}_2=\\left[\\begin{array}{r}\n",
    "1 \\\\\n",
    "-1\n",
    "\\end{array}\\right] \\quad \\text { and } & A \\boldsymbol{x}_2=\\left[\\begin{array}{ll}\n",
    ".8 & .3 \\\\\n",
    ".2 & .7\n",
    "\\end{array}\\right]\\left[\\begin{array}{r}\n",
    "1 \\\\\n",
    "-1\n",
    "\\end{array}\\right]=\\left[\\begin{array}{r}\n",
    ".5 \\\\\n",
    "-.5\n",
    "\\end{array}\\right] \\quad\\left(\\text { this is } \\frac{1}{2} \\boldsymbol{x}_2 \\text { so } \\lambda_2=\\frac{1}{2}\\right) .\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "If $x_1$ is multiplied again by $A$, we still get $x_1$.  Every power of $A$ will give $A^nx_1 = x_1$.  Multiplying $x_2$ by $A$ gave $\\frac{1}{2}x_2$, and if we multiply again we get $(\\frac{1}{2})^2$ times $x_2$.\n",
    "\n",
    "When $A$ is squared, the eigenvectors stay the same.  The eigenvalues are squared.\n",
    "\n",
    "This pattern keeps going, because the eigenvectors stay in their own directions (see pic below) and never get mixed.  The eigenvectors of $A^{100}$ are the same $x_1$ and $x_2$.  The eigenvalues of $A^{100}$ are $1^{100} = 1$ and $(\\frac{1}{2})^100$ = very small number.\n",
    "\n",
    "![image.png](images/eigenvectors-direction.png)\n",
    "\n",
    "Other vectors do change direction.  But all other vectors are combinations of the two eigenvectors.  The first column of $A$ is the combination $x_1 + (.2)x_2$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\text { Separate into eigenvectors } \\\\\n",
    "& \\text { Then multiply by } \\boldsymbol{A}\n",
    "\\end{aligned} \\quad\\left[\\begin{array}{l}\n",
    ".8 \\\\\n",
    ".2\n",
    "\\end{array}\\right]=x_1+(.2) x_2=\\left[\\begin{array}{l}\n",
    ".6 \\\\\n",
    ".4\n",
    "\\end{array}\\right]+\\left[\\begin{array}{r}\n",
    ".2 \\\\\n",
    "-.2\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "So each eigenvector is multiplied by its eigenvalue when we multiply by $A$.  At every step $x_1$ is unchanged, and $x_2$ is multiplied by $\\frac{1}{2}$, so 99 steps give the small number $(\\frac{1}{2})^99$:\n",
    "\n",
    "$$\n",
    "A^{99}\\left[\\begin{array}{l}\n",
    ".8 \\\\\n",
    ".2\n",
    "\\end{array}\\right] \\quad \\text { is really } \\quad \\boldsymbol{x}_1+(.2)\\left(\\frac{1}{2}\\right)^{99} \\boldsymbol{x}_2=\\left[\\begin{array}{l}\n",
    ".6 \\\\\n",
    ".4\n",
    "\\end{array}\\right]+\\left[\\begin{array}{c}\n",
    "\\text { very } \\\\\n",
    "\\text { small } \\\\\n",
    "\\text { vector }\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "This is the first column of $A^{100}$.  The number we originally wrote as $.6000$ was not exact.  We left out $(.2)(\\frac{1}{2})^99$ which wouldn't show up for 30 decimal places.\n",
    "\n",
    "The eigenvector $x_1$ is a \"steady state\" that doesn't change, because $\\lambda_1 = 1$.  The eigenvector $x_2$ is a \"decaying mode\" that virtually dissapears, because $\\lambda_2 = .5$.  The higher the power of $A$, the more closely its columns approach the steady state.\n",
    "\n",
    "This particular $A$ is a Markov matrix.  Its largest eigenvalue is $\\lambda = 1$.  Its eigenvector $x_1 = (.6, .4)$ is the **steady state**-- which all columns of $A^k$ will approach.\n",
    "\n",
    "For projection matrices $P$, we can see when $Px$ is parallel to $x$.  The eigenvectors for $\\lambda = 1$ and $\\lambda = 0$ fill the column space and nullspace.  The column space doesn't move ($Px =x$).  The nullspace goes to zero ($Px = 0x$).\n",
    "\n",
    "Let's look at this example: The projection matrix $P = \\begin{bmatrix}.5 & .5 \\\\ .5 & .5\\end{bmatrix}$ has eigenvalues $\\lambda = 1$ and $\\lambda = 0$.\n",
    "\n",
    "Its eigenvectors are $x_1 = (1,1)$ and $x_2 = (1, -1)$.  For those vectors, $Px_1 = x_1$ (steady state) and $Px_2 = 0$ (nullspace).  This example illustrates Markov matrices and singular matricxes and (most important) symmetric matrices.  All have special $\\lambda$'s and $x$'s:\n",
    "\n",
    "1. **Markov matrix**: Each column of $P$ adds to $1$, so $\\lambda = 1$ is an eigenvalue.\n",
    "2. $P$ is **singular**, so $\\lambda = 0$ is an eigenvalue.\n",
    "3. $P$ is symmetric, so its eigenvectors-- $(1,1)$ and $(1,-1)$ are perpindicular.\n",
    "\n",
    "The only eigenvalues of a projection matrix are 0 and 1.  The eigenvector for $\\lambda = 0$ (which means $Px = 0x$) fill up the nullspace.  The eigenvectors $\\lambda = 1$ (which means $Px = x$) fill up the column space.\n",
    "\n",
    "Projections have $\\lambda = 0$ and $1$.  Permutations all have $|\\lambda| = 1$.  The next matrix $R$ is a reflection and at the same time a permutation.  $R$ also has special eigenvalues.\n",
    "\n",
    "$$\n",
    "\\text { The reflection matrix } R=\\left[\\begin{array}{ll}\n",
    "0 & 1 \\\\\n",
    "1 & 0\n",
    "\\end{array}\\right] \\text { has eigenvalues } 1 \\text { and }-1 \\text {. }\n",
    "$$\n",
    "\n",
    "The eigenvector $(1,1)$ is unchanged by $R$.  The second eigenvector is $(1, -1)$-- its signs are reversed by $R$.\n",
    "\n",
    "### The Equation for the Eigenvalues\n",
    "\n",
    "The key calculation for this chapter is $Ax = \\lambda x$.  Let's go over the general steps for solving this.\n",
    "\n",
    "First move $\\lambda x$ to the left side.  Write the equation $Ax = \\lambda x$ as $(A - \\lambda I)x = 0$.  The matrix $A - \\lambda I$ times the eigenvector $x$ is the zero vector.  The eigenvectors make up the nullspace of $A - \\lambda I$.  When we know an eigenvalue $\\lambda$, we find an eigenvector by solving $(A - \\lambda I)x = 0$.\n",
    "\n",
    "Eigenvalues first. If $(A - \\lambda I)x = 0$ has a nonzero solution, $A - \\lambda I$ is not invertible.  The determinant of $A - \\lambda I$ must be zero.  This is how to recognize an eigenvalue $\\lambda$: The number $\\lambda$ is an eigenvalue of $A$ if and only if $A - \\lambda I$ is singular.\n",
    "\n",
    "This \"characteristic polynomial\" $\\det(A - \\lambda I)$ involves only $\\lambda$, not $x$. When $A$ is $n$ by $n$, the equation has degree $n$.  Then $A$ has $n$ eigenvalues (repeats are possible!).  Each $\\lambda$ leads to an eigenvector $x$ by solving $(A - \\lambda I)x = 0$ or $Ax = \\lambda x$.\n",
    "\n",
    "A note on the eigenvectors of 2 by 2 matrices. When $A - \\lambda I$ is singular, both rows are multiples of a vector $(a, b)$.  The eigenvector is any multiple of $(b, -a)$.\n",
    "\n",
    "Some 2 by 2 matrices ahve only one line of eigenvectors.  This can only happen when two eigenvalues are equal.\n",
    "\n",
    "### Determinant and Trace\n",
    "\n",
    "Bad news first: If you add a row of $A$ to another row, or exchange rows, the eigenvalues usually change.  Elimination does not preserve the $\\lambda$'s.  The triangular $U$ has its eigenvalues along the diagonal-- they are the pivots.  But they are not the eigenvalues of $A$!\n",
    "\n",
    "Good news second: The product $\\lambda_1$ times $\\lambda_2$ and the sum $\\lambda_1 + \\lambda_2$ can be found quickly from the matrix. These quick checks always work:\n",
    "- The product of the $n$ eigenvalues equals the determinant\n",
    "- The sum of the $n$ eigenvalues equals the sum of the $n$ diagonal entries.\n",
    "\n",
    "The sum of the entries along the main diagonal called the **trace** of $A$.\n",
    "\n",
    "$$\n",
    "\\lambda_1+\\lambda_2+\\cdots+\\lambda_n=\\text { trace }=a_{11}+a_{22}+\\cdots+a_{n n}\n",
    "$$\n",
    "\n",
    "These checks are very useful.  They don't remove the pain of computing $\\lambda$'s.  But when the computation is wrong, they generally tell us so. The trace and determinant _do_ tell everything when the matrix is 2 by 2, though.  And for triangular matrices, the the eigenvalues simply lie across the diagonal.\n",
    "\n",
    "### Imaginary Eigenvalues\n",
    "\n",
    "Eigenvalues can sometimes not be real numbers.\n",
    "\n",
    "$$\n",
    "Q=\\left[\\begin{array}{rr}\n",
    "0 & -1 \\\\\n",
    "1 & 0\n",
    "\\end{array}\\right] \\text { has no real eigenvectors. Its eigenvalues} \\\\\n",
    "\\text {are } \\lambda_1=i \\text { and } \\lambda_2=-i \\text {. Then } \\lambda_1+\\lambda_2=\\text { trace }=0 \\text { and } \\lambda_1 \\lambda_2=\\text { determinant }=1 \\text {. }\n",
    "$$\n",
    "\n",
    "Those $\\lambda$'s come as usual from $\\det(Q - \\lambda I) = 0$.  This equation gives $\\lambda^2 + 1 = 0$.  Its roots are $i$ and $-i$.  We see the imaginary number $i$ also in the eigenvectors:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\text { Complex } \\\\\n",
    "& \\text { eigenvectors }\n",
    "\\end{aligned}\\left[\\begin{array}{rr}\n",
    "0 & -1 \\\\\n",
    "1 & 0\n",
    "\\end{array}\\right]\\left[\\begin{array}{l}\n",
    "1 \\\\\n",
    "i\n",
    "\\end{array}\\right]=-i\\left[\\begin{array}{l}\n",
    "1 \\\\\n",
    "i\n",
    "\\end{array}\\right] \\quad \\text { and } \\quad\\left[\\begin{array}{rr}\n",
    "0 & -1 \\\\\n",
    "1 & 0\n",
    "\\end{array}\\right]\\left[\\begin{array}{l}\n",
    "i \\\\\n",
    "1\n",
    "\\end{array}\\right]=i\\left[\\begin{array}{l}\n",
    "i \\\\\n",
    "1\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Somehow those complex vectors $x_1 = (1,i)$ and $x_2 = (i, 1)$ keep their direction as they are rotated.  Don't ask how.  This example makes the all-important point that real matrices can easily have complex eigenvalues and eigenvectors.  The particular eigenvalues $i$ and $-i$ also illustrate two special properties of $Q$:\n",
    "\n",
    "1. $Q$ is an orthogonal matrix, so the absolute value of each $\\lambda$ is $|\\lambda| = 1$.  (Note this took me by surprise, but yes $|i| = 1$.\n",
    "2. $Q$ is a skew-symmetric matrix so each $\\lambda$ is pure imaginary.\n",
    "\n",
    "A symmetric matrix ($S^T = S$) can be compared to a real number.  A skew-symmetric matrix ($A^T = -A$) can be compared to an imaginary number. An orthogonal matrix ($Q^TQ = I$) corresponds to a complex number with $|\\lambda| = 1$. \n",
    "\n",
    "The eigenvectors for all these special matrices are perpindicular.  Somehow $(i, 1)$ and $(1, i)$ are perpindicular (chapter 9 of the book covers the dot product for complex vectors).\n",
    "\n",
    "### Eigenvalues of AB and A+B\n",
    "\n",
    "The first guess about the eigenvalues of $AB$ is not true.  An eigenvalue $\\lambda$ of $A$ times an eigenvalue $\\beta$ of $B$ usually does _not_ give an eigenvalue of $AB$.\n",
    "\n",
    "It seems that $\\beta$ times $\\lambda$ is an eigenvalue.  When $x$ is an eigenvector for $A$ and $B$, this is true.  The mistake is to expect that $A$ and $B$ automatically share the same eigenvector $x$.  Usually they don't.  Eigenvectors of $A$ are not generally eigenvectors of $B$.  $A$ and $B$ could have all zero eigenvalues while $1$ is an eigenvalue of $AB$:\n",
    "\n",
    "$$\n",
    "A=\\left[\\begin{array}{ll}\n",
    "0 & 1 \\\\\n",
    "0 & 0\n",
    "\\end{array}\\right\\rfloor \\quad \\text { and } \\quad B=\\left\\lfloor\\begin{array}{ll}\n",
    "0 & 0 \\\\\n",
    "1 & 0\n",
    "\\end{array}\\right\\rfloor ; \\quad \\text { then } \\quad A B=\\left\\lfloor\\begin{array}{ll}\n",
    "1 & 0 \\\\\n",
    "0 & 0\n",
    "\\end{array}\\right\\rfloor \\quad \\text { and } \\quad A+B=\\left\\lfloor\\begin{array}{ll}\n",
    "0 & 1 \\\\\n",
    "1 & 0\n",
    "\\end{array}\\right\\rfloor\n",
    "$$\n",
    "\n",
    "For the same reason, the eigenvalues of $A + B$ are generally not $\\lambda + \\beta$.  Here $\\lambda + \\beta = 0$ while $A + B$ has eigenvalues $1$ and $-1$.\n",
    "\n",
    "Suppose $x$ really was an eigenvector for both $A$ and $B$.  Then we do have $ABx = \\lambda \\beta x$ and $BAx= \\lambda \\beta x$.  When all $n$ eigenvectors are shared, we _can_ multiply eigenvalues.  $A$ and $B$ share the same $n$ independent eigenvectors if and only if $AB = BA$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cd8b33a",
   "metadata": {},
   "source": [
    "## Diagonalizing a Matrix (6.2)\n",
    "\n",
    "[Lecture](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/lecture-22-diagonalization-and-powers-of-a-1/)\n",
    "\n",
    "When $x$ is an eigenvector, multiplication by $A$ is just multiplication by a number $\\lambda$: $Ax = \\lambda x$.  All the difficulties of matrices are swept away.\n",
    "\n",
    "The point of this section is very direct.  The matrix $A$ turns into a diagonal matrix $\\Lambda$ when we use the eigenvectors properly.  This is the matrix form of our key idea.  We start right off with that one essential computation.  We will explain soon why $AX = X\\Lambda$.\n",
    "\n",
    "**Diagonalization**: Suppose the $n$ by $n$ matrix $A$ has $n$ linearly independent eigenvectors $x_1,\\dots,x_n$.  Put them into the columns of an eigenvector matrix $X$.  Then $X^{-1}AX$ is the **eigenvalue matrix $\\Lambda$**:\n",
    "\n",
    "$$\n",
    "X^{-1} A X=\\Lambda=\\left[\\begin{array}{lll}\n",
    "\\lambda_1 & & \\\\\n",
    "& \\ddots & \\\\\n",
    "& & \\lambda_n\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "The matrix A is \"diagonalized.\" We use capital lambda for the eigenvalue matrix, because the small $\\lambda$'s (the eigenvalues) are on its diagonal.\n",
    "\n",
    "Example: This $A$ is triangular so its eigenvalues are on the diagonal: $\\lambda = (1,6)$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\text { Eigenvectors } \\\\\n",
    "& \\text { go into } X\n",
    "\\end{aligned}\\left[\\begin{array}{l}\n",
    "\\mathbf{1} \\\\\n",
    "\\mathbf{0}\n",
    "\\end{array}\\right]\\left[\\begin{array}{l}\n",
    "\\mathbf{1} \\\\\n",
    "\\mathbf{1}\n",
    "\\end{array}\\right] \\quad\\left[\\begin{array}{rr}\n",
    "1 & -1 \\\\\n",
    "0 & 1\n",
    "\\end{array}\\right] \\left[\\begin{array}{ll}\n",
    "\\mathbf{1} & \\mathbf{5} \\\\\n",
    "0 & \\mathbf{6}\n",
    "\\end{array}\\right]\\left[\\begin{array}{ll}\n",
    "1 & 1 \\\\\n",
    "0 & 1\n",
    "\\end{array}\\right]=\\left[\\begin{array}{ll}\n",
    "\\mathbf{1} & 0 \\\\\n",
    "0 & \\mathbf{6}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "This is $X^{-1}AX = \\Lambda$.  In other words $A = X\\Lambda X^{-1}$.  Then $A^2 = X\\Lambda X^{-1}X\\Lambda X^{-1}$.  So $A^2 = X\\Lambda^2 X^{-1}$.  $A^2$ has the same eigenvectors in $X$ and squared eigenvalues in $\\Lambda^2$.\n",
    "\n",
    "Why is $AX = X\\Lambda$? $A$ multiplies its eigenvectors, which are the columns of $X$.  The first column of $AX$ is $Ax_1$.  That is $\\lambda_1x_1$. Each column of $X$ is multiplied by its eigenvalue:\n",
    "\n",
    "$$\n",
    "A X=A\\left[\\begin{array}{lll}\n",
    "& & \\\\\n",
    "x_1 & \\cdots & x_n \\\\\n",
    "& &\n",
    "\\end{array}\\right]=\\left[\\begin{array}{lll}\n",
    "& & \\\\\n",
    "\\lambda_1 x_1 & \\cdots & \\lambda_n x_n \\\\\n",
    "& &\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "The trick is to split this matrix $AX$ into $X$ times $\\Lambda$:\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{ccc}\n",
    "& & \\\\\n",
    "\\lambda_1 x_1 & \\cdots & \\lambda_n x_n \\\\\n",
    "& &\n",
    "\\end{array}\\right]=\\left[\\begin{array}{lll} \n",
    "& & \\\\\n",
    "x_1 & \\cdots & x_n \\\\\n",
    "& &\n",
    "\\end{array}\\right]\\left[\\begin{array}{lll}\n",
    "\\lambda_1 & & \\\\\n",
    "& \\ddots & \\\\\n",
    "& & \\lambda_n\n",
    "\\end{array}\\right]=X \\Lambda .\n",
    "$$\n",
    "\n",
    "Keep those matrices in the right order!  Then $\\lambda_1$ multiplies the first column $x_1$, as shown.  The diagonalization is complete, and we can write $AX = X\\Lambda$ in two good ways:\n",
    "\n",
    "$$\n",
    "A X=X \\Lambda \\quad \\text { is } \\quad \\boldsymbol{X}^{-1} \\boldsymbol{A X}=\\boldsymbol{\\Lambda} \\quad \\text { or } \\quad \\boldsymbol{A}=\\boldsymbol{X} \\boldsymbol{\\Lambda} \\boldsymbol{X}^{-1}\n",
    "$$\n",
    "\n",
    "The matrix $X$ has an inverse, because its columns (the eigenvectors of $A$) were assumed to be linearly independent.  Without $n$ independent eigenvectors, we can't diagonalize.\n",
    "\n",
    "$A$ and $\\Lambda$ have the same eigenvalues $\\lambda_1,\\dots,\\lambda_n$.  The eigenvectors are different.  The job of the original eigenvectors $x_1,\\dots,x_n$ was to diagonalize $A$.  Those eigenvectors in $X$ produce $A=X\\Lambda X^{-1}$.  We'll soon see their simplicity and importance and meaning.  The kth power will be $A^k = X\\Lambda^kX^{-1}$ which is easy to compute:\n",
    "\n",
    "$$\n",
    "A^k=\\left(X \\Lambda X^{-1}\\right)\\left(X \\Lambda X^{-1}\\right) \\ldots\\left(X \\Lambda X^{-1}\\right)=X \\Lambda^k X^{-1}\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{ll}\n",
    "1 & 5 \\\\\n",
    "0 & 6\n",
    "\\end{array}\\right]^k=\\left[\\begin{array}{ll}\n",
    "1 & 1 \\\\\n",
    "0 & 1\n",
    "\\end{array}\\right]\\left[\\begin{array}{ll}\n",
    "1 & \\\\\n",
    "& 6^k\n",
    "\\end{array}\\right]\\left[\\begin{array}{rr}\n",
    "1 & -1 \\\\\n",
    "0 & 1\n",
    "\\end{array}\\right]=\\left[\\begin{array}{cc}\n",
    "\\mathbf{1} & \\mathbf{6}^k-\\mathbf{1} \\\\\n",
    "0 & \\mathbf{6}^k\n",
    "\\end{array}\\right]=A^k\n",
    "$$\n",
    "\n",
    "With $k=1$ we get $A$.  With $k=0$ we get $A^0 = I$.  With $k=-1$ we get $A^{-1}$.  You can see how $A^2 = \\begin{bmatrix}1 & 35 \\\\ 0 & 36\\end{bmatrix} fits the formula when $k=2$.\n",
    "\n",
    "A couple of remarks before we look at the next example: \n",
    "1. Suppose that the eigenvalues $\\lambda_1, \\dots, \\lambda_n$ are all different.  Then it is automatic that the eigenvectors $x_1, \\dots, x_n$ are independent.  The eigenvector matrix $X$ will be invertible.  Any matrix that has no repeated eigenvalues can be diagonalized.  Second,\n",
    "2. We can multiply eigenvectors by any nonzero constants. $A(cx) = \\lambda(cx)$ is still true.\n",
    "3. The eigenvectors in $X$ come in the same order as the eigenvalues in $\\Lambda$.  To reverse the order in $\\Lambda$, reverse the order of the eigenvectors.  Compare to our previous example:\n",
    "\n",
    "$$\n",
    "\\text { New order 6, } 1 \\quad\\left[\\begin{array}{rr}\n",
    "0 & 1 \\\\\n",
    "1 & -1\n",
    "\\end{array}\\right]\\left[\\begin{array}{ll}\n",
    "1 & 5 \\\\\n",
    "0 & 6\n",
    "\\end{array}\\right]\\left[\\begin{array}{ll}\n",
    "1 & 1 \\\\\n",
    "1 & 0\n",
    "\\end{array}\\right]=\\left\\lfloor\\begin{array}{ll}\n",
    "6 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{array}\\right]=\\Lambda_{\\text {new }}\n",
    "$$\n",
    "\n",
    "4. Some matrices have too few eigenvectors.  Those matrices cannot be diagonalized.  Here are two examples:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text {Not diagonalizable}\\quad\n",
    "A=\\left[\\begin{array}{ll}\n",
    "1 & -1 \\\\\n",
    "1 & -1\n",
    "\\end{array}\\right] \\quad \\text { and } \\quad B=\\left[\\begin{array}{ll}\n",
    "0 & 1 \\\\\n",
    "0 & 0\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Their eigenvalues happen to be 0 and 0.  Nothing is special about $\\lambda = 0$, the problem is the repitition of $\\lambda$.  All eigenvectors of the first matrix are multiples of $(1,1)$.  There is no second eigenvector, so this unusual matrix $A$ cannot be diagonalized.\n",
    "\n",
    "Those matrices are the best examples to test any statement about eigenvectors.  In many true-false questions, non-diagonalizable matrices lead to false.\n",
    "\n",
    "- Invertibility is concerned with the eigenvalues ($\\lambda = 0$ or $\\lambda \\ne 0$).\n",
    "- Diagonalizability is concerned with the eigenvectors (too few or enough for $X$)\n",
    "\n",
    "Each eigenvalue has at least one eigenvector! $A - \\lambda I$ is singular.  If $(A - \\lambda I)x = 0$ leads you to $x = 0$, $\\lambda$ is _not_ an eigenvalue.  Look for a mistake in solving $\\det(A - \\lambda I) = 0$.\n",
    "\n",
    "Eigenvectors for $n$ different $\\lambda$'s are independent.  Then we can diagonalize $A$.\n",
    "\n",
    "Independent $x$ from different $\\lambda$'s: Eigenvectors $x_1, \\dots, x_j$ that correspond to distinct (all different) eigenvalues are linearly independent. An $n$ by $n$ matrix that has $n$ different eigenvalues (no repeated $\\lambda$'s) must be diagonalizable.\n",
    "\n",
    "### Similar Matrices: Same Eigenvalues\n",
    "\n",
    "Suppose the eigenvalue matrix $\\Lambda$ is fixed. As we change the eigenvector matrix $X$, we get a whole family of different matrices $A = X\\Lambda X^{-1}$-- all with the same eigenvalues in $\\Lambda$.  All those matrices $A$ (with the same $\\Lambda$) are called **similar**.\n",
    "\n",
    "This idea extends to matrices that can't be diagonalized.  Again we choose one constant matrix $C$ (not necessarily $\\Lambda$).  And we look again at the whole family of matrices $A = BCB^{-1}$, allowing all invertible matrices $B$.  Again those matrices $A$ and $C$ are called similar.\n",
    "\n",
    "We are using $C$ instead of $\\Lambda$ because $C$ might not be diagonal. We are using $B$ instead of $X$ because the columns of $B$ might not be eigenvectors.  We only require that $B$ is invertible-- its columns can contain any basis for $R^n$.  The key fact about similar matrices stays true.  Similar matrices $A$ and $C$ have the same eigenvalues.\n",
    "\n",
    "A fixed matrix $C$ produces a family of similar matrices $BCB^{-1}$, allowing all $B$.  When $C$ is the identity matrix, the \"family\" is very small.  The only member is $BIB^{-1} = I$.  The identity matrix is the only diagonalizable matrix with all eigenvalues $\\lambda = 1$.\n",
    "\n",
    "### Fibonacci Numbers\n",
    "\n",
    "Let's look at a famous example, where eigenvalues tell how fast the Fibonacci numbers grow.  Every Fibonacci number is the sum of the two previous $F$'s:\n",
    "\n",
    "$$\n",
    "\\text { The sequence } \\quad 0,1,1,2,3,5,8,13, \\ldots \\quad \\text { comes from } \\quad F_{k+2}=F_{k+1}+F_k \\text {. }\n",
    "$$\n",
    "\n",
    "Our problem is to find $F^{100}$.  The slow way is to apply the rule $F_{k+2} = F_{k+1} + F_k$ one step at a time.  Linear algebra gives a better way.\n",
    "\n",
    "The key is to begin with the matrix equation $u_{k+1} = Au_k$.  That is a one-step rule for vectors, while Fibonacci gave a two-step rule for scalars.  We match those rules by putting two Fibonacci numbers into a vector.  Then you will see the matrix $A$:\n",
    "\n",
    "$$\n",
    "\\text { Let } u_k=\\left[\\begin{array}{c}\n",
    "F_{k+1} \\\\\n",
    "F_k\n",
    "\\end{array}\\right] . \\text { The rule } \\begin{aligned}\n",
    "& F_{k+2}=F_{k+1}+F_k \\\\\n",
    "& F_{k+1}=F_{k+1}\n",
    "\\end{aligned} \\text { is } u_{k+1}=\\left[\\begin{array}{ll}\n",
    "\\mathbf{1} & \\mathbf{1} \\\\\n",
    "\\mathbf{1} & \\mathbf{0}\n",
    "\\end{array}\\right] \\boldsymbol{u}_k \\text {. }\n",
    "$$\n",
    "\n",
    "Every step multiplies by $A = \\begin{bmatrix}1 & 1 \\\\ 1 & 0\\end{bmatrix}$.  After 100 steps we reach $u_{100} = A^{100}u_0$:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{u}_0=\\left[\\begin{array}{l}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{array}\\right], \\quad \\boldsymbol{u}_1=\\left[\\begin{array}{l}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{array}\\right], \\quad \\boldsymbol{u}_2=\\left[\\begin{array}{l}\n",
    "2 \\\\\n",
    "1\n",
    "\\end{array}\\right], \\quad \\boldsymbol{u}_3=\\left[\\begin{array}{l}\n",
    "3 \\\\\n",
    "2\n",
    "\\end{array}\\right], \\quad \\ldots, \\quad \\boldsymbol{u}_{100}=\\left[\\begin{array}{l}\n",
    "F_{101} \\\\\n",
    "F_{100}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "This problem is just right for eigenvalues.  Subtract $\\lambda$ from the diagonal of $A$:\n",
    "\n",
    "$$\n",
    "A-\\lambda I=\\left[\\begin{array}{cc}\n",
    "1-\\lambda & 1 \\\\\n",
    "1 & -\\lambda\n",
    "\\end{array}\\right] \\quad \\text { leads to } \\quad \\operatorname{det}(A-\\lambda I)=\\lambda^2-\\lambda-1\n",
    "$$\n",
    "\n",
    "The equation $\\lambda^2 - \\lambda - 1 = 0$ is solved by the quadratic formula, arriving at eigenvalues:\n",
    "\n",
    "$$\n",
    "\\lambda_1=\\frac{1+\\sqrt{5}}{2} \\approx 1.618 \\quad \\text { and } \\quad \\lambda_2=\\frac{1-\\sqrt{5}}{2} \\approx-.618\n",
    "$$\n",
    "\n",
    "Those eigenvalues [lead to](https://medium.com/@andrew.chamberlain/the-linear-algebra-view-of-the-fibonacci-sequence-4e81f78935a3) eigenvectors $x_1 = (\\lambda_1, 1)$ and $x_2 = (\\lambda_2, 1)$.  Next we find the combination of those eigenvectors that gives $u_0 = (1,0)$:\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{l}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{array}\\right]=\\frac{1}{\\lambda_1-\\lambda_2}\\left(\\left[\\begin{array}{c}\n",
    "\\lambda_1 \\\\\n",
    "1\n",
    "\\end{array}\\right]-\\left[\\begin{array}{c}\n",
    "\\lambda_2 \\\\\n",
    "1\n",
    "\\end{array}\\right]\\right) \\quad \\text { or } \\quad u_0=\\frac{x_1-x_2}{\\lambda_1-\\lambda_2}\n",
    "$$\n",
    "\n",
    "Lastly we multiply $u_0$ by $A_{100}$ to find $u_{100}$.  The eigenvectors $x_1$ and $x_2$ are multiplied by $(\\lambda_1)^{100}$ and $(\\lambda_2)^{100}$ respectively:\n",
    "\n",
    "$$\n",
    "u_{100}=\\frac{\\left(\\lambda_1\\right)^{100} x_1-\\left(\\lambda_2\\right)^{100} x_2}{\\lambda_1-\\lambda_2}\n",
    "$$\n",
    "\n",
    "We want $F_{100} = $ second component of $u_{100}$.  The second components of $x_1$ and $x_2$ are 1.  The difference between $\\lambda_1$ and $\\lambda_2$ is $\\sqrt{5}$.  And $\\lambda_2^{100} \\approx 0$.\n",
    "\n",
    "$$\n",
    "\\text { 100th Fibonacci number }=\\frac{\\lambda_1^{100}-\\lambda_2^{100}}{\\lambda_1-\\lambda_2}=\\text { nearest integer to } \\frac{1}{\\sqrt{5}}\\left(\\frac{1+\\sqrt{5}}{2}\\right)^{100} \\text {. }\n",
    "$$\n",
    "\n",
    "### Matrix Powers $A^k$\n",
    "\n",
    "Fibonacci's example is a typical difference equation $u_{k+1} = Au_k$.  Each step multiplies by $A$.  The solution is $u_k = A^ku_0$.  We want to make clear how diagonalizing the matrix gives a quick way to compute $A^k$ and find $u_k$ in three steps.\n",
    "\n",
    "The eigenvector matrix $X$ produces $A = X\\Lambda X^{-1}$.  This is a factorization of the matrix, like $A = LU$ or $A = QR$.  The new factorization is perfectly suited to computing powers, because every time $X^{-1}$ multiplies $X$ we get $I$:\n",
    "\n",
    "$$\n",
    "\\text { Powers of } A \\quad A^k \\boldsymbol{u}_0=\\left(X \\Lambda X^{-1}\\right) \\cdots\\left(X \\Lambda X^{-1}\\right) \\boldsymbol{u}_0=X \\Lambda^k X^{-1} \\boldsymbol{u}_0\n",
    "$$\n",
    "\n",
    "Let's split $X\\Lambda^kX^{-1}u_0$ into three steps that show how eigenvalues work:\n",
    "\n",
    "1. Write $u_0$ as a combination $c_1x_1 + \\cdots + c_nx_n$ of the eigenvectors.  Then $c = X^{-1}u_0$.\n",
    "2. Multiply each eigenvector $x_i$ by $(\\lambda_i)^k$.  Now we have $\\Lambda^kX^{-1}u_0$.\n",
    "3. Add up the pieces $c_i(\\lambda_i)^kx_i$ to find the solution $u_k = A^ku_0$.  This is $X\\Lambda^kX^{-1}u_0$.\n",
    "\n",
    "$$\n",
    "\\text { Solution for } \\boldsymbol{u}_{k+1}=A \\boldsymbol{u}_k \\quad \\boldsymbol{u}_k=A^k \\boldsymbol{u}_0=c_1\\left(\\lambda_1\\right)^k \\boldsymbol{x}_1+\\cdots+c_n\\left(\\lambda_n\\right)^k \\boldsymbol{x}_n .\n",
    "$$\n",
    "\n",
    "In matrix language $A^k$ equals $(X\\Lambda X^{-1})^k$ which is $X$ times $\\Lambda^k$ times $X^{-1}$.  In Step 1, the eigenvectors in $X$ lead to the $c$'s in the combination $u_0 = c_1x_1 + \\cdots + c_nx_n$:\n",
    "\n",
    "$$\n",
    "\\text { Step } 1 \\quad u_0=\\left[\\begin{array}{lll}\n",
    "x_1 & \\cdots & x_n \\\\\n",
    "& &\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "c_1 \\\\\n",
    "\\vdots \\\\\n",
    "c_n\n",
    "\\end{array}\\right] \\text {. This says that } \\boldsymbol{u}_0=X \\mathbf{c} \\text {. }\n",
    "$$\n",
    "\n",
    "The coefficients in Step 1 are $c = X^{-1}u_0$.  Then Step 2 multiplies by $\\Lambda^k$.  The final result $u_k = \\Sigma c_i(\\lambda_i)^kx_i$ in Step 3 is the product of $X$ and $\\Lambda^k$ and $X^{-1}u_0$\n",
    "\n",
    "$$\n",
    "A^k u_0=X \\Lambda^k X^{-1} u_0=X \\Lambda^k \\boldsymbol{c}=\\left[\\begin{array}{lll} \n",
    "& & \\\\\n",
    "x_1 & \\ldots & x_n \\\\\n",
    "& &\n",
    "\\end{array}\\right]\\left[\\begin{array}{lll}\n",
    "\\left(\\lambda_1\\right)^k & & \\\\\n",
    "& \\ddots & \\\\\n",
    "& & \\left(\\lambda_n\\right)^k\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "c_1 \\\\\n",
    "\\vdots \\\\\n",
    "c_n\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "This result is exactly $u_k = c_1(\\lambda_1)^kx_1 + \\cdots + c_n(\\lambda_n)^kx_n$.  It solves $u_{k+1} = Au_k$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fcd9b5a4",
   "metadata": {},
   "source": [
    "## Systems of Differential Equations (6.3)\n",
    "[Lecture](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/lecture-23-differential-equations-and-exp-at-1/)\n",
    "\n",
    "Eigenvalues and eigenvectors and $A = X\\Lambda X^{-1}$ are perfect for matrix powers $A^k$.  They are also perfect for differential equations $du/dt = Au$.  This section is mostly linear algebra, but to read it you need one fact from calculus: The derivative of $e^{\\lambda t} = \\lambda e^{\\lambda t}$.  The whole point of this section is this: to convert constant-coefficient differential equations into linear algebra.\n",
    "\n",
    "The ordinary equations $\\frac{du}{dt} = u$ and $\\frac{du}{dt} = \\lambda u$ are solved by exponentials:\n",
    "\n",
    "$$\n",
    "\\frac{d u}{d t}=u \\text { produces } u(t)=\\boldsymbol{C} \\boldsymbol{e}^t \\quad \\frac{d u}{d t}=\\lambda u \\text { produces } u(t)=\\boldsymbol{C} \\boldsymbol{e}^{\\lambda t}\n",
    "$$\n",
    "\n",
    "At time $t = 0$ those solutions include $e^0 = 1$.  So they both reduce to $u(0) = C$.  This \"initial value\" tells us the right choice for $C$.  The solutions that start from the number $u(0)$ at time $t = 0$ are $u(t) = u(0)e^t$ and $u(t) = u(0)e^{\\lambda t}$.\n",
    "\n",
    "We just solved a 1 by 1 problem.  Linear algebra moves to $n$ by $n$.  The unknown is a vector $u$.  It starts from the initial vector $u(0)$, which is given.  The $n$ equations contain a square matrix $A$.  We expect $n$ exponents $e^{\\lambda t}$ in $u(t)$, from $n$ $\\lambda$'s:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\begin{array}{l}\n",
    "\\text { System of } \\\\\n",
    "\\boldsymbol{n} \\text { equations }\n",
    "\\end{array}\n",
    "\\end{aligned} \\quad \\frac{d \\boldsymbol{u}}{d t}=A \\boldsymbol{u} \\quad \\text { starting from the vector } \\boldsymbol{u}(0)=\\left[\\begin{array}{c}\n",
    "u_1(0) \\\\\n",
    "\\cdots \\\\\n",
    "u_n(0)\n",
    "\\end{array}\\right] \\text { at } t=0 \\text {. }\n",
    "$$\n",
    "\n",
    "These differential equations are _linear_.  If $u(t)$ and $v(t)$ are solutions, so is $Cu(t) + Dv(t)$.  We will need $n$ constants like $C$ and $D$ to match the $n$ components of $u(0)$.  Our first job is to find $n$ \"pure exponential solutions\" $u = e^{\\lambda t}x$ by using $Ax = \\lambda x$.\n",
    "\n",
    "Notices that $A$ is a _constant_ matrix.  In other linear equations, $A$ changes as $t$ changes.  In nonlinear equations, $A$ changes as $u$ changes.  We don't have those difficulties, $du/dt = A$ is \"linear with constant coefficients\".  Those and only those are the differential equations that we will convert directly to linear algebra.  Here is the key: Solve linear constant coefficient equations by exponentials $e^{\\lambda t}x$, when $Ax = \\lambda x$.\n",
    "\n",
    "### Solution of $du/dt = Au$\n",
    "\n",
    "Our pure exponential solution will be $e^{\\lambda t}$ times a fixed vector $x$.  You may guess that $\\lambda$ is an eigenvalue of $A$, and $x$ is the eigenvector.  Substitute $u(t) = e^{\\lambda t}x$ into the equation $du/dt = Au$ to prove it's a solution.  The factor $e^{\\lambda t}$ will cancel to leave $\\lambda x = Ax$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\text { Choose } \\boldsymbol{u}=e^{\\lambda t} \\boldsymbol{x} \\\\\n",
    "& \\text { when } \\boldsymbol{A} \\boldsymbol{x}=\\lambda \\boldsymbol{x}\n",
    "\\end{aligned} \\quad \\frac{d \\boldsymbol{u}}{d t}=\\lambda e^{\\lambda t} \\mathfrak{x} \\quad \\text { agrees with } \\quad A \\boldsymbol{u}=A e^{\\lambda t} \\mathfrak{x}\n",
    "$$\n",
    "\n",
    "All components of this special solution $u = e^{\\lambda t}x$ share the same $e^{\\lambda t}$.  The solution grows when $\\lambda > 0$.  It decays when $\\lambda < 0$.  If $\\lambda$ is a complex number, its real part decides growth or decay.  The imaginary part $\\omega$ gives oscillation $e^{i\\omega t}$ like a sine wave.\n",
    "\n",
    "Let's look at an example.  We will solve $\\frac{du}{dt} = Au = \\begin{bmatrix}0 & 1 \\\\ 1 & 0\\end{bmatrix}u$ starting from $u(0) = \\begin{bmatrix}4 \\\\ 2\\end{bmatrix}$.  This is a vector equation for $u$.  \n",
    "\n",
    "The matrix $A$ has eigenvalues $1$ and $-1$.  The eigenvectors $x$ are $(1,1)$ and $(1,-1)$.  the pure exponential solutions $u_1$ and $u_2$ take the form $e^{\\lambda t}x$ with $\\lambda_1 = 1$ and $\\lambda_2 = -1$:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{u}_1(t)=e^{\\lambda_1 t} \\boldsymbol{x}_1=e^t\\left[\\begin{array}{l}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{array}\\right] \\quad \\text { and } \\quad \\boldsymbol{u}_2(t)=e^{\\lambda_2 t} \\boldsymbol{x}_2=e^{-t}\\left[\\begin{array}{r}\n",
    "1 \\\\\n",
    "-1\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "We have two solutions to $du/dt = Au$.  To find al other solutions, multiply those special solutions by any numbers $C$ and $D$ and add:\n",
    "\n",
    "$$\n",
    "\\text { Complete solution } \\quad \\boldsymbol{u}(t)=\\boldsymbol{C} \\boldsymbol{e}^t\\left[\\begin{array}{l}\n",
    "\\mathbf{1} \\\\\n",
    "\\mathbf{1}\n",
    "\\end{array}\\right]+\\boldsymbol{D} \\boldsymbol{e}^{-t}\\left[\\begin{array}{r}\n",
    "\\mathbf{1} \\\\\n",
    "-\\mathbf{1}\n",
    "\\end{array}\\right]=\\left[\\begin{array}{l}\n",
    "C e^t+D e^{-t} \\\\\n",
    "C e^t-D e^{-t}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "With these two constants $C$ and $D$, we can match any starting vector $u(0) = (u_1(0), u_2(0))$.  Set $t=0$ and $e^0 = 1$.  We were told the intiial value $u(0) = (4,2)$:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{u}(0) \\text { decides } C, \\boldsymbol{D} \\quad C\\left[\\begin{array}{l}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{array}\\right]+D\\left[\\begin{array}{r}\n",
    "1 \\\\\n",
    "-1\n",
    "\\end{array}\\right]=\\left[\\begin{array}{l}\n",
    "4 \\\\\n",
    "2\n",
    "\\end{array}\\right] \\quad \\text { yields } \\quad \\boldsymbol{C}=\\mathbf{3} \\quad \\text { and } \\quad \\boldsymbol{D}=\\mathbf{1}\n",
    "$$\n",
    "\n",
    "With $C = 3$ and $D = 1$, the initial value problem is compeltely solved.\n",
    "\n",
    "The same three steps that solved $u_{k+1} = Au_k$ now solve $du/dt = Au$:\n",
    "\n",
    "1. Write $u(0)$ as a combination of the eigenvectors of $A$\n",
    "2. Multiply each eigenvector $x_i$ by its growth factor $e^{\\lambda_i t}$.\n",
    "3. The solution is the same combination of those pure solutions $e^{\\lambda t}x$:\n",
    "\n",
    "$$\n",
    "\\frac{d u}{d t}=A u \\quad u(t)=c_1 e^{\\lambda_1 t} x_1+\\cdots+c_n e^{\\lambda_n t} x_n\n",
    "$$\n",
    "\n",
    "Let's take another example.  Solve $du/dt = Au$ knowing eigenvalues $\\lambda = 1,2,3$:\n",
    "\n",
    "$$\n",
    "\\frac{d \\boldsymbol{u}}{d t}=\\left[\\begin{array}{lll}\n",
    "1 & 1 & 1 \\\\\n",
    "0 & 2 & 1 \\\\\n",
    "0 & 0 & 3\n",
    "\\end{array}\\right] \\boldsymbol{u} \\quad \\text { starting from } \\quad \\boldsymbol{u}(0)=\\left[\\begin{array}{l}\n",
    "9 \\\\\n",
    "7 \\\\\n",
    "4\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "The eigenvectors are $x_1 = (1,0,0)$ and $x_2 = (1,1,0)$ and $x_3 = (1,1,1)$.\n",
    "\n",
    "Step 1: The vector $u(0) = (9,7,4)$ is $2x_1 + 3x_2 + 4x_3$.  Thus $(c_1, c_2, c_3) = (2,3,4)$.\n",
    "Step 2: The factors $e^{\\lambda t}$ give exponential solutions $e^tx_1$ and $e^{2t}x_2$ and $e^{3t}x_3$.\n",
    "Step 3: The combination that starts from $u(0)$ is $u(t) = 2e^tx_1 + 3e^{2t}x_2 + 4e^{3t}x_3$.\n",
    "\n",
    "We now have the basic idea of how to solve $du/dt = Au$.  The rest of this section goes further.  We solve equations that contain _second_ derivatives, because they arise so often in applications.  We also decide whether $u(t)$ approaches zero or blows up or just oscillates.\n",
    "\n",
    "At the end comes the matrix exponential $e^{At}$.  The short formula $e^{At}u(0)$ solves the equation $du/dt = Au$ in the same way that $A^ku_0$ solves the equations $u_{k+1} = Au_k$.  We will see how \"difference equations\" help to solve differential equations.\n",
    "\n",
    "All these steps use the $\\lambda$'s and the $x$'s.  We're concerned with the constant coefficient problems that turn into linear algebra.  We'll clarify these simplest but most important differential equations-- whose solution is completely based on growth factors $e^{\\lambda t}$.\n",
    "\n",
    "### Second Order Equations\n",
    "\n",
    "The most important equation in mechanics is $my^{\\prime\\prime} + by^{\\prime} + ky = 0$.  The first term is the mass $m$ times the acceleration $a = y{\\prime\\prime}$.  This term $ma$ balances the force $F$ (that is Newton's Law).  The force includes the damping $-by^{\\prime}$ and the elastic force $-ky$, proportional to the distance moved.  This is a second-order equation because it contains the second derivative $y^{\\prime\\prime} = d^2y/dt^2$.  It is still linear with constant coefficients $m, b, k$.\n",
    "\n",
    "In a differential equations course, the method of solution is to substitute $y = e^{\\lambda t}$.  Each derivative of $y$ brings down a factor $\\lambda$.  We want $y = e^{\\lambda t}$ to solve the equation:\n",
    "\n",
    "$$\n",
    "m \\frac{d^2 y}{d t^2}+b \\frac{d y}{d t}+k y=0 \\quad \\text { becomes } \\quad\\left(m \\lambda^2+b \\lambda+k\\right) e^{\\lambda t}=0\n",
    "$$\n",
    "\n",
    "Everything depends on $m\\lambda^2 + b\\lambda + k = 0$.  This equation for $\\lambda$ has two roots $\\lambda_1$ and $\\lambda_2$.  Then the equation for $y$ has two pure solutions $y_1 = e^{\\lambda_1t}$ and $y_2 = e^{\\lambda_2t}$.  Their combinations $c_1y_1 + c_2y_2$ gives the complete solution unless $\\lambda_1 = \\lambda_2$.\n",
    "\n",
    "In a linear algebra course we expect matrices and eigenvalues.  Therefore we turn the scalar equation (with $y^{\\prime\\prime}$) into a vector equation for $y$ and $y^{\\prime}$: first derivative only.  Suppose the mass is $m = 1$.  Two equations for $(y, y^{\\prime})$ give $du/dt = Au$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& d y / d t=y^{\\prime} \\\\\n",
    "& d y^{\\prime} / d t=-k y-b y^{\\prime} \\quad \\text { converts to }\n",
    "\\end{aligned} \\quad \\frac{d}{d t}\\left[\\begin{array}{c}\n",
    "y \\\\\n",
    "y^{\\prime}\n",
    "\\end{array}\\right]=\\left[\\begin{array}{rr}\n",
    "0 & 1 \\\\\n",
    "-k & -\\boldsymbol{b}\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "y \\\\\n",
    "y^{\\prime}\n",
    "\\end{array}\\right]=A u\n",
    "$$\n",
    "\n",
    "The first equation $dy/dt = y'$ is trivial (but true).  The second is our previous equation, connecting $y^{\\prime\\prime}$ to $y^{prime}$ and $y$. Together they connect $u^{\\prime}$ to $u$.  So we solve $u^{\\prime} = Au$ by eigenvalues of $A$:\n",
    "\n",
    "$$\n",
    "A-\\lambda I=\\left[\\begin{array}{cc}\n",
    "-\\lambda & 1 \\\\\n",
    "-k & -b-\\lambda\n",
    "\\end{array}\\right] \\quad \\text { has determinant } \\quad \\lambda^2+b \\lambda+k=0\n",
    "$$\n",
    "\n",
    "The equation for the $\\lambda$'s is the same as we saw above.  It is still $\\lambda^2 + b\\lambda + k = 0$, since $m=1$.  The roots $\\lambda_1$ and $\\lambda_2$ are now eigenvalues of $A$.  The eigenvectors and the solution are:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}_1=\\left[\\begin{array}{c}\n",
    "1 \\\\\n",
    "\\lambda_1\n",
    "\\end{array}\\right] \\quad \\boldsymbol{x}_2=\\left[\\begin{array}{c}\n",
    "1 \\\\\n",
    "\\lambda_2\n",
    "\\end{array}\\right] \\quad \\boldsymbol{u}(t)=c_1 e^{\\lambda_1 t}\\left[\\begin{array}{c}\n",
    "1 \\\\\n",
    "\\lambda_1\n",
    "\\end{array}\\right]+c_2 e^{\\lambda_2 t}\\left[\\begin{array}{c}\n",
    "1 \\\\\n",
    "\\lambda_2\n",
    "\\end{array}\\right] .\n",
    "$$\n",
    "\n",
    "The first component of $u(t)$ has $y = c_1e^{\\lambda_1t} + c_2e^{\\lambda_2t}-- the same solution as before.  In the second component of $u(t)$ you see the velocity $dy/dt$.  The vector problem is completely consistent with the scalar problem.  The 2 by 2 matrix $A$ is called a _companion matrix_-- a companion to the second order equation with $y^{\\prime\\prime}$.\n",
    "\n",
    "### Stability of 2 by 2 Matrices\n",
    "\n",
    "For the solution of $du/dt = Au$, there is a fundamental question.  Does the solution approach $u = 0$ as $t \\rightarrow \\infty$?  Is the problem _stable_ by dissipating energy?  A solution that includes $e^t$ is unstable.  Stablity depends on the eigenvalues of $A$.\n",
    "\n",
    "The complete solution $u(t)$ is built from pure solutions $e^{\\lambda t}x$.  If the eigenvalue $\\lambda$ is real, we know exactly when $e^{\\lambda t}$ will approach zero: The number $\\lambda$ must be negative.  If the eigenvalue is a complex number $\\lambda = r + is$, the real part $r$ must be negative.  When $e^{\\lambda t}$ splits into $e^{rt}e^{ist}$, the factor $e^{ist}$ has absolute value fixed at 1:\n",
    "\n",
    "$$\n",
    "e^{i s t}=\\cos s t+i \\sin s t \\quad \\text { has } \\quad\\left|e^{i s t}\\right|^2=\\cos ^2 s t+\\sin ^2 s t=1 .\n",
    "$$\n",
    "\n",
    "The real part of $\\lambda$ controls the growth ($r > 0$) or the decay ($r < 0$).  \n",
    "\n",
    "The question is: Which matrices have negative eigenvalues.  More accurately, when are the real parts of the $\\lambda$'s all negative?  2 by matrices allow a clear answer.\n",
    "\n",
    "**Stability**: $A$ is **stable** and $u(t) \\rightarrow 0$ when all eigenvalues $\\lambda$ have negative real parts.  The 2 by 2 matrix $A = \\begin{bmatrix}a & b \\\\ c & d\\end{bmatrix}$ must pass two tests:\n",
    "\n",
    "- $\\lambda_1 + \\lambda_2 < 0$.  The trace $T = a + d$ must be negative.\n",
    "- $\\lambda_1\\lambda_2 > 0$. The determinant $D = ad - bc$ must be positive.\n",
    "\n",
    "Reason: If the $\\lambda$'s are real and negative, their sum is negative.  This is the trace $T$.  Their product is positive.  This is the determinant $D$.  The argument also goes in the reverse direction.  If $D = \\lambda_1\\lambda_2$ is positive then $\\lambda_1$ and $\\lambda_2$ have the same sign. If $T = \\lambda_1 + \\lambda_2$ is negative, that sign will be negative.  We can test $T$ and $D$.\n",
    "\n",
    "If the $\\lambda$'s are complex numbers, they must have the form $r + is$ and $r - is$.  Otherwise $T$ and $D$ will not be real.  The determinant $D$ is automatically positive, since $(r + is)(r - is) = r^2 + s^2$.  The trace $T$ is $r + is + r - is = 2r$.  So a negative trace $T$ means that the real part $r$ is negative and the matrix is stable. Q.E.D.\n",
    "\n",
    "### The Exponential of a Matrix\n",
    "\n",
    "We want to write the solution $u(t)$ in a new form $e^{At}u(0)$.  First we have to say what $e^{At}$ means, with a matrix in the exponent.  To define $e^{At}$ for matrices, we copy $e^x$ for numbers.\n",
    "\n",
    "The direct definition of $e^{x}$ is by the infinite series $1 + x + \\frac{1}{2}x^2 + \\frac{1}{6}x^3 + \\cdots$.  When you change $x$ to a square matrix $At$, this series defines the matrix exponential $e^{At}$:\n",
    "\n",
    "- Matrix exponential $e^{At}$: $e^{At} = I + At + \\frac{1}{2}(At)^2 + \\frac{1}{6}(At)^3 + \\cdots$\n",
    "- Its $t$ derivative is $Ae^{At}$: $A + A^2t + \\frac{1}{2}A^3t^2 + \\cdots = Ae^{At}$\n",
    "- Its eigenvalues are $e^{\\lambda t}$: $(I + At + \\frac{1}{2}(At)^2 + \\cdots)x = (1 + \\lambda t + \\frac{1}{2}(\\lambda t)^2 + \\cdots)x$\n",
    "\n",
    "The number that divides $(At)^n$ is n factorial.  The series $e^{At}$ always converges and its derivative is always $Ae^{At}$.  Therefore $e^{At}u(0)$ solves the differential equation with one quick formula -- even if there is a shortage of eigenvectors.\n",
    "\n",
    "We'll look at an example to see that it works with a missing eigenvector.  It will produce $te^{\\lambda t}$.  First let's reach $Xe^{\\Lambda t}X^{-1}$ in the good (diagonalizable) case.\n",
    "\n",
    "We're going to emphasize how to find $u(t) = e^{At}u(0)$ by diagonalization.  Assume $A$ does have $n$ independent eigenvectors, so it is diagonalizable.  Substitute $A = X\\Lambda X^{-1}$ into the series for $e^{At}$.  Whenever $X\\Lambda X^{-1}X\\Lambda X^{-1}$ appears, we cancel $X^{-1}X$ in the middle:\n",
    "\n",
    "- Use the series: $e^{A t}=I+X \\Lambda X^{-1} t+\\frac{1}{2}\\left(X \\Lambda X^{-1} t\\right)\\left(X \\Lambda X^{-1} t\\right)+\\cdots$\n",
    "- Factor out $X$ and $X^{-1}$: $=X\\left[I+\\Lambda t+\\frac{1}{2}(\\Lambda t)^2+\\cdots\\right] X^{-1}$\n",
    "- $e^{At}$ is diagonalized! $e^{At} = Xe^{\\Lambda t}X^{-1}$.\n",
    "\n",
    "$e^{At}$ has the same eigenvector matrix $X$ as $A$.  Then $\\Lambda$ is a diagonal matrix and so is $e^{\\Lambda t}$.  The numbers $e^{\\lambda_it}$ are on the diagonal.  Multiply $Xe^{\\Lambda t}X^{-1}u(0)$ to recognize $u(t)$:\n",
    "\n",
    "$$\n",
    "u(t) = e^{A t} \\boldsymbol{u}(0)=X e^{\\Lambda t} X^{-1} \\boldsymbol{u}(0)=\\left[\\begin{array}{lll} \n",
    "& & \\\\\n",
    "\\mathfrak{x}_1 & \\cdots & \\boldsymbol{x}_n \\\\\n",
    "& &\n",
    "\\end{array}\\right]\\left[\\begin{array}{lll}\n",
    "e^{\\lambda_1 t} & & \\\\\n",
    "& \\ddots & \\\\\n",
    "& & e^{\\lambda_n t}\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "c_1 \\\\\n",
    "\\vdots \\\\\n",
    "c_n\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "This solution $e^{At}u(0)$ is the same answer that we saw before from three steps:\n",
    "\n",
    "1. $u(0) = c_1x_1 + \\cdots + c_nx_n = Xc$.  Here we need $n$ independent eigenvectors.\n",
    "2. Multiply each $x_i$ by its growth factor $e^{\\lambda_it}$ to follow it forward in time.\n",
    "3. The best form of $e^{At}u(0)$ is $u(t) = c_1e^{\\lambda_1t}x_1 + \\cdots + c_ne^{\\lambda_nt}x_n$.\n",
    "\n",
    "A few rules worth noting:\n",
    "- $e^{At} always has the inverse $e^{-At}$.\n",
    "- The eigenvalues of $e^{At}$ are always $e^{\\lambda t}$.\n",
    "- When $A$ is antisymmetric, $e^{At}$ is orthogonal.  Inverse = transpose = $e^{-At}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01c15f3c",
   "metadata": {},
   "source": [
    "## Symmetric Matrices (6.4)\n",
    "\n",
    "Symmetric meaning $S = S^T$.\n",
    "\n",
    "It is no exaggeration to say that symmetric matrices $S$ are the most important matrices the world will ever see-- in the theory of linear algebra but also in the applications.\n",
    "\n",
    "What is special about $Sx = \\lambda x$ when $S$ is symmetric?  The diagonalization $S = X\\Lambda X^{-1}$ will reflect the symmetry of $S$.  We get some hint by transposing $S^T = (X^{-1})^T\\Lambda X^T$.  (Recall from our chapter on Transposes: The transpose of $AB$ is $(AB)^T = B^TA^T$). Those are the same since $S = S^T$. Possibly $X^{-1}$ in the first form equals $X^T$ in the second form? Then $X^TX = I$.  That makes each eigenvector in $X$ orthogonal to the other eigenvectors when $S=S^T$.  Here are the key facts:\n",
    "\n",
    "[grok what they said in terms of transpose/inverse rule]\n",
    "\n",
    "1. A symmetric matrix has only real eigenvalues.\n",
    "2. The eigenvectors can be chosen orthonormal.\n",
    "\n",
    "Those $n$ orthonormal eigenvectors go into the columns of $X$.  Every symmetric matrix can be diagonalized.  Its eigenvectors matrix $X$ becomes an orthogonal matrix $Q$.  Orthogonal matrices have $Q^{-1} = Q^T$-- what we suspected about the eigenvector matrix is true.  To remember it we write $Q$ instead of $X$, when we choose orthonormal eigenvectors.\n",
    "\n",
    "Why do we use the word \"choose\"?  Because the eigenvectors do not _have_ to be unit vectors.  Their lengths are at our disposal.  We will choose unit vectors-- eigenvectors of length one, which are orthonormal and not just orthogonal.  Then $A = X\\Lambda X^{-1}$ is in it special and paticular form $S = Q\\Lambda Q^T$ for symmetric matrices.\n",
    "\n",
    "**Spectral Theorem**: Every symmetric matrix has the factorization $S = Q\\Lambda Q^T$ with real eigenvalues in $\\Lambda$ and orthonormal eigenvectors in the columns of $Q$.  **Symmetric diagonalization**:\n",
    "\n",
    "$$S = Q\\Lambda Q^{-1} = Q\\Lambda Q^T \\text{ with } Q^{-1} = Q^T$$\n",
    "\n",
    "It is easy to see that $Q\\Lambda Q^T$ is symmetric.  Take its transpose.  You get $(Q^T)^T\\Lambda^TQ^T$, which is $Q\\Lambda Q^T$ again.  The harder part is to prove that every symmetric matrix has real $\\lambda$'s and orthonormal $x$'s.  This is the \"spectral theorem\" in geometry and physics.  We have to prove it!  No choice.  We'll approach the proof in three steps:\n",
    "\n",
    "1. By an example, showing real $\\lambda$'s in $\\Lambda$ and orthonormal $x$'s in $Q$.\n",
    "2. By a proof of those facts when no eigenvalues are repeated.\n",
    "3. By a proof that allows eigenvalues (at the end of this section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ecc056f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0 & 1\\\\1 & 2\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0, 1],\n",
       "[1, 2]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(1 - sqrt(2),\n",
       "  1,\n",
       "  [Matrix([\n",
       "   [-sqrt(2) - 1],\n",
       "   [           1]])]),\n",
       " (1 + sqrt(2),\n",
       "  1,\n",
       "  [Matrix([\n",
       "   [-1 + sqrt(2)],\n",
       "   [           1]])])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0 & 1\\\\-1 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[ 0, 1],\n",
       "[-1, 0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sympy import Matrix\n",
    "\n",
    "# Check if symmetric\n",
    "m = Matrix(2, 2, [0, 1, 1, 2])\n",
    "display(m)\n",
    "display(m.is_symmetric())\n",
    "\n",
    "# Check if anti-symmetric\n",
    "m = Matrix(2, 2, [0, 1, -1, 0])\n",
    "display(m)\n",
    "display(m.is_anti_symmetric())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09b44fb",
   "metadata": {},
   "source": [
    "\n",
    "## Positive Definite Matrices (6.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72acbd94-93c6-4dd3-8984-78ff1547c5ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Sources\n",
    "\n",
    "- [Gilbert Strangâ€™s Class - MIT Linear Algebra Fall 2011](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/resource-index/)\n",
    " - Uses Introduction to Linear Algebra, 5th Edition\n",
    "- [3 blue 1 brown vids - Essence of Linear Algebra](https://www.youtube.com/watch?v=LyGKycYT2v0&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=10)\n",
    "- May test myself on [Khan](https://www.khanacademy.org/math/linear-algebra) but not planning to use his videos between the textbook and the above vids\n",
    "- Going to distribute [Numpy](https://numpy.org/doc/stable/user/absolute_beginners.html) stuff as I go.  Taking notes separately on that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4c47c69",
   "metadata": {},
   "source": [
    "# Introduction to Vectors (1)\n",
    "Lectures:\n",
    "* [The Geometry of Linear Equations](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/the-geometry-of-linear-equations-1/) - Note, this also covers (2.1)\n",
    "* [An Overview of Linear Algebra](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/an-overview-of-linear-algebra-1/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "672b86a6",
   "metadata": {},
   "source": [
    "## Linear combinations (1.1)\n",
    "\n",
    "$cv + dw$ for linear combinations of vectors $v$ and $w$, where $c$ and $d$ are scalars."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d76a9a2a",
   "metadata": {},
   "source": [
    "\n",
    "## Lengths and Dot Products (1.2)\n",
    "The dot product of vectors $v = \\begin{bmatrix}1 \\\\ 2\\end{bmatrix}$ and $w = \\begin{bmatrix}4 \\\\ 5\\end{bmatrix}$ is $v \\cdot w = (1)(4) + (2)(5) = 4 + 10 = 14$.\n",
    "\n",
    "Some algebraic properties of the dot product:\n",
    "1. Commutative Property: For any two vectors $u$ and $v$, $u \\cdot $v = v \\cdot u$.\n",
    "2. Scalar Multiplication Property: For any two vectors $u$ and $v$ and any real number $c$, $(cu) \\cdot v = u \\cdot (cv) = c(u \\cdot v)$\n",
    "3. Distributive Property: For any 3 vectors $u$, $v$, and $w$, $u \\cdot (v+w) = u \\cdot v + u \\cdot w$.\n",
    "\n",
    "When you multiply two vectors and the dot product is zero, they are perpindicular.  More generally, the angle $\\theta$ between vectors $v$ and $w$ has:\n",
    "$$\n",
    "\\cos \\theta = \\frac{v \\cdot w}{||v||\\;||w||}\n",
    "$$\n",
    "\n",
    "The length $||v||$ of a vector is $\\sqrt{v \\cdot v}$. This follows from the pythagorean theorem.\n",
    "\n",
    "The **unit vector** is a vector with length 1. Divide any vector by its length to get a unit vector."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "132623cc",
   "metadata": {},
   "source": [
    "\n",
    "### Explanation of Angle Between Two Vectors\n",
    "\n",
    "The unit vector that makes an angle $\\theta$ with the x axis is $\\begin{bmatrix}\\cos \\theta \\\\ \\sin \\theta\\end{bmatrix}$, we can see this from the unit circle\n",
    "\n",
    "![image.png](images/unit-circle.png)\n",
    "\n",
    "Let's get a geometric understanding for the rule\n",
    "$$\n",
    "\\cos \\theta = \\frac{v \\cdot w}{||v||\\;||w||}\n",
    "$$\n",
    "\n",
    "Now suppose instead of forming $\\theta$ with the x axis, we have two unit vectors, $U$ and $u$, and they are both rotated from the x axis:\n",
    "\n",
    "![image.png](images/unit-vector-addition.png)\n",
    "\n",
    "$u \\cdot U$ would then be $\\cos{\\alpha}\\cos{\\beta} + \\sin{\\alpha}\\sin{\\beta}$. From the cosine angle addition rule in trignometry, this is equal to $\\cos(\\theta)$.\n",
    "\n",
    "So we have arrived at the preliminary rule that unit vectors $u$ and $U$ at angle $\\theta$ have:\n",
    "\n",
    "$$u \\cdot U = \\cos{\\theta}$$\n",
    "\n",
    "Combine this with our observation before that you can divide any vector by its length to get its unit vector, and we arrive at our **cosine formula** for any vectors $v$ and $w$ by just dividing their lengths:\n",
    "\n",
    "$$\n",
    "\\cos \\theta = \\frac{v \\cdot w}{||v||\\;||w||}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4867f5b",
   "metadata": {},
   "source": [
    "\n",
    "### Schwarz and Triangle Inequalities\n",
    "\n",
    "Because all cosines are between -1 and 1, it follows that the absolute value of the dot product, $|v \\cdot w|$, cannot exceed the product of the lengths, this is the **Schwarz Inequality**:\n",
    "\n",
    "$$|v \\cdot w| \\le ||v||\\: ||w||$$\n",
    "\n",
    "From the Schwarz Inequality [follows](https://math.stackexchange.com/a/91194) the **Triangle Inequality**:\n",
    "\n",
    "$$||u + v|| \\le ||u|| + ||v||$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a034db",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Independence and Dependence\n",
    "\n",
    "Vectors are **independent** if no combination other than 0 multiples gives $b=0$.  Vectors are **dependent** if multiple combinations give $b=0$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2be52981",
   "metadata": {},
   "source": [
    "## Matrices (1.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d2bb8b4",
   "metadata": {},
   "source": [
    "A matrix is **invertible** (aka **non-singular**) if it has independent (see definition above) column vectors, meaning $Ax = 0$ has only one solution between them.\n",
    "\n",
    "A matrix is **singular** if $Ax=0$ has many solutions, or none at all."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56e07f00",
   "metadata": {},
   "source": [
    "\n",
    "# Solving Linear Equations (2)\n",
    "\n",
    "* [Elemination with Matrices](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/elimination-with-matrices-1/) - Lecture covering 2.2 and 2.3\n",
    "## Vectors and Linear Equations (2.1)\n",
    "\n",
    "Geometrically, it's worth noting that the dot product of each row with $x$ gives the equation of a plane.\n",
    "When the number of unknowns matches the number of equations, there is _usually_ one solution.\n",
    "\n",
    "### Matrix, Row, and Column Pictures\n",
    "\n",
    "Lets say we have $n$ equations and $n$ unknowns, and go over:\n",
    "* Matrix Form\n",
    "* Row Picture\n",
    "* Column Picture\n",
    "\n",
    "Let's look specifically at these two equations with two unknowns:\n",
    "$$\n",
    "2x - y = 0 \\\\\n",
    "-x + 2y = 3\n",
    "$$\n",
    "\n",
    "In **matrix form**, with the **coefficient matrix**, followed by the unknowns matrix, equal to solutions/right hand side would be:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2 & -1 \\\\\n",
    "-1 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These three matrices are abstractly referred to as $Ax=b$. When we are solving for $x$ (the inverse), we are abstractly solving $x = A^{-1}b$. And note that only with an invertible matrix (see below) can we solve this.\n",
    "\n",
    "The **row picture** is looking at one equation at a time, it's what we've seen before with systems of equations, or looking for where lines meet when we graph them geometrically.\n",
    "\n",
    "The **column picture** would have us formulate the equations as combinations of the columns, so:\n",
    "\n",
    "$$\n",
    "x \n",
    "\\begin{bmatrix}\n",
    "2 \\\\\n",
    "-1\n",
    "\\end{bmatrix}\n",
    "+ \n",
    "y\n",
    "\\begin{bmatrix}\n",
    "-1 \\\\\n",
    "2\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Geometrically, the column picture can solve these linear equations through vector addition, which we know geometrically means combining the column vectors each a certain number of times to produce the right hand side.\n",
    "\n",
    "### The Identity Matrix\n",
    "\n",
    "Multiplying $Ix$ where $I$ is the identity matrix, you get back the x you started with, $Ix=x$.  An example 3x3 identity matrix:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63361923",
   "metadata": {},
   "source": [
    "## The Idea of Elimination (2.2)\n",
    "\n",
    "**Elimination** is the systematic way of solving linear equations. Elimination proceeds by producing an **upper triangular system** from top to bottom, then solving with **back substitution** from the bottom up.\n",
    "\n",
    "In the first part, where you're producing the upper triangular system, you subtract a multiple of the above equation from the equation below.  This **multiplier** ($l$) is determined from the **pivot** above.  For example, if we have\n",
    "\n",
    "$$\n",
    "4x - 8y = 4 \\\\\n",
    "3x + 2y = 11\n",
    "$$\n",
    "\n",
    "Our multiplier of the first equation would be $l=\\frac{3}{4}$, and we'd then subtract that multiplied equation from the 2nd.  We'd then be left with the pivot of $8$ at the bottom right.  To solve $n$ equations we want $n$ pivots.  If there were a 3rd equation we'd use the $8$ pivot to determine our next multiplier and subtract, and so on.\n",
    "\n",
    "### The breakdown of elimination\n",
    "\n",
    "It's possible for the process of elimination to fail along the way.  Specifically, we might reach a 0 pivot.  In this case, we may be able to rescue this with row exchange, or may not be able to.  It may be that the 0 pivot:\n",
    "\n",
    "* Implies no solutions (e.g. $0y=8$).  Geometrically this would be non-intersecting lines. OR \n",
    "* It may be that it arrives at infinite solutions (e.g. $0y=0$). Geometrically this would be represented by more than one intersection, e.g. two identical lines.\n",
    "* It may be that a row exchange can rescue things, for example:\n",
    "\n",
    "$$\n",
    "0x + 2y = 4 \\\\\n",
    "3x - 2y = 5\n",
    "$$\n",
    "\n",
    "Here would just want to perform **row substitution** to get a triangular system we could then back-substitute on.\n",
    "\n",
    "Recall our terminology from earlier on, when we can complete elimination, we are dealing with a non-singular matrix, whereas the no solutions or infinite solutions cases are singular.\n",
    "\n",
    "### Extending into 3+ equations\n",
    "\n",
    "The process involves clear out columns below the pivots, using multipliers of that pivot, before moving onto the next pivot.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e25d296b",
   "metadata": {},
   "source": [
    "## Elimination Using Matrices (2.3)\n",
    "\n",
    "**Elimination matrices** execute our elimination steps.  An elimination matrix $E_{ij}$ eliminates row $i$, column $j$ by multiplying the $j$th equation by $l_{ij}$ and subtracting it from the $i$th equation.  So for example $E_{21}$ would would be the first elimination step, clearing out row 2, column 1.\n",
    "\n",
    "We need a lot of these $E_{ij}$ matrices to complete elimination, which is why we'll later see they can be combined into one big matrix $E$.  The neatest way to do that is by combining all their inverses $(E_{ij})^{-1}$ into one overall matrix $L = E^{-1}$.  \n",
    "\n",
    "The special property of $L$ is that all the multipliers $l_{ij}$ fall into place.  Those numbers are mixed up in $E$ (forward elimination from A to U).  Inverting puts the steps and their elimination matrices in the opposite order and prevents the mixup.\n",
    "\n",
    "### The Matrix Form of One Elimination Step\n",
    "\n",
    "Suppose we want to subtract two times row 1 from row 2.  The elimination matrix for this step would be:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "-2 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The first and third rows come from the identity matrix $I$. The $-2$ comes from the negative of the multiplier $l$ (2).\n",
    "\n",
    "### Matrix Multiplication\n",
    "\n",
    "Via [MathIsFun](https://www.mathsisfun.com/algebra/matrix-multiplying.html):\n",
    "\n",
    "![image.svg](images/matrix-multiply.svg)\n",
    "\n",
    "It works through the dot product of each row and column.\n",
    "\n",
    "In order to multiply two matrices, the number of columns of A must equal the number of rows of B. The product\n",
    "AB will have the same number of rows as the first matrix and the same number of columns as the second.\n",
    "\n",
    "Algebraic rules for matrix multiplication:\n",
    "* Associative Law is true: $A(BC) = (AB)C$\n",
    "* Commutative Law is false: Often $AB \\ne BA$\n",
    "\n",
    "A note on matrix multiplication order.  When we multiply on the left side vs right side, it's the difference between acting on rows vs columns, which switches based on order.  Multiplying from the left, we're doing row operations.  Multiplying from the right, we're doing column operations.\n",
    "\n",
    "3Blue1Brown emphasized:\n",
    "- Viewing Matrices as transformation of space\n",
    "- Matrix multiplication is just one transformation after another [this may belong in subsequent section]\n",
    "\n",
    "### The Row Exchange Matrix\n",
    "To exchange aka permute rows we use another matrix $P_{ij}$ called the **permutation matrix**.  For example, the permutation matrix $P_{23}$ exchanges rows 2 and 3:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Permutation matrices can swap multiple rows as well, not just one.  We'll see that soon.\n",
    "\n",
    "### The Augmented Matrix\n",
    "\n",
    "We can augment the matrix $A$ in $Ax=b$ to include $b$ as an extra column, and allow it to change through the process of elimination.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62a3c076",
   "metadata": {},
   "source": [
    "## Rules for Matrix Operations (2.4)\n",
    "\n",
    "- Lecture for 2.4 and 2.5: [Multiplication and Inverse Matrices](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/multiplication-and-inverse-matrices/)\n",
    "\n",
    "A matrix with $n$ columns can multiply a matrix with $n$ rows:\n",
    " \n",
    "$$A_{m \\times n}B_{n \\times p} = C_{m \\times p}$$\n",
    "\n",
    "\n",
    "### Multiple ways to multiply matrices\n",
    "\n",
    "1. We went over the typical dot product way of multiplying matrices above, where the entry in row $i$, and column $j$ of $AB$ is (row $i$ of $A$) $\\cdot$ (column $j$ of $B$).\n",
    "\n",
    "Terminology note: A row times a column (a dot product) is also called an **inner product**.  A column times a row is called an **outer product**.\n",
    "\n",
    "Now let's talk about additional ways to multiply matrices..\n",
    "\n",
    "2. Matrix $A$ times every column of $B$: $A\\begin{bmatrix}b_1 \\cdots b_p \\end{bmatrix} = A\\begin{bmatrix}Ab_1 \\cdots Ab_p \\end{bmatrix}$.  Recall from the column picture perspective, that we can therefore see each column of $AB$ as a combination of columns of $A$.\n",
    "\n",
    "3. Every row of matrix $A$ times matrix $B$: \n",
    "$\\begin{bmatrix} \\text{row }i\\text{ of }A\\end{bmatrix}B = \\begin{bmatrix}\\text{row }i \\text{ of }AB\\end{bmatrix}.$\n",
    "\n",
    "4. Multiply columns $1$ to $n$ of $A$ times rows $1$ to $n$ of $B$. Add those matrices. So for example:\n",
    "$$\n",
    "AB = \\begin{bmatrix}a \\\\ c\\end{bmatrix}\\begin{bmatrix}E & F\\end{bmatrix} + \\begin{bmatrix}b \\\\ d\\end{bmatrix}\\begin{bmatrix}G & H\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "You'll find that it works out just like the other methods.\n",
    "\n",
    "### Blocks\n",
    "\n",
    "Matrices can be added and multiplied by **blocks**, so long as the block sizes correspond to the normal rules-- same size for addition, and rows of 1 = cols of 2 for multiplication. \n",
    "\n",
    "Important: Cuts between columns of $A$ must match cuts between rows of $B$.\n",
    "\n",
    "Matrix block multiplication example:\n",
    "\n",
    "$A = \\begin{bmatrix}A_1 & A_2\\end{bmatrix}$ times $B = \\begin{bmatrix}B_1 \\\\ B_2\\end{bmatrix}$ is $A_{1}B_1 + A_{2}B_2$.\n",
    "\n",
    "The blocks must be equal across transposition, so for example you could have:\n",
    "* Two square matrices split up with each corner a block\n",
    "* Block columns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbcd6dc0",
   "metadata": {},
   "source": [
    "## Inverse Matrices (2.5)\n",
    "\n",
    "If the square matrix $A$ has an inverse, then both $A^{-1}A = I$ and $A^{-1}A = I$.  Note that non-square matrices are not invertible.\n",
    "\n",
    "Testing for invertibility:\n",
    "\n",
    "- The _algorithm_ to test invertibility is elimination. $A$ must have $n$ (nonzero) pivots\n",
    "- The _algebra_ test for invertibility is the determinant of $A$. $\\det A$ must not be $0$.\n",
    "- The _equation_ that test for invertibility is $Ax = 0$.  $x = 0$ must be the only solution.\n",
    "\n",
    "A matrix cannot have more than one inverse.  If you found the left-inverse, it must be the same as the right-inverse.\n",
    "\n",
    "### The Inverse of a Product AB\n",
    "\n",
    "If $A$ and $B$ are invertible, then so is $AB$:\n",
    "\n",
    "$$(AB)^{-1} = B^{-1}A^{-1}$$\n",
    "\n",
    "### Gauss-Jordan Elimination\n",
    "Gauss-Jordan eliminates $\\begin{bmatrix}A & I\\end{bmatrix}$ to $\\begin{bmatrix}I & A^{-1}\\end{bmatrix}$.\n",
    "\n",
    "The Gauss-Jordan method is to begin with that augmented matrix, $\\begin{bmatrix}A & I\\end{bmatrix}$, and performing elimination until you get the left block upper triangular.  Then, continue doing elimination upwards, so that you have only a diagonal of pivots on the left.  Finally, divide each row to get **reduced echelon form** ($R=I$) on the left hand side.  Then your inverse will be on the right hand side.\n",
    "\n",
    "This helps explain why the determininant can't be 0 for a matrix with an inverse, you have to divde by the pivots, and you can't divide by 0.\n",
    "\n",
    "**Diagonally dominant** matrices are invertible.  If the absolute value of the diagonal entries are larger than the sum of the absolute values of the rest of their rows, then the matrix is invertible.  This follows from the fact that the other row entires cannot add up to equal those entries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abfb2997",
   "metadata": {},
   "source": [
    "## Elimination = Factorization: A = LU (2.6)\n",
    "Lecture: [Factorization into A=LU](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/factorization-into-a-lu-1/)\n",
    "\n",
    "In the previous section, we went from $A$ to $U$ with elimination.  In this section, we look at elimination in the most useful way.\n",
    "\n",
    "Many key ideas of linear algebra, when you look at them closely, are really _factorizations_ of a matrix. The first factorization we look at comes from elimination.  The factors $L$ and $U$ are triangular matrices. The factorization that comes from elimination is $A=LU$.\n",
    "\n",
    "We already know about $U$, the upper triangular matrix, from producing it during elimination. Reversing those steps, taking $U$ back to $A$ is achieved by a lower triangular $L$.\n",
    "\n",
    "Each elimination step $E_{ij}$ is inverted by $L_{ij}$. The entries of $L$ are exactly the multipliers $l_{ij}$. Every multiplier $l_{ij}$ is in row $i$, column $j$ of $L$.\n",
    "\n",
    "Here's a 2x2 example going forward from $A$ to $U$, then back from $U$ to $A$:\n",
    "\n",
    "$$\n",
    "E_{21}A = \\begin{bmatrix}1 & 0 \\\\ -3 & 1\\end{bmatrix}\\begin{bmatrix}2 & 1 \\\\ 6 & 8\\end{bmatrix} = \\begin{bmatrix}2 & 1 \\\\ 0 & 5\\end{bmatrix} = U \\\\\n",
    "E_{21}^{-1}U = \\begin{bmatrix}1 & 0 \\\\ 3 & 1\\end{bmatrix}\\begin{bmatrix}2 & 1 \\\\ 0 & 5\\end{bmatrix} = \\begin{bmatrix}2 & 1 \\\\ 6 & 8\\end{bmatrix} = A\n",
    "$$\n",
    "\n",
    "The second line is our factorization $LU=A$. The whole forward elimination process (with no row exchanges) is inverted by $L$.  Just as $E$ is all eliminations, $L$ is all the inverse eliminations.\n",
    "\n",
    "### Predicting zeroes in L and U\n",
    "\n",
    "We can predict the zeroes in $L$ and $U$ from $A$:\n",
    "\n",
    "- When a row of $A$ starts with zeroes, so does that row of $L$\n",
    "- When a column of $A$ starts with zeroes, so does that column of $U$\n",
    "\n",
    "But note that zeros in the middle of the matrix are likely to be filled in, while elimination sweeps forward.\n",
    "\n",
    "### Better balance from LDU\n",
    "\n",
    "$A=LU$ is not \"symmetric\" in that $A$ has 1s on its pivots while $U$ does not.  This is easy to fix.  Divide $U$ by a diagonal matrix $D$ that contains the pivots. That leaves a new triangular matrix with 1's on the diagonal. E.g:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}1 & 0 \\\\ 3 & 1\\end{bmatrix}\\begin{bmatrix}2 & 8 \\\\ 0 & 5\\end{bmatrix} \\text{ splits further into }\n",
    "\\begin{bmatrix}1 & 0 \\\\ 3 & 1\\end{bmatrix}\\begin{bmatrix}2 & 0 \\\\ 0 & 5\\end{bmatrix}\\begin{bmatrix}1 & 4 \\\\ 0 &1\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### How expensive is elimination\n",
    "\n",
    "The first stage of elimination produces zeros below the first pivot in column 1. To find each entry below the pivot requires one multiplication and one subtraction. We count this first stage as $n^2$ multiplications and $n^2$ subtractions. It is actually less ($n^2 -n$) because row 1 doesn't change.\n",
    "\n",
    "The next stage clears out the second column below the second pivot. The working matrix is now of size $n-1$. We estimate this stage as $(n-1)^2$ multiplications and subtractions.\n",
    "\n",
    "The rough count to reach $U$ is the sum of squares $n^2 + (n-1)^2 + \\cdots + 2^2 + 1^2$. There is an exact formula ([proofs here](https://math.stackexchange.com/questions/48080/sum-of-first-n-squares-equals-fracnn12n16)) $\\frac{1}{3}n(n+\\frac{1}{2})(n + 1)$ for this sum of squares. For considering the cost/complexity here, we can just pay attention to the largest term, and say:\n",
    "\n",
    "Elimination on A requires about $\\frac{1}{3}n^3$ multiplications and $\\frac{1}{3}n^3$ subtractions.\n",
    "\n",
    "What about the right side? Going forward, we subtract multiple of $b_1$ from the components below. This is $n-1$ steps. The second stage takes only $n-2$ steps, because $b_1$ is not involved. The last stage of forward elimination takes one step.\n",
    "\n",
    "Then, for back substitution, $x_n$ takes one step (divide by the last pivot).  The next unknown takes two steps. When we reach $x_1$ it will require $n$ steps ($n-1$ substitutions of the other unknowns, then division by the first pivot). \n",
    "\n",
    "The total count on the right side, from $b$ to $c$ to $x$, forward and backward, is therefore exactly $n^2$, which we can see from:\n",
    "\n",
    "$$\n",
    "[(n - 1) + (n - 2) + \\cdots 1] + [1 + 2 + \\cdots + (n-1) + n] = n^2\n",
    "$$\n",
    "\n",
    "So the right side takes $n^2$ multiplications and $n^2$ subtractions in total."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67d9e6c6",
   "metadata": {},
   "source": [
    "## Transposes and Permutations (2.7)\n",
    "\n",
    "[Lecture - Transposes, Permutations, Vector Spaces](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/transposes-permutations-vector-spaces-1/).  This lecture covers Vector Spaces as well.\n",
    "\n",
    "The **transpose** of $A$ is denoted by $A^T$. The columns of $A^T$ are the rows of $A$:\n",
    "\n",
    "$$(A^T)_{ij} = A_{ji}$$\n",
    "\n",
    "When $A$ is an $m$ by $n$ matrix, the transpose is $n$ by $m$:\n",
    "\n",
    "$$\n",
    "\\text{If }A=\\begin{bmatrix}1 & 2 & 3 \\\\ 0 & 0 & 4\\end{bmatrix} \\text{ then } A^T = \\begin{bmatrix}1 & 0 \\\\ 2 & 0 \\\\ 3 & 4\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The matrix \"flips over\" its main diagonal.\n",
    "\n",
    "### Rules of transposes\n",
    "\n",
    "- Sum: The transpose of $A + B$ is $A^T + B^T$\n",
    "- Product: The transpose of $AB$ is $(AB)^T = B^TA^T$\n",
    "- Inverse: The transpose of $A^{-1}$ is $(A^{-1})^T = (A^T)^{-1}$\n",
    "\n",
    "Notice how $B^TA^T$ comes in reverse order. This follows from how matrix multiplication works.  We get back to the same operations by transposing and flipping the order.\n",
    "\n",
    "The reverse order rule applies to three or more factors, so $(ABC)^T = C^TB^TA^T$.\n",
    "\n",
    "Now let's prove the inverse rule.  Start with $A^{-1}A = I$.  Apply the product rule above, and we get $A^T(A^{-1})^T = I$.  This shows that $(A^{-1})^T = (A^T)^{-1}$. It also follows that $A^T$ is invertible exactly when $A$ is invertible.\n",
    "\n",
    "[there's a section on The meaning of inner products which i dont grok, but maybe will after watching the lecture video.]\n",
    "\n",
    "### Symmetric Matrices\n",
    "\n",
    "For a symmetric matrix, transposing $A$ to $A^T$ produces no change. A symmetric matrix has $S^T = S$, meaning $S_{ji} = S_{ij}$.\n",
    "\n",
    "The inverse of a symmetric matrix is also symmetric, so $(S^{-1})^T = (S^T)^{-1} = S^{-1}$.\n",
    "\n",
    "The product of a matrix and its transpose will always be symmetric.  $A^TA$ is always symmetric.  We can see why this is true from the transpose equaling itself, which is our definition of symmetric: $(AA^T)^T = A^{TT}A^T = AA^T$.\n",
    "\n",
    "### Symmetric Products\n",
    "\n",
    "It follows from the product rule that the transpose of $A^TA$ is $A^T(A^T)^T$ which is $A^TA$ again.  \n",
    "\n",
    "Also, a symmetric invertible matrix will have a symmetric factorization, simpler than $S = LDU$, it will have $S=LDL^T$.\n",
    "\n",
    "### Permutation Matrices\n",
    "\n",
    "The transpose plays a special role for a **permutation matrix**.  This matrix P has a single \"1\" in every row and column.  \n",
    "\n",
    "Then $P^T$ is also a permutation matrix, maybe the same as $P$ or maybe different.  Any product $P_1P_2$ is again a permutation matrix.\n",
    "\n",
    "The simplest permutation matrix is $P = I$ (no exchanges).  The next simplest are the row exchanges $P_{ij}$. Other permutations reorder more rows.  By doing all possible row exchanges to $I$, we get all possible permutation matrices.  There are 6 3x3 permtuation matrices:\n",
    "\n",
    "$$\n",
    "\\;\\;I=\\begin{bmatrix}\n",
    "1 & & \\\\\n",
    "& 1 & \\\\\n",
    "& & 1\n",
    "\\end{bmatrix} \\quad \n",
    "P_{21}=\\begin{bmatrix}\n",
    "& 1 & \\\\\n",
    "1 & & \\\\\n",
    "& & 1\n",
    "\\end{bmatrix} \\quad \n",
    "P_{32}P_{21}=\\begin{bmatrix}\n",
    "& 1 & \\\\\n",
    "& & 1 \\\\\n",
    "1 & &\n",
    "\\end{bmatrix} \\\\ \\\\\n",
    "\n",
    "P_{31}=\\begin{bmatrix}\n",
    "& & 1 \\\\\n",
    "& 1 & \\\\\n",
    "1 & &\n",
    "\\end{bmatrix} \\quad \n",
    "P_{32}=\\begin{bmatrix}\n",
    "1 & & \\\\\n",
    "& & 1 \\\\\n",
    "& 1 &\n",
    "\\end{bmatrix} \\quad \n",
    "P_{21}P_{32}=\\begin{bmatrix}\n",
    "& & 1 \\\\\n",
    "1 & & \\\\\n",
    "& 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "There are $n!$ permutation matrices of order $n$.\n",
    "\n",
    "$P^{-1}$ is also a permutation matrix.  Among the P's displayed above, the four matrices on the left are their own inverses.  The two matrices on the right are inverses of each other.  In all cases, a single row exchange is its own inverse.\n",
    "\n",
    "$P^{-1}$ is always the same as $P^T$. So $PP^T = I$.\n",
    "\n",
    "Permutations (row exchanges before elimination) lead to $PA = LU$.\n",
    "\n",
    "### The PA = LU Factorization with Row Exchanges\n",
    "\n",
    "There are multiple ways we could approach permutations during elimination. \n",
    "\n",
    "1. We could do row exchanges in advance.  Then $PA=LU$.\n",
    "2. If we hold row exchanges until after elimination, the pivot rows are in a strange order.  Then $A= LPU$\n",
    "\n",
    "We will focus on the 1st one for our work; it's also the way computers do it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fceefd4",
   "metadata": {},
   "source": [
    "# Vector Spaces and Subspaces (3)\n",
    "## Spaces of Vectors (3.1)\n",
    "* First lecture on it [starts here](https://youtu.be/JibVXBElKL0?t=1246), the second part of the previous lecture\n",
    "* Second lecture on it is [first part here](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/column-space-and-nullspace-1/)\n",
    "\n",
    "The space $R^n$ consists of all column vectors $v$ with $n$ components.\n",
    "\n",
    "The components of $v$ are real numbers, which is the reason for the letter $R$.  A vector with complex numbers lies in the space $C^n$.\n",
    "\n",
    "We can add vectors and multiply by scalars (produce linear combinations) in a space, and the results remain in the space.  Every vector space must include the zero vector.\n",
    "\n",
    "The smallest possible vector space is $Z$, which only includes the zero vector.  Each space has its own zero vector.\n",
    "\n",
    "### Subspaces\n",
    "\n",
    "There are important vector spaces inside $R^n$.  Those are the **subspaces** of $R^n$.\n",
    "\n",
    "An example is a plane through the origin of $R^3$. That plane is a vector space in its own right. If we add two vectors in the plane, their sum is in the plane. If we multiply an in-plane vector by 2 or -5, it is still in the plane.\n",
    "\n",
    "Formal definition: A subspace of a vector space is a set of vectors (including 0) that satisfies two requirements.  If $v$ and $w$ are vectors in the subspace and $c$ is any scalar:\n",
    "1. $v + w$ is in the subspace\n",
    "2. $cv$ is in the subspace\n",
    "\n",
    "Or, to compress both rules into one: a subspace containing $v$ and $w$ must contain all linear combinations $cv + dw$.\n",
    "\n",
    "So a plane that doesn't go through the origin would fail that definition.\n",
    "\n",
    "Note that vector spaces count as subspaces, so $R^3$ etc. are subspaces.  They are subspaces of themselves, really.  Here is a list of all the possible subspaces of $R^3$:\n",
    "- Any line through $(0,0,0)$\n",
    "- The whole space ($R^3$)\n",
    "- Any plane through $(0,0,0)$\n",
    "- The zero vector $(0,0,0)$.\n",
    "\n",
    "An upper quadrant line would not be a subspace, because you could multiply by -1 and end up outside of the subspace.\n",
    "\n",
    "The union of two subspaces will only be a subspace if one of the subspaces contains the other.  The intersection of two subspaces will always be a subspace.\n",
    "\n",
    "### The Column Space of A\n",
    "\n",
    "The **column space** consists of all linear combinations of the columns (all possible $b$s in $Ax=b$). They fill the column space $C(A)$.\n",
    "\n",
    "The system $Ax=b$ is solvable if and only if $b$ is in the column space of $A$.\n",
    "\n",
    "Suppose we have an $m$ by $n$ matrix. The columns belong to $R^m$.  The column space of $A$ is a subspace of $R^m$.\n",
    "\n",
    "The set of all column combinations $Ax$ satisfies the rules for a subspace: when we add linear combinations of them, we still produce combinations of the columns.\n",
    "\n",
    "Instead of columns of $R^n$ we could start with any set $S$ of vectors in a vectors space $V$.  To get a subspace $SS$ of $V$, we take all combinations of the vectors in that set.\n",
    "\n",
    "So column space is an example of a span, of the column vectors.  The columns there \"span\" the column space.\n",
    "\n",
    "The subspace $SS$ is the span of $S$, containing all combinations of vectors in $S$.\n",
    "\n",
    "### 8 rules of vector spaces\n",
    "These rules are also covered [on Wolfram](https://mathworld.wolfram.com/VectorSpace.html).\n",
    "\n",
    "In the definition of a vector space, vector addition $X + Y$ and scalar multiplication $cx$ must obey the following 8 rules:\n",
    "\n",
    "1. $X + Y = Y + X$ (Commutativity of vector addition)\n",
    "2. $X + (Y + Z) = (X + Y) + Z$ (Associativity of vector addition)\n",
    "3. There is a unique zero vector, such that $X + 0 = X$ for all $X$ (Additive identity)\n",
    "4. For each $X$ there is a unique vector $-X$ such that $X + (-X) = 0$ (Existence of additive inverse)\n",
    "5. $1$ times $X$ equals $X$ (Scalar multiplication identity)\n",
    "6. $(c_1c_2)X = c_1(c_2X)$ (Associativity of scalar multiplications)\n",
    "7. $c(X + Y) = cX + cY$ (Distributivity of vector sums)\n",
    "8. $(c_1 + c_2)X = c_1X + c_2X$ (Distributivity of scalar sums)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "334ba622",
   "metadata": {},
   "source": [
    "## The Nullspace of A: Solving Ax = 0 and Rx = 0 (3.2)\n",
    "* Lecture [begins here](https://youtu.be/8o5Cmfpeo6g?t=1677)\n",
    "\n",
    "The *nullspace*, denoted $N(A)$, consists of all solutions to $Ax=0$.  These vectors $x$ are in $R^n$. \n",
    "\n",
    "For invertible matrices, $x=0$ is the only solution to $Ax=0$. For noninvertible matrices, there are non-zero solutions to $Ax=0$.  Each solution $x$ belongs to the nullspace of $A$.\n",
    "\n",
    "The solution vectors, the null space, forms a subspace.  Suppose $x$ and $y$ are in the nullspace (meaning $Ax=0$ and $Ay=0$).  Then $A(x + y) = 0 + 0$ and $A(cx) = c0$, meaning that both adding vectors in the nullspace, and scalar multiplying vectors in the nullspace, produces more vectors within the nullspace.  Since we can add and multiply without leaving the nullspace, it's a subspace.\n",
    "\n",
    "### Special solutions\n",
    "\n",
    "To describe the solutions to $Ax=0$, an efficient way is to choose one point on the line (one *special solution*).  Then all points on the line are multiples of this one.\n",
    "\n",
    "Example with a 2x2 matrix: The nullspace of $A = \\begin{bmatrix}1 & 2 \\\\ 3 & 6\\end{bmatrix}$ contains all multiples of $s = \\begin{bmatrix}-2 \\\\ 1\\end{bmatrix}$.  This solution is special because we set the free variable to $x_2 = 1$.\n",
    "\n",
    "The nullspace consists of all combinations of the special solutions to $Ax = 0$.\n",
    "\n",
    "Example with two free variables: $x + 2y + 3z = 0$ comes from the 1x3 matrix $A = \\begin{bmatrix}1 & 2 & 3\\end{bmatrix}$.  Then $Ax=0$ produces a plane, which is the nullspace of $A$.  There are two free variables, $y$ and $z$, which we alternately set to 0 and 1:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}x \\\\ y \\\\ z\\end{bmatrix} \\text{ has two special solutions }s_1 = \\begin{bmatrix}-2 \\\\ \\textbf{1} \\\\ \\textbf{0}\\end{bmatrix} \\text{ and } s_2 = \\begin{bmatrix}-3 \\\\ \\textbf{0} \\\\ \\textbf{1}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Those vectors $s_1$ and $s_2$ lie on the plane $x + 2y + 3z = 0$.  All vectors on the plane are combinations of $s_1$ and $s_2$.\n",
    "\n",
    "The last two components are \"free\" and we choose them specially as 1,0 and 0,1.  Then the first components -2 and -3 are determined by the equation $Ax=0$.\n",
    "\n",
    "What about when dealing with more than two free components?  Then each special solution will have a 1 in a free spot and 0s in the rest.\n",
    "\n",
    "The solutions to $x + 2y + 3z = 6$ also lie on a plane, but that plane is not a subspace.  We will explore the solutions for these types of equations later.\n",
    "\n",
    "### Pivots, and (reduced) row echelon form in rectangular matrices\n",
    "Previously we've dealt with square matrices where the pivots would always be clean across the diagonal.  More broadly, the pivots are the leading non-zero value of each row when the matrix is in [row echelon form](https://en.wikipedia.org/wiki/Row_echelon_form), which is defined as:\n",
    "- All rows consisting of only zeroes are at the bottom\n",
    "- The leading entry of every nonzero row is to the right of the leading entry of every row above\n",
    "\n",
    "Here's an example in row echelon form:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & a_0 & a_1 & a_2 & a_3 \\\\\n",
    "0 & 0 & 2 & a_4 & a_5 \\\\\n",
    "0 & 0 & 0 & 1 & a_6 \\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note: Strang doesn't really use \"row echelon form\", instead referring to that stage as upper-triangular U, which terminologically I think is only supposed to apply to square matrices, but whatever.\n",
    "\n",
    "A matrix is in reduced row echelon form if:\n",
    "- It is in row echelon form (see above)\n",
    "- The leading entry (pivot) in each non-zero row is a 1\n",
    "- Each column containing a leading 1 has zeros in all other entries\n",
    "\n",
    "Here is a matrix in reduced row echelon form:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & a_1 & 0 & b_1 \\\\\n",
    "0 & 1 & a_2 & 0 & b_2 \\\\\n",
    "0 & 0 & 0 & 1 & b_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "### Pivot columns and free columns\n",
    "\n",
    "The first column of $A = \\begin{bmatrix}1 & 2 & 3\\end{bmatrix}$ contains the only pivot, so the first component of $x$ is not free.  The **free components** correspond to columns with no pivots.  The special choice (one or zero) is only for the free variables in the special solutions.\n",
    "\n",
    "### Reduced Row Echelon Form\n",
    "\n",
    "The Nullspace stays the same as we go from the starting matrix to upper triangular to reduced row echelon form.  But the nullspace / special solutions are easiest to calculate from the reduced row echelon form.\n",
    "\n",
    "### Matrix Shape and Free Variables\n",
    "\n",
    "Suppose $Ax=0$ has more unknowns than equations ($n > m$, more columns than rows).  There must be at least one free column.  Then $Ax = 0$ has nonzero solutions.\n",
    "\n",
    "This follows from the fact that free variables can be set to 1 (special solutions), which negates x being a zero a solution.\n",
    "\n",
    "The nullspace is a subspace. Its \"dimension\" is the number of free variables.  Let's explore this further..\n",
    "\n",
    "### The Rank of a Matrix\n",
    "\n",
    "The numbers $m$ and $n$ give the size of a matrix, but not necessarily the true size of a linear system.  An equation like $0=0$ shouldn't count.  If there are two identical rows in $A$, the second one dissapears in elimination. Also if row 3 is a combination of rows 1 and 2, then row 3 will become zeros in row echelon form.  The true size of $A$ is given by its rank.\n",
    "\n",
    "The **rank** ($r$) of $A$ is the number of pivots.\n",
    "\n",
    "### Rank one, more on ranks\n",
    "\n",
    "Matrices of rank one have only one pivot.  With these matrices, when elimination produces zero in the first column, it produces zero in all the columns.  As a result, every row is a multiple of the pivot row, and every column is a multiple of the pivot column:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "1 & 3 & 10 \\\\\n",
    "2 & 6 & 20 \\\\\n",
    "3 & 9 & 30\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "R = \\begin{bmatrix}\n",
    "1 & 3 & 10 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The column space of a rank one matrix is \"one-dimensional\".  In the example above, all the columns are on the line through $u = (1,2,3)$. The columns of $A$ above are $u$ and $3u$ and $10u$.  Put those numbers into the row $v^T = \\begin{bmatrix} 1 & 3 & 10 \\end{bmatrix}$ and you have the special rank one from $A = uv^T$, which is $A = \\text{column times row} = uv^T$:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 3 & 10 \\\\\n",
    "2 & 6 & 20 \\\\\n",
    "3 & 9 & 30\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}1 \\\\ 2 \\\\ 3\\end{bmatrix}\n",
    "\\begin{bmatrix}1 & 3 & 10\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Our second definition of rank will be at a higher level.  It deals with entire rows and entire columns-- vectors and not just numbers.  We can define rank in terms of number of independent rows/cols.  \n",
    "\n",
    "Lastly, we can define rank in terms of spaces of vectors.  The rank is the \"dimension\" of the column space.  It is also the dimension of the row space.  And the great fact which we're saving for last: $n - r$ is the dimension of the nullspace (this follows from free columns producing the dimensions of the nullspace)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8a72789",
   "metadata": {},
   "source": [
    "## The Complete Solution to Ax = b (3.3)\n",
    "[Lecture for 3.3 and 3.4](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/independence-basis-and-dimension-1/)\n",
    "\n",
    "We're going to expand on the previous section by also considering cases where $b$ is not 0.  \n",
    "\n",
    "This means we'll have to keep track of it as we perform row operations on the left.  One way to organize this is to use the augmented matrix $\\begin{bmatrix}A b\\end{bmatrix}$, and go from $\\begin{bmatrix}A b\\end{bmatrix}$ to $\\begin{bmatrix}R d\\end{bmatrix}$ once in reduced row echelon form.\n",
    "\n",
    "### One particular solution $Ax_p = b$\n",
    "\n",
    "We denote our particular solution with $x_p$. For an easy particular solution $x_p$, choose the free variables to be zero, then we can solve for a particular solution using the multiples of the pivot varables that reach $d$.\n",
    "\n",
    "$x_n$ is the symbol for the nullspace solutions.  Unlike the particular, there can be multiple of these as we saw before, one for each special solution to the nullspace.\n",
    "\n",
    "Here's an example where we write out the **complete solution** $x_p + x_n$ to $Ax=b$:\n",
    "\n",
    "Let's start with $Ax=b$ as:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 3 & 0 & 2 \\\\\n",
    "0 & 0 & 1 & 4 \\\\\n",
    "1 & 3 & 1 & 6\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "x_4\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "6 \\\\\n",
    "7\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We perform elimination on the augmented matrix (subtract row 1 from row 3, then subtract row 2 from row 3):\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 3 & 0 & 2 & 1 \\\\\n",
    "0 & 0 & 1 & 4 & 6 \\\\\n",
    "1 & 3 & 1 & 6 & 7\n",
    "\\end{bmatrix} \n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 3 & 0 & 2 & 1 \\\\\n",
    "0 & 0 & 1 & 4 & 6 \\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "We find the particular solution by setting the free variables in columns 2 and 4 to 0:\n",
    "\n",
    "$$\n",
    "Rx_p = \n",
    "\\begin{bmatrix}\n",
    "1 & 3 & 0 & 2 \\\\\n",
    "0 & 0 & 1 & 4 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "1 \\\\ 0 \\\\ 6 \\\\ 0\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\ 6 \\\\ 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Next we incorporate our special solutions for the nullspace, i.e. Rx = **0**.  Bringing it all together we have:\n",
    "\n",
    "$$\n",
    "x = x_p + x_n = \\begin{bmatrix}1 \\\\ 0 \\\\ 6 \\\\ 0\\end{bmatrix} + x_2\\begin{bmatrix}-3 \\\\ 1 \\\\ 0 \\\\ 0\\end{bmatrix} + x_4\\begin{bmatrix}-2 \\\\ 0 \\\\ -4 \\\\ 1\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Suppose we had a square invertible matrix, then with no free variables, and a nullspace of only $0$, there's only one answer: $x = x_p + x_n = A^{-1}b + 0$.\n",
    "\n",
    "### Full column rank\n",
    "\n",
    "With **full column rank** ($r = n$), every column has a pivot.  Reducing these matrices puts $I$ at the top:\n",
    "\n",
    "$$\n",
    "R = \\begin{bmatrix}I \\\\ 0\\end{bmatrix} = \\begin{bmatrix}\\text{$n$ by $n$ identity matrix} \\\\ \\text{$m - n$ rows of zeros}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "There are no free columns or variables with these.  So the nullspace only contains the zero vector.\n",
    "\n",
    "If $A$ has full column rank, then $Ax=b$ has either 0 or 1 solutions, depending on whether $b$ is reachable.\n",
    "\n",
    "There will be $m - n$ rows of zeros in $R$.  Only $b$s on the right side that follow the conditions of those zero rows will be solvable.\n",
    "\n",
    "As we'll see in a following section, full column rank matrices have **independent columns**.\n",
    "\n",
    "### Full row rank\n",
    "\n",
    "The other extreme case is **full row rank** ($r=m$).  Now $Ax=b$ will have either one or infinitely many solutions.\n",
    "\n",
    "Every full row rank matrix will:\n",
    "1. Have pivots in every row\n",
    "2. Have a solution for every right side $b$\n",
    "3. Fill all of the column space $R^m$.\n",
    "4. Has $n -r = n - m$ special solutions in the nullspace of $A$.\n",
    "\n",
    "In this case we have **independent rows**.\n",
    "\n",
    "### Summing it up\n",
    "\n",
    "The four possibilities for linear equations depend on the rank $r$:\n",
    "\n",
    "\n",
    "|  | | |\n",
    "| --- | --- | --- |\n",
    "| $r=m$ and $r=n$ | Square and invertible | 1 solution |\n",
    "| $r=m$ and $r<n$ | Full row rank | $\\infty$ solutions |\n",
    "| $r<m$ and $r=n$ | Full column rank | 0 or 1 solution |\n",
    "| $r<m$ and $r<n$ | Not full rank | 0 or $\\infty$  solutions |\n",
    "\n",
    "And we'll get four types of $R$ after reduction:\n",
    "\n",
    "|  | | | |\n",
    "| --- | --- | --- | --- |\n",
    "| $\\begin{bmatrix}I\\end{bmatrix}$ | $\\begin{bmatrix}I & F\\end{bmatrix}$ | $\\begin{bmatrix}I \\\\ 0\\end{bmatrix}$ | $\\begin{bmatrix}I & F \\\\ 0 & 0\\end{bmatrix}$ |\n",
    "| $r = m = n$ | $r = m < n$ | $r = n < m$ | $r < m, r < n$ |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d4134dc",
   "metadata": {},
   "source": [
    "## Independence, Basis and Dimension (3.4)\n",
    "[Lecture for 3.3 and 3.4](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/independence-basis-and-dimension-1/)\n",
    "\n",
    "The true dimension of the column space is the rank $r$.\n",
    "\n",
    "The **basis** for a space are independent vectors that span the space.  Every vector in the space is a unique combination of the basis vectors.\n",
    "\n",
    "The **dimension** of a space is the number of vectors in a basis.\n",
    "\n",
    "### Linear Independence\n",
    "\n",
    "The columns of $A$ are **linearly independent** when the only solution to $Ax=0$ is $x=0$.  Or put another way, the columns are independent when the nullspace $N(A)$ contains only the zero vector.  \n",
    "\n",
    "Geometrically, if three vectors are not in the same plane, they are independent.  Conversely, if three vectors are in the same plane, they are dependent.\n",
    "\n",
    "The formal definition of linear independence covers more broadly combinations of vectors not in a matrix $A$, i.e sequences of vectors.  Same idea though; the only linear combination of them that equals 0 should occur when you take 0 times each one.\n",
    "\n",
    "Note that a sequence containing the zero vector will always be dependent.  \n",
    "\n",
    "Three vectors in $R^2$ cannot be independent.  A couple of ways of seeing this. One is that the matrix $A$ with those three columns must have a free variable and then a special solution to $Ax=0$.  Another way: if the two vectors are independent, some combination of them will produce the third vector, because they fill up $R^2$.\n",
    "\n",
    "Let's say we get a three vectors in $R^3$, and we're asked to determine if they're dependent.  We could see this by plugging them into a 3x3 matrix $A$ and seeing if $Ax=0$ has a non-zero solution.  If it does, they're dependent.\n",
    "\n",
    "In a square matrix, dependent columns imply dependent rows.\n",
    "\n",
    "When vectors are independent, the matrix of independent columns will be of full column rank. Whereas any set of vectors in $R^m$ must be linearly dependent if $n > m$. \n",
    "\n",
    "The columns might be dependent or independent, if $n \\le m$.  Elimination will reveal the pivot columns, which are the independent ones.\n",
    "\n",
    "### Vectors that span a subspace\n",
    "\n",
    "A set of vectors **spans** a space if their linear combinations fill the space.  For example, $\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$ and  $\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$ span all of $R^2$. $\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$ and  $\\begin{bmatrix}-1 \\\\ -1\\end{bmatrix}$ only span a line in $R^2$.\n",
    "\n",
    "It's fine to \"overkill\" on a span, like have another vector that doesn't add anything, so long as it's spanning.\n",
    "\n",
    "Let's introduce a new subspace, which is spanned by the rows.  The combination of the rows produces the **row space** in $R^n$.  The row space of $A$ is the column space of $A^T$.\n",
    "\n",
    "### A basis for a vector space\n",
    "\n",
    "We want enough independent vectors to span a space, and not more. A **basis** for a vector space, defined as a sequence of vectors which are linearly independent and span the space, provides this. \n",
    "\n",
    "The columns of the $n$ by $n$ identity matrix give the **standard basis** for $R^n$.\n",
    "\n",
    "The columns for every invertible $n$ by $n$ matrix give a basis for $R^n$.\n",
    "\n",
    "The vectors $v_1, \\dots, v_n$ are a basis for $R^n$ precisely when they are the columns of an $n$ by $n$ invertible matrix. Thus $R^n$ has infinitely many different bases.\n",
    "\n",
    "Note that going from $A$ to $R$, the column spaces/bases change, while retaining the same dimension.  But row space however does not change between $A$ and $R$.\n",
    "\n",
    "### Dimension of a vector space\n",
    "\n",
    "The number of vectors in every basis is the **dimension** of the space. A line has dimension 1, a plane dimension 2.\n",
    "\n",
    "Column Space of $A$ has dimension $r$, and the nullspace of $A$ has dimension $n-r$.\n",
    "\n",
    "### Bases for Matrix Spaces and Function Spaces\n",
    "\n",
    "Independence/basis/dimension is not limited to column vectors.  We can apply these concepts to matrices and functions as well.\n",
    "\n",
    "#### Matrix spaces\n",
    "\n",
    "We can ask whether matrices are dependent by asking whether some combination of them produces the zero matrix.  And we can ask the dimenion, for example the dimension of a 3 by 4 matrix space is 12.\n",
    "\n",
    "- The dimension of the whole $n$ by $n$ matrix space is $n^2$\n",
    "- The dimension of the subspace upper triangular matrices is $\\frac{1}{2}n^2 + \\frac{1}{2}n$.\n",
    "- The dimension of the subspace of diagonal matrices is $n$.\n",
    "- The dimension of the subspace of symmetric matrices is $\\frac{1}{2}n^2 + \\frac{1}{2}n$.\n",
    "\n",
    "#### Function spaces\n",
    "\n",
    "In differential equations $d^2y/dx^2 = y$ has a space of solutions. One basis is $y = e^x$ and $y = e^{-x}$.  The dimension there is 2, because of the second derivative.\n",
    "\n",
    "- $y'' =0$ is solved by any linear function $y = cx + d$\n",
    "- $y'' = -y$ is solved by any combination $y = c\\sin{x} + d\\cos{x}$\n",
    "- $y'' = y$ is solved by any combination $y = ce^x + de^{-x}$\n",
    "\n",
    "That solution space for $y'' = -y$ has two basis functions: $\\sin{x}$ and $\\cos{x}$. The space for $y'' = 0$ has $x$ and $1$.  It is the \"nullspace\" of the second derivative.  The dimension in each case is 2.\n",
    "\n",
    "The solutions of $y'' = 2$ don't form a subspace, because the rightside $b=2$ is not zero.  A particular solution is $x^2$.  The complete solution is $y(x) = x^2 + cx + d$.  All those functions satisfy $y'' = 2$.  Notice the particular solution plus any function $cx + d$ in the nullspace.  A linear differential equation is like a linear matrix equation $Ax=b$, but we solve it by calculus instead of linear algebra.\n",
    "\n",
    "### Basis of space Z\n",
    "\n",
    "The space $Z$ contains only the zero vector. The dimension of this space is zero. The empty set (containing no vectors) is a basis for $Z$. We can never allow the zero vector into a basis, because then linear independence is lost.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "528876fa",
   "metadata": {},
   "source": [
    "## Dimensions of the Four Subspaces (3.5)\n",
    "\n",
    "[Lecture here](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/the-four-fundamental-subspaces-1/)\n",
    "\n",
    "The rank of $A$ reveals the dimensions of all four fundamental subspaces.  We're introducing a new one here:\n",
    "\n",
    "1. The row space is $C(A^T)$, a subspace of $R^n$\n",
    "2. The column space is $C(A)$, a subspace of $R^m$\n",
    "3. The nullspace is $N(A)$, a subspace of $R^n$\n",
    "4. The **left nullspace** is $N(A^T)$, a subspace of $R^m$.\n",
    "\n",
    "For the left nullspace we solve $A^Ty = 0$. The vectors $y$ go on the left side of $A$ when the equation is written $y^TA = 0^T$.\n",
    "\n",
    "Whereas the row space and the column space have the same dimension $r$,  $N(A)$ and $N(A^T)$ have dimensions $n-r$ and $m-r$, to make up the full $n$ and $m$.\n",
    "\n",
    "### The four subspaces for R\n",
    "\n",
    "Suppose $A$ is reduced to its reduced row echelon form $R$. Two of the subspaces will remain the same, and two will change-- Row space and Null space are the same between $A$ and $R$, but Column Space and Left Nullspace change.   But all of them will retain the same dimension for both $A$ and $R$.\n",
    "\n",
    "The left nullspace looks for combinations of rows that equal to zero.  In reduced echelon form, this will always be multiples of the zero rows at the bottom, with the previous rows set to 0 because they are linearly independent and can't add up to zero together.\n",
    "\n",
    "### Rank one matrices\n",
    "\n",
    "Every rank one matrix is one column times one row. $A=uv^T$.  Example:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2 & 3 & 7 & 8 \\\\\n",
    "2a & 3a & 7a & 8a \\\\\n",
    "2b & 3b & 7b & 8b \n",
    "\\end{bmatrix} = \\begin{bmatrix}1 \\\\ a \\\\ b\\end{bmatrix}\\begin{bmatrix}2 & 3 & 7 & 8\\end{bmatrix} = uv^T\n",
    "$$\n",
    "\n",
    "### Rank two matrices = Rank one plus rank one\n",
    "\n",
    "Every rank $r$ matrix is a sum of $r$ rank one matrices.\n",
    "\n",
    "If $EA=R$, the last $m-r$ rows of $E$ are a basis for the left nullspace of $A$.\n",
    "\n",
    "### Products and Rank\n",
    "\n",
    "All the rows of $AB$ are combinations of the rows of $B$.  So the row space of $AB$ is contained in or equal to the row space of $B$. Rank(AB) $\\le$ Rank(B).\n",
    "\n",
    "All the columns of $AB$ are combinations of the columns of $A$.  So the column space of $AB$ is contained in or equal to the column space of $A$. Rank(AB) $\\le$ Rank(A).\n",
    "\n",
    "If we multiply by an invertible matrix, the rank will not change.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c07195b6",
   "metadata": {},
   "source": [
    "# Orthogonality (4)\n",
    "## Orthogonality of the Four Subspaces (4.1)\n",
    "\n",
    "[Lecture video here](https://youtu.be/YzZUIYRCE38)\n",
    "\n",
    "Two vectors are orthogonal when their dot product is zero: $v \\cdot w = v^Tw = 0$. \n",
    "\n",
    "$||v||^2 + ||w||^2 = ||v + w||^2$ [why?]\n",
    "\n",
    "The row space is perpindicular to the nullspace.  Every row of $A$ is perpindicular to every solution of $Ax=0$.\n",
    "\n",
    "The column space is perpindicular to the nullspace of $A^T$.\n",
    "\n",
    "Two subspaces $V$ and $W$ of the a vector space are **orthogonal** if every vector $v$ in $V$ is perpindicular to every vector $w$ in $W$.\n",
    "\n",
    "Examples for clarification: The floor of your room is a subspace $V$. The line where two walls meet is a one-dimensional subspace $W$.  Those subspaces are orthogonal.  Every vector up the meeting line of the walls is perpindicular to every vector in the floor.\n",
    "\n",
    "In contrast, two walls are not orthogonal.  Their meeting line is in both $V$ and $W$, and the line is not perpindicular to itself.  Two planes (dimensions 2 and 2 in $R^3$) cannot be orthogonal subspaces.\n",
    "\n",
    "When a vector is in two orthogonal subspaces, it must be zero, which is perpindicular to itself.\n",
    "\n",
    "Orthogonality is impossible when dim $V$ + dim $W \\gt$ dim (whole space).\n",
    "\n",
    "This graphic explains why the row space being perpindicular to the nullspace follows from $Ax = 0$:\n",
    "\n",
    "![image.png](images/nullspace-rowspace.png)\n",
    "\n",
    "Every row has a zero dot product with $x$.  So $x$ is also perpindicular to every combination of the rows.  So the whole row space is orthogonal to the nullspace.\n",
    "\n",
    "The same logic we applied to showing why the row space is perpindicular to the nullspace can be applied to recognize the column space as perpindicular to the left null space:\n",
    "\n",
    "![image.png](images/leftnullspace-colspace.png)\n",
    "\n",
    "### Orthogonal Complements\n",
    "\n",
    "The **orthogonal complement** of a subspace $V$ contains every vector that is perpindicular to $V$.  The orthogonal complement is denoted by $V^\\perp$.  \n",
    "\n",
    "The orthogonal complement is the largest dimension orthogonal subspace.\n",
    "\n",
    "Part 2 of the fundamental theorem of linear algebra is that the nullspace is the orthogonal complement of the row space in $R^n$, and the left null space is the orthogonal complement of the column space in $R^m$.\n",
    "\n",
    "The point of complements is that every $x$ can be split into a row space component, $x_r$, and a nullspace component $x_n$.  When $A$ multiplies $x = x_r + x_n$, what happens to $Ax = Ax_r + Ax_n$ is the null space component goes to zero, $Ax_n = 0$, and the row space component goes to the column space $Ax_r = Ax$.\n",
    "\n",
    "If a matrix is of full rank, every vector $b$ in the column space comes from one and only one vector $x_r$ in the row space.\n",
    "\n",
    "There is an $r$ by $r$ invertible matrix hiding inside $A$, if we throw away the two nullspaces.  From the row space to the column space, $A$ is invertible.  Example:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "3 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 5 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\quad\\text{ contains the submatrix }\n",
    "\\begin{bmatrix}\n",
    "3 & 0 \\\\\n",
    "0 & 5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The other eleven zeroes are responsible for the nullspaces.\n",
    "\n",
    "### Combining Bases from Subspaces\n",
    "\n",
    "With bases for the rowspace and nullspace, we have $r + (n -r) = n$ vectors.  Those $n$ vectors are independent.  Therefore they span $R^n$.\n",
    "\n",
    "Each $x$ is the sum $x_r$ + $x_n$ of a rowspace vector $x_r$ and a nullspace vector $x_n$.  For example:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}1 & 2 \\\\ 3 & 6\\end{bmatrix} \\text{ split }x = \\begin{bmatrix}4 \\\\ 3\\end{bmatrix} \\text{ into } x_r + x_n = \\begin{bmatrix}2 \\\\ 4\\end{bmatrix} + \\begin{bmatrix}2 \\\\ -1\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The next section will compute this splitting for any $A$ and $x$, by a projection.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44d6834d",
   "metadata": {},
   "source": [
    "## Projections (4.2)\n",
    "[Lecture here](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/lecture-15-projections-onto-subspaces-1/)\n",
    "\n",
    "Projection matrices are symmetric matrices with $P^2 = P$. The **projection** of $b$ is $Pb$.\n",
    "\n",
    "When a vector $b$ is projected onto a line, it's projection $p$ is the part of $b$ on that line.  When $b$ is projected onto a plane, $p$ is the part in that plane.\n",
    "\n",
    "The projection $p$ is $Pb$, where $P$ is the projection matrix that multiplies $b$ to give $p$.\n",
    "\n",
    "Suppose we have $b = (2,3,4)$. If we wanted to project it onto the $z$ axis, we'd have projection $(0,0,4)$.  If we projected it onto the xy plane, we'd have projection $(2,3,0)$.  Those are the parts of $b$ along the z axis and the xy plane.\n",
    "\n",
    "The projection matrices $P_1$ and $P_2$ are 3 by 3. They multiply $b$ with 3 components to produce $p$ with 3 components. Projection onto a line comes from a rank one matrix.  Projection onto a plane comes from a rank 2 matrix.\n",
    "\n",
    "$$\n",
    "P_1 = \n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "P_2 = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The line and the plane we're projecting onto are orthogonal complements.  Every vector $b$ in the whole space is the sum of its parts in the two subspaces.  The projections $p_1$ and $p_2$ are exactly those two parts of $b$.\n",
    "\n",
    "The vectors give $p_1 + p_2 = b$.  The matrices give $P_1 + P_2 = I$.\n",
    "\n",
    "We can project any vector $b$ onto the column space of of any matrix, which will be our focus.\n",
    "\n",
    "### Projection onto a Line\n",
    "\n",
    "A line goes through the origin in the direction of $a = (a_1, \\dots, a_m)$.  Along that line, we want the point $p$ closest to $b = (b_1, \\dots, b_m)$.  The key to projection is orthogonality.  The line from $b$ to $p$ is perpindicular to the vector $a$.  This is the dotted line marked $e = b - p$ on the left side of the figure below:\n",
    "\n",
    "![image.png](images/projection-error.png)\n",
    "\n",
    "The projection $p$ will be some multiple of $a$.  Call it $p = \\hat{x}a$.  Computing this number $\\hat{x}$ will give the vector $p$.  Then, from formula for $p$, we will read off the projection matrix $P$.  These three steps will lead to all projection matrices:\n",
    "\n",
    "1. Find $\\hat{x}$\n",
    "2. Find the vector $p$\n",
    "3. Find the matrix $P$\n",
    "\n",
    "The dotted line $b - p$ is the **error** $e = b - \\hat{x}a$.  It is perpindicular to $a$-- this will determine $\\hat{x}$.  Use the fact that $b - \\hat{x}a$ is perpindicular to $a$ when their dot product is zero:\n",
    "\n",
    "$$\n",
    "a \\cdot (b - \\hat{x}a) = 0 \\text{ or } a \\cdot b - \\hat{x}a \\cdot a = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{x} = \\frac{a \\cdot b}{a \\cdot a} = \\frac{a^Tb}{a^Ta}\n",
    "$$\n",
    "\n",
    "The multiplication $a^Tb$ is the same as $a \\cdot b$.  Using the transpose is better, because it also applies to matrices. Our formula $\\hat{x} = \\frac{a^Tb}{a^Ta}$ gives the projection $p = \\hat{x}a$.\n",
    "\n",
    "So the projection of $b$ onto the line through $a$ is the vector $p = \\hat{x}a =\\frac{a^Tb}{a^Ta}a$\n",
    "\n",
    "Special cases pop out from this: \n",
    "\n",
    "1. If $b = a$, then $\\hat{x} = 1$.  The projection of $a$ onto itself is itself. $Pa =a$.\n",
    "2. If $b$ is perpindicular to $a$, then $a^Tb = 0$. The projection is $p = 0$.\n",
    "\n",
    "$p$ has length $||p|| = ||b||\\cos{\\theta}$. $e$ has length $||e|| = b\\sin{\\theta}$.\n",
    "\n",
    "The formula for the projection matrix $P$ follows from $p = \\hat{x}a =\\frac{a^Tb}{a^Ta}a$ and $p=Pb$:\n",
    "\n",
    "$$P = \\frac{aa^T}{a^Ta}$$\n",
    "\n",
    "The line we project into is in the column space of $P$.\n",
    "\n",
    "Since $P^2 = P$, projecting a second time changes nothing.\n",
    "\n",
    "The matrix $I - P$ should be a projection too. It produces $e$, the perpindicular part of $b$. Note that $(I - P)b = b - p = e$.  When $P$ projects onto one subspace, $I-P$ projects onto the perpindicular subspace.\n",
    "\n",
    "### Projection onto a subspace\n",
    "\n",
    "Start with vectors $a_1, \\dots, a_n$ in $R^m$.  Assume that these $a$'s are linearly independent. \n",
    "\n",
    "Our problem is to find the combination $p = \\hat{x_1}a_1 + \\cdots + \\hat{x_n}a_n$ closest to a given vector $b$. We are projecting each $b$ in $R^m$ onto the subspace spanned by the $a$'s.\n",
    "\n",
    "With $n=1$ (one vector $a_1$) this is projection onto a line.  The line is the column space of $A$, which has just one column.  In general the matrix $A$ has $n$ columns, $a_1, \\dots, a_n$.\n",
    "\n",
    "The combinations in $R^m$ are the vectors $Ax$ in the column space.  We are looking for the particular combination $p = A\\hat{x}$ (the projection) that is closest to $b$.  The hat over $\\hat{x}$ indicates the best choice $\\hat{x}$, to give the closest vector in the column space. When $n=1$, was saw that choice was $\\hat{x} = a^Tb/a^Ta.  For $n>1$, the best $\\hat{x} = (\\hat{x1}, \\dots\\, \\hat{x_n})$ is to be found now.\n",
    "\n",
    "We compute projections onto n-dimensional subspaces in the same three steps as before: find the vector $\\hat{x}$, find the projection $p=A\\hat{x}$, then find the projection matrix $P$.\n",
    "\n",
    "The key to solving this is in the fact that the error vector $e = b - A\\hat{x}$ is perpindicular to the subspace we're projecting upon.  The error $b - A\\hat{x}$ makes a right angle with the vectors $a_1, \\dots, a_n$. The $n$ right angles give the $n$ equations for $\\hat{x}$\n",
    "\n",
    "\n",
    "![image.png](images/perp-subspace.png)\n",
    "\n",
    "The matrix with those rows $a_i^T$ is $a^T$.  The $n$ equations are exactly $A^T(b - A\\hat{x}) = 0$.\n",
    "\n",
    "We rewrite $A^T(b - A\\hat{x}) = 0$ into its famous form $A^TA\\hat{x} = A^Tb$.  This is the equation for $\\hat{x}$, and the coefficient matrix is $A^TA$.  Now we can find $\\hat{x}$ and $p$ and $P$, in that order..\n",
    "\n",
    "The solution for $\\hat{x} = (A^TA)^{-1}A^Tb$.  The solution for $p = A\\hat{x}$.  The solution for $P$ comes from $P = A(A^TA)^{-1}A^T$, which is a modification of the equation $p = Pb$.\n",
    "\n",
    "### Invertibility of $A^TA$\n",
    "\n",
    "We will prove that $A^TA$ is invertible only if $A$ has linearly independent columns.\n",
    "\n",
    "We have to show that for every matrix $A$, $A^TA$ has the same nullspace as $A$.  When the columns of $A$ are linearly independent, its nullspace contains only the zero vector.  Then $A^TA$, with the same nullspace, is invertible.\n",
    "\n",
    "Let $A$ be any matrix.  If $x$ is in its nullspace, then $Ax=0$.  Multiplying by $A^T$ gives $A^TAx = 0$. So this $x$ is also in the nullspace of $A^TA$.\n",
    "\n",
    "Now we start with the nullspace of $A^TA$.  From $Ax=0$ we must prove that $Ax=0$.  We multiply by $x^T$:\n",
    "\n",
    "$$(x^T)A^TAx = 0 \\text{ or } (Ax)^T(Ax) = 0 \\text{ or } ||Ax^2|| =0$$\n",
    "\n",
    "We have shown that if $A^TAx=0$ then $Ax$ has length 0.  Therefore $Ax=0$.  Meaning every vector $x$ in one nullspace is in the other nullspace. If $A^TA$ has dependent columns, so has $A$.  If $A^TA$ has independent columns, so has $A$.\n",
    "\n",
    "When $A$ has independent columns, $A^TA$ is square, symmetric, and invertible.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77d1f890",
   "metadata": {},
   "source": [
    "## Least Squares Approximations (4.3)\n",
    "\n",
    "When the length of $e$ (the error) is as small as possible, $\\hat{x}$ is a **least squares solution**.\n",
    "\n",
    "To fit points $(t_1, b_1), \\dots (t_m, b_m)$ by a straight line, we project onto $A$ with columns $(1,\\dots,1)$ and $t_1,\\dots,t_m$.  When there's no solution to this $Ax=b$, the $\\hat{x}$ provides the best fitting line.\n",
    "\n",
    "### Minimizing the error\n",
    "\n",
    "The best $x$ can be found by geometry, algebra, or calculus.  In geometry, the error space of $e$ meets the column space at $90\\degree$. In algebra, $A^TA\\hat{x} = A^Tb$.  And in Calculus, the derivative of the error $||Ax -b||$ is zero at $\\hat{x}$.\n",
    "\n",
    "The squared length for any $x$: $||Ax - b||^2 = ||Ax - p||^2 + ||e||^2$.  We reduce $Ax - p$ to zero by choosing $x = \\hat{x}$.  This leaves the smallest possible error which we can't reduce.  Notice what \"smallest\" means.  The squared length of $Ax - b$ is minimized.  The least squares solution $\\hat{x}$ makes $E = ||Ax - b||^2$ as small as possible.\n",
    "\n",
    "The errors will add to zero, because they're perpindicular to the 1's in the first column of $A$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519e86c5",
   "metadata": {},
   "source": [
    "\n",
    "## Orthonormal Bases and Gram-Schmidt \n",
    "# Determinants \n",
    "## The Properties of Determinants \n",
    "## Permutations and Cofactors \n",
    "## Cramerâ€™s Rule, Inverses, and Volumes \n",
    "# Eigenvalues and Eigenvectors \n",
    "## Introduction to Eigenvalues \n",
    "## Diagonalizing a Matrix \n",
    "## Systems of Differential Equations \n",
    "## Symmetric Matrices \n",
    "## Positive Definite Matrices "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fb58bdd",
   "metadata": {},
   "source": [
    "# Applications (10)\n",
    "## Graphs and Networks (10.1)\n",
    "\n",
    "[Lecture here](https://youtu.be/2IdtqGM6KWU)\n",
    "\n",
    "A **graph** consists of **nodes** connected by **edges**.\n",
    "\n",
    "The **incidence matrix** of a graph tells us how $n$ nodes are connected by $m$ edges.\n",
    "\n",
    "By focusing on incidence matrices, the laws of linear algebra become [Kirchoff's laws](https://en.wikipedia.org/wiki/Kirchhoff%27s_circuit_laws).\n",
    "\n",
    "Each entry of an incidence matrix is 0 or 1 or -1.  This continues to apply after elimination.  All four subspaces in fact will use these simple components.\n",
    "\n",
    "### The Incidence Matrix\n",
    "\n",
    "If node one has an arrow to node two, the corresponding row for that edge will have a -1 in column 1 and a 1 in column 2.  The negative number means the arrow is going out from node 1, and the positive number means it's going into node 2.\n",
    "\n",
    "![image.png](images/incidence-matrix-graph.png)\n",
    "\n",
    "This graph is **complete**-- every pair of nodes is connected by an edge.\n",
    "\n",
    "A graph with no closed loops is called a **tree**.\n",
    "\n",
    "The maximum number of edges is $\\frac{1}{2}n(n-1)$ and the minimum to connect all nodes in some way is $n-1$.\n",
    "\n",
    "Elimination reduces every graph to a tree.  Loops produce dependent rows in $A$ and zero rows in echelon forms $U$ and $R$.\n",
    "\n",
    "When $x$ is a vector of voltages at the nodes, $Ax$ gives the voltage differences.\n",
    "\n",
    "**Kirchoff's Voltage Law**: The components of $Ax=b$ add to zero around every loop.\n",
    "\n",
    "**Kirchoff's Current Law**: $A^Ty = 0$.  Flow in equals flow out at each node.\n",
    "\n",
    "The incidence matrix $A$ comes from a connected graph with $n$ nodes and $m$ edges. The row space and column space have dimensions $r = n - 1$.  The nullspaces of $A$ and $A^T$ have dimensions 1 and $m-n + 1$.\n",
    "\n",
    "* $N(A)$ - The constant vectors $(c, c, \\dots, c)$ make up the nullspace of A.  dim = 1.\n",
    "* $C(A^T)$ - The edges of any tree give $r$ dependent rows of $A$: $r = n - 1$.\n",
    "* $C(A)$ - Voltage Law: The components of $Ax$ add to zero around all loops: dim = $n - 1$.\n",
    "* $N(A^T)$ - Current Law: $A^Ty = \\text{flow in - flow out} = 0$ is solved by loop currents.  There are $m - r = m - n + 1$ independent small loops in the graph.\n",
    "\n",
    "For every graph in a plane, linear algebra yields **Euler's formula**:\n",
    "\n",
    "$$\\text{(number of nodes) - (number of edges) + (number of small loops)} = 1$$\n",
    "\n",
    "This is saying $(n) - (m) + (m - n + 1) = 1$\n",
    "\n",
    "When we have a current source, Kirchoff's Current Law changes from $A^Ty = 0$ to $A^Ty = f$, to balanace the source $f$ from outside.  Flow into each node still equals flow out.  \n",
    "\n",
    "### Voltages and Currents and $A^TAx = f$\n",
    "\n",
    "[this is rather complex, will see if he covers it in lecture]\n",
    "\n",
    "### Networks and $A^TCA$\n",
    "\n",
    "**Conductance** is the inverse of **resistance**, and measures how easily flow gets through.\n",
    "\n",
    "A **network** is graph that has conductance at each edge.  These numbers go into the conductance matrix $C$, which is diagonal.\n",
    "\n",
    "**Ohm's Law**: $\\text{Current along edge = conductance times voltage difference.}$\n",
    "\n",
    "Ohm's Law for all $m$ currents is $y = -CAx$.  The vector $Ax$ gives the potential differences, and $C$ multiplies by the conductances.\n",
    "\n",
    "Combining Ohm's Law with Kirchoff's Current Law we get $A^TCAx = 0$.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

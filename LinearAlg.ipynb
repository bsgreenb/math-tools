{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72acbd94-93c6-4dd3-8984-78ff1547c5ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Sources\n",
    "\n",
    "- [Gilbert Strang’s Class - MIT Linear Algebra Fall 2011](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/resource-index/)\n",
    " - Uses Introduction to Linear Algebra, 5th Edition\n",
    "- [3 blue 1 brown vids - Essence of Linear Algebra](https://www.youtube.com/watch?v=LyGKycYT2v0&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=10)\n",
    "- May test myself on [Khan](https://www.khanacademy.org/math/linear-algebra) but not planning to use his videos between the textbook and the above vids\n",
    "- Going to distribute [Numpy](https://numpy.org/doc/stable/user/absolute_beginners.html) stuff as I go.  Taking notes separately on that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4c47c69",
   "metadata": {},
   "source": [
    "# Introduction to Vectors (1)\n",
    "Lectures:\n",
    "* [The Geometry of Linear Equations](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/the-geometry-of-linear-equations-1/) - Note, this also covers (2.1)\n",
    "* [An Overview of Linear Algebra](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/an-overview-of-linear-algebra-1/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "672b86a6",
   "metadata": {},
   "source": [
    "## Linear combinations (1.1)\n",
    "\n",
    "$cv + dw$ for linear combinations of vectors $v$ and $w$, where $c$ and $d$ are scalars."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d76a9a2a",
   "metadata": {},
   "source": [
    "\n",
    "## Lengths and Dot Products (1.2)\n",
    "The dot product of vectors $v = \\begin{bmatrix}1 \\\\ 2\\end{bmatrix}$ and $w = \\begin{bmatrix}4 \\\\ 5\\end{bmatrix}$ is $v \\cdot w = (1)(4) + (2)(5) = 4 + 10 = 14$.\n",
    "\n",
    "Some algebraic properties of the dot product:\n",
    "1. Commutative Property: For any two vectors $u$ and $v$, $u \\cdot $v = v \\cdot u$.\n",
    "2. Scalar Multiplication Property: For any two vectors $u$ and $v$ and any real number $c$, $(cu) \\cdot v = u \\cdot (cv) = c(u \\cdot v)$\n",
    "3. Distributive Property: For any 3 vectors $u$, $v$, and $w$, $u \\cdot (v+w) = u \\cdot v + u \\cdot w$.\n",
    "\n",
    "When you multiply two vectors and the dot product is zero, they are perpindicular.  More generally, the angle $\\theta$ between vectors $v$ and $w$ has:\n",
    "$$\n",
    "\\cos \\theta = \\frac{v \\cdot w}{||v||\\;||w||}\n",
    "$$\n",
    "\n",
    "The length $||v||$ of a vector is $\\sqrt{v \\cdot v}$. This follows from the pythagorean theorem.\n",
    "\n",
    "The **unit vector** is a vector with length 1. Divide any vector by its length to get a unit vector."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "132623cc",
   "metadata": {},
   "source": [
    "\n",
    "### Explanation of Angle Between Two Vectors\n",
    "\n",
    "The unit vector that makes an angle $\\theta$ with the x axis is $\\begin{bmatrix}\\cos \\theta \\\\ \\sin \\theta\\end{bmatrix}$, we can see this from the unit circle\n",
    "\n",
    "![image.png](images/unit-circle.png)\n",
    "\n",
    "Let's get a geometric understanding for the rule\n",
    "$$\n",
    "\\cos \\theta = \\frac{v \\cdot w}{||v||\\;||w||}\n",
    "$$\n",
    "\n",
    "Now suppose instead of forming $\\theta$ with the x axis, we have two unit vectors, $U$ and $u$, and they are both rotated from the x axis:\n",
    "\n",
    "![image.png](images/unit-vector-addition.png)\n",
    "\n",
    "$u \\cdot U$ would then be $\\cos{\\alpha}\\cos{\\beta} + \\sin{\\alpha}\\sin{\\beta}$. From the cosine angle addition rule in trignometry, this is equal to $\\cos(\\theta)$.\n",
    "\n",
    "So we have arrived at the preliminary rule that unit vectors $u$ and $U$ at angle $\\theta$ have:\n",
    "\n",
    "$$u \\cdot U = \\cos{\\theta}$$\n",
    "\n",
    "Combine this with our observation before that you can divide any vector by its length to get its unit vector, and we arrive at our **cosine formula** for any vectors $v$ and $w$ by just dividing their lengths:\n",
    "\n",
    "$$\n",
    "\\cos \\theta = \\frac{v \\cdot w}{||v||\\;||w||}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4867f5b",
   "metadata": {},
   "source": [
    "\n",
    "### Schwarz and Triangle Inequalities\n",
    "\n",
    "Because all cosines are between -1 and 1, it follows that the absolute value of the dot product, $|v \\cdot w|$, cannot exceed the product of the lengths, this is the **Schwarz Inequality**:\n",
    "\n",
    "$$|v \\cdot w| \\le ||v||\\: ||w||$$\n",
    "\n",
    "From the Schwarz Inequality [follows](https://math.stackexchange.com/a/91194) the **Triangle Inequality**:\n",
    "\n",
    "$$||u + v|| \\le ||u|| + ||v||$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a034db",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Independence and Dependence\n",
    "\n",
    "Vectors are **independent** if no combination other than 0 multiples gives $b=0$.  Vectors are **dependent** if multiple combinations give $b=0$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2be52981",
   "metadata": {},
   "source": [
    "## Matrices (1.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d2bb8b4",
   "metadata": {},
   "source": [
    "A matrix is **invertible** (aka **non-singular**) if it has independent (see definition above) column vectors, meaning $Ax = 0$ has only one solution between them.\n",
    "\n",
    "A matrix is **singular** if $Ax=0$ has many solutions, or none at all."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56e07f00",
   "metadata": {},
   "source": [
    "\n",
    "# Solving Linear Equations (2)\n",
    "\n",
    "* [Elemination with Matrices](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/elimination-with-matrices-1/) - Lecture covering 2.2 and 2.3\n",
    "## Vectors and Linear Equations (2.1)\n",
    "\n",
    "Geometrically, it's worth noting that the dot product of each row with $x$ gives the equation of a plane.\n",
    "When the number of unknowns matches the number of equations, there is _usually_ one solution.\n",
    "\n",
    "### Matrix, Row, and Column Pictures\n",
    "\n",
    "Lets say we have $n$ equations and $n$ unknowns, and go over:\n",
    "* Matrix Form\n",
    "* Row Picture\n",
    "* Column Picture\n",
    "\n",
    "Let's look specifically at these two equations with two unknowns:\n",
    "$$\n",
    "2x - y = 0 \\\\\n",
    "-x + 2y = 3\n",
    "$$\n",
    "\n",
    "In **matrix form**, with the **coefficient matrix**, followed by the unknowns matrix, equal to solutions/right hand side would be:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2 & -1 \\\\\n",
    "-1 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These three matrices are abstractly referred to as $Ax=b$. When we are solving for $x$ (the inverse), we are abstractly solving $x = A^{-1}b$. And note that only with an invertible matrix (see below) can we solve this.\n",
    "\n",
    "The **row picture** is looking at one equation at a time, it's what we've seen before with systems of equations, or looking for where lines meet when we graph them geometrically.\n",
    "\n",
    "The **column picture** would have us formulate the equations as combinations of the columns, so:\n",
    "\n",
    "$$\n",
    "x \n",
    "\\begin{bmatrix}\n",
    "2 \\\\\n",
    "-1\n",
    "\\end{bmatrix}\n",
    "+ \n",
    "y\n",
    "\\begin{bmatrix}\n",
    "-1 \\\\\n",
    "2\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Geometrically, the column picture can solve these linear equations through vector addition, which we know geometrically means combining the column vectors each a certain number of times to produce the right hand side.\n",
    "\n",
    "### The Identity Matrix\n",
    "\n",
    "Multiplying $Ix$ where $I$ is the identity matrix, you get back the x you started with, $Ix=x$.  An example 3x3 identity matrix:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63361923",
   "metadata": {},
   "source": [
    "## The Idea of Elimination (2.2)\n",
    "\n",
    "**Elimination** is the systematic way of solving linear equations. Elimination proceeds by producing an **upper triangular system** from top to bottom, then solving with **back substitution** from the bottom up.\n",
    "\n",
    "In the first part, where you're producing the upper triangular system, you subtract a multiple of the above equation from the equation below.  This **multiplier** ($l$) is determined from the **pivot** above.  For example, if we have\n",
    "\n",
    "$$\n",
    "4x - 8y = 4 \\\\\n",
    "3x + 2y = 11\n",
    "$$\n",
    "\n",
    "Our multiplier of the first equation would be $l=\\frac{3}{4}$, and we'd then subtract that multiplied equation from the 2nd.  We'd then be left with the pivot of $8$ at the bottom right.  To solve $n$ equations we want $n$ pivots.  If there were a 3rd equation we'd use the $8$ pivot to determine our next multiplier and subtract, and so on.\n",
    "\n",
    "### The breakdown of elimination\n",
    "\n",
    "It's possible for the process of elimination to fail along the way.  Specifically, we might reach a 0 pivot.  In this case, we may be able to rescue this with row exchange, or may not be able to.  It may be that the 0 pivot:\n",
    "\n",
    "* Implies no solutions (e.g. $0y=8$).  Geometrically this would be non-intersecting lines. OR \n",
    "* It may be that it arrives at infinite solutions (e.g. $0y=0$). Geometrically this would be represented by more than one intersection, e.g. two identical lines.\n",
    "* It may be that a row exchange can rescue things, for example:\n",
    "\n",
    "$$\n",
    "0x + 2y = 4 \\\\\n",
    "3x - 2y = 5\n",
    "$$\n",
    "\n",
    "Here would just want to perform **row substitution** to get a triangular system we could then back-substitute on.\n",
    "\n",
    "Recall our terminology from earlier on, when we can complete elimination, we are dealing with a non-singular matrix, whereas the no solutions or infinite solutions cases are singular.\n",
    "\n",
    "### Extending into 3+ equations\n",
    "\n",
    "The process involves clear out columns below the pivots, using multipliers of that pivot, before moving onto the next pivot.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e25d296b",
   "metadata": {},
   "source": [
    "## Elimination Using Matrices (2.3)\n",
    "\n",
    "**Elimination matrices** execute our elimination steps.  An elimination matrix $E_{ij}$ eliminates row $i$, column $j$ by multiplying the $j$th equation by $l_{ij}$ and subtracting it from the $i$th equation.  So for example $E_{21}$ would would be the first elimination step, clearing out row 2, column 1.\n",
    "\n",
    "We need a lot of these $E_{ij}$ matrices to complete elimination, which is why we'll later see they can be combined into one big matrix $E$.  The neatest way to do that is by combining all their inverses $(E_{ij})^{-1}$ into one overall matrix $L = E^{-1}$.  \n",
    "\n",
    "The special property of $L$ is that all the multipliers $l_{ij}$ fall into place.  Those numbers are mixed up in $E$ (forward elimination from A to U).  Inverting puts the steps and their elimination matrices in the opposite order and prevents the mixup.\n",
    "\n",
    "### The Matrix Form of One Elimination Step\n",
    "\n",
    "Suppose we want to subtract two times row 1 from row 2.  The elimination matrix for this step would be:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "-2 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The first and third rows come from the identity matrix $I$. The $-2$ comes from the negative of the multiplier $l$ (2).\n",
    "\n",
    "### Matrix Multiplication\n",
    "\n",
    "Via [MathIsFun](https://www.mathsisfun.com/algebra/matrix-multiplying.html):\n",
    "\n",
    "![image.svg](images/matrix-multiply.svg)\n",
    "\n",
    "It works through the dot product of each row and column.\n",
    "\n",
    "In order to multiply two matrices, the number of columns of A must equal the number of rows of B. The product\n",
    "AB will have the same number of rows as the first matrix and the same number of columns as the second.\n",
    "\n",
    "Algebraic rules for matrix multiplication:\n",
    "* Associative Law is true: $A(BC) = (AB)C$\n",
    "* Commutative Law is false: Often $AB \\ne BA$\n",
    "\n",
    "A note on matrix multiplication order.  When we multiply on the left side vs right side, it's the difference between acting on rows vs columns, which switches based on order.  Multiplying from the left, we're doing row operations.  Multiplying from the right, we're doing column operations.\n",
    "\n",
    "3Blue1Brown emphasized:\n",
    "- Viewing Matrices as transformation of space\n",
    "- Matrix multiplication is just one transformation after another [this may belong in subsequent section]\n",
    "\n",
    "### The Row Exchange Matrix\n",
    "To exchange aka permute rows we use another matrix $P_{ij}$ called the **permutation matrix**.  For example, the permutation matrix $P_{23}$ exchanges rows 2 and 3:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Permutation matrices can swap multiple rows as well, not just one.  We'll see that soon.\n",
    "\n",
    "### The Augmented Matrix\n",
    "\n",
    "We can augment the matrix $A$ in $Ax=b$ to include $b$ as an extra column, and allow it to change through the process of elimination.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62a3c076",
   "metadata": {},
   "source": [
    "## Rules for Matrix Operations (2.4)\n",
    "\n",
    "- Lecture for 2.4 and 2.5: [Multiplication and Inverse Matrices](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/multiplication-and-inverse-matrices/)\n",
    "\n",
    "A matrix with $n$ columns can multiply a matrix with $n$ rows:\n",
    " \n",
    "$$A_{m \\times n}B_{n \\times p} = C_{m \\times p}$$\n",
    "\n",
    "\n",
    "### Multiple ways to multiply matrices\n",
    "\n",
    "1. We went over the typical dot product way of multiplying matrices above, where the entry in row $i$, and column $j$ of $AB$ is (row $i$ of $A$) $\\cdot$ (column $j$ of $B$).\n",
    "\n",
    "Terminology note: A row times a column (a dot product) is also called an **inner product**.  A column times a row is called an **outer product**.\n",
    "\n",
    "Now let's talk about additional ways to multiply matrices..\n",
    "\n",
    "2. Matrix $A$ times every column of $B$: $A\\begin{bmatrix}b_1 \\cdots b_p \\end{bmatrix} = A\\begin{bmatrix}Ab_1 \\cdots Ab_p \\end{bmatrix}$.  Recall from the column picture perspective, that we can therefore see each column of $AB$ as a combination of columns of $A$.\n",
    "\n",
    "3. Every row of matrix $A$ times matrix $B$: \n",
    "$\\begin{bmatrix} \\text{row }i\\text{ of }A\\end{bmatrix}B = \\begin{bmatrix}\\text{row }i \\text{ of }AB\\end{bmatrix}.$\n",
    "\n",
    "4. Multiply columns $1$ to $n$ of $A$ times rows $1$ to $n$ of $B$. Add those matrices. So for example:\n",
    "$$\n",
    "AB = \\begin{bmatrix}a \\\\ c\\end{bmatrix}\\begin{bmatrix}E & F\\end{bmatrix} + \\begin{bmatrix}b \\\\ d\\end{bmatrix}\\begin{bmatrix}G & H\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "You'll find that it works out just like the other methods.\n",
    "\n",
    "### Blocks\n",
    "\n",
    "Matrices can be added and multiplied by **blocks**, so long as the block sizes correspond to the normal rules-- same size for addition, and rows of 1 = cols of 2 for multiplication. \n",
    "\n",
    "Important: Cuts between columns of $A$ must match cuts between rows of $B$.\n",
    "\n",
    "Matrix block multiplication example:\n",
    "\n",
    "$A = \\begin{bmatrix}A_1 & A_2\\end{bmatrix}$ times $B = \\begin{bmatrix}B_1 \\\\ B_2\\end{bmatrix}$ is $A_{1}B_1 + A_{2}B_2$.\n",
    "\n",
    "The blocks must be equal across transposition, so for example you could have:\n",
    "* Two square matrices split up with each corner a block\n",
    "* Block columns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbcd6dc0",
   "metadata": {},
   "source": [
    "## Inverse Matrices (2.5)\n",
    "\n",
    "If the square matrix $A$ has an inverse, then both $A^{-1}A = I$ and $A^{-1}A = I$.  Note that non-square matrices are not invertible.\n",
    "\n",
    "Testing for invertibility:\n",
    "\n",
    "- The _algorithm_ to test invertibility is elimination. $A$ must have $n$ (nonzero) pivots\n",
    "- The _algebra_ test for invertibility is the determinant of $A$. $\\det A$ must not be $0$.\n",
    "- The _equation_ that test for invertibility is $Ax = 0$.  $x = 0$ must be the only solution.\n",
    "\n",
    "A matrix cannot have more than one inverse.  If you found the left-inverse, it must be the same as the right-inverse.\n",
    "\n",
    "### The Inverse of a Product AB\n",
    "\n",
    "If $A$ and $B$ are invertible, then so is $AB$:\n",
    "\n",
    "$$(AB)^{-1} = B^{-1}A^{-1}$$\n",
    "\n",
    "### Gauss-Jordan Elimination\n",
    "Gauss-Jordan eliminates $\\begin{bmatrix}A & I\\end{bmatrix}$ to $\\begin{bmatrix}I & A^{-1}\\end{bmatrix}$.\n",
    "\n",
    "The Gauss-Jordan method is to begin with that augmented matrix, $\\begin{bmatrix}A & I\\end{bmatrix}$, and performing elimination until you get the left block upper triangular.  Then, continue doing elimination upwards, so that you have only a diagonal of pivots on the left.  Finally, divide each row to get **reduced echelon form** ($R=I$) on the left hand side.  Then your inverse will be on the right hand side.\n",
    "\n",
    "This helps explain why the determininant can't be 0 for a matrix with an inverse, you have to divde by the pivots, and you can't divide by 0.\n",
    "\n",
    "**Diagonally dominant** matrices are invertible.  If the absolute value of the diagonal entries are larger than the sum of the absolute values of the rest of their rows, then the matrix is invertible.  This follows from the fact that the other row entires cannot add up to equal those entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb2997",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Elimination = Factorization: A = LU\n",
    "## Transposes and Permutations\n",
    "# Vector Spaces and Subspaces\n",
    "## Spaces of Vectors\n",
    "## The Nullspace of A: Solving Ax = 0 and Rx = 0 \n",
    "## The Complete Solution to Ax = b\n",
    "## Independence, Basis and Dimension\n",
    "## Dimensions of the Four Subspaces\n",
    "# Orthogonality \n",
    "## Orthogonality of the Four Subspaces \n",
    "## Projections \n",
    "## Least Squares Approximations \n",
    "## Orthonormal Bases and Gram-Schmidt \n",
    "# Determinants 247\n",
    "## The Properties of Determinants \n",
    "## Permutations and Cofactors \n",
    "## Cramer’s Rule, Inverses, and Volumes \n",
    "# Eigenvalues and Eigenvectors \n",
    "## Introduction to Eigenvalues \n",
    "## Diagonalizing a Matrix \n",
    "## Systems of Differential Equations \n",
    "## Symmetric Matrices \n",
    "## Positive Definite Matrices "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb58bdd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

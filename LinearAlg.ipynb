{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72acbd94-93c6-4dd3-8984-78ff1547c5ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Sources\n",
    "\n",
    "- [Gilbert Strang’s Class - MIT Linear Algebra Fall 2011](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/resource-index/)\n",
    " - Uses Introduction to Linear Algebra, 5th Edition\n",
    "- [3 blue 1 brown vids - Essence of Linear Algebra](https://www.youtube.com/watch?v=LyGKycYT2v0&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=10)\n",
    "- May test myself on [Khan](https://www.khanacademy.org/math/linear-algebra) but not planning to use his videos between the textbook and the above vids\n",
    "- Going to distribute [Numpy](https://numpy.org/doc/stable/user/absolute_beginners.html) stuff as I go.  Taking notes separately on that.\n",
    "\n",
    "# Introduction to Vectors\n",
    "\n",
    "[Take notes on the different Forms being equivalent that he discusses in his lecture https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/resource-index/#?w=535]\n",
    "\n",
    "\n",
    "## Vectors and Linear Combinations\n",
    "\n",
    "\n",
    "\n",
    "## Lengths and Dot Products\n",
    "Dot product relates to how you can split matrix into combos. Strang said you can think of matrix multiplication as combination of columns\n",
    "\n",
    "### Matrices\n",
    "\n",
    "3Blue1Brown emphasized:\n",
    "- Viewing Matrices as transformation of space\n",
    "- Matrix multiplication is just one transformation affer another [this may belong in subsequent section]\n",
    "\n",
    "# Solving Linear Equations\n",
    "## Vectors and Linear Equations\n",
    "## The Idea of Elimination \n",
    "## Elimination Using Matrices \n",
    "## Rules for Matrix Operations\n",
    "## Inverse Matrices \n",
    "## Elimination = Factorization: A = LU\n",
    "## Transposes and Permutations\n",
    "# Vector Spaces and Subspaces\n",
    "## Spaces of Vectors\n",
    "## The Nullspace of A: Solving Ax = 0 and Rx = 0 \n",
    "## The Complete Solution to Ax = b\n",
    "## Independence, Basis and Dimension\n",
    "## Dimensions of the Four Subspaces\n",
    "# Orthogonality \n",
    "## Orthogonality of the Four Subspaces \n",
    "## Projections \n",
    "## Least Squares Approximations \n",
    "## Orthonormal Bases and Gram-Schmidt \n",
    "# Determinants 247\n",
    "## The Properties of Determinants \n",
    "## Permutations and Cofactors \n",
    "## Cramer’s Rule, Inverses, and Volumes \n",
    "# Eigenvalues and Eigenvectors 288\n",
    "## Introduction to Eigenvalues \n",
    "## Diagonalizing a Matrix \n",
    "## Systems of Differential Equations \n",
    "## Symmetric Matrices \n",
    "## Positive Definite Matrices "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757eb249-dda3-4358-92c6-3fea39573519",
   "metadata": {},
   "source": [
    "# Vectors from Calc\n",
    "\n",
    "[MAKES SENSE to integrate this in line with below sections from Linear Alg]\n",
    "\n",
    "Using [Khan](https://www.khanacademy.org/math/get-ready-for-ap-calc/xa350bf684c056c5c:get-ready-for-parametric-polar-vector) to get the basics here just as part of doing AP Calc.  But will expand this once taking on Linear Algebra.\n",
    "\n",
    "A **Vector** has a **magnitude/size** and a **direction/angle**.  A **Scalar** only has the former.\n",
    "\n",
    "A vector is not defined by it's starting point, just its magnitude and direction.  It could be dropped anywhere on a graph.\n",
    "\n",
    "Vectors can be split up into Parametric Equations.  [0 upvote MSE Question](https://math.stackexchange.com/a/1940372/49487) which hopefully is correct:\n",
    "\n",
    "> parametric equation with only one parameter, it's the same thing. The parametric equations are the components of the vector function.\n",
    "\n",
    "[Euclidean Vectors](https://en.wikipedia.org/wiki/Euclidean_vector) are all I dealt with in Calc.  When taking on Linear, will move beyond these. [Position Vectors](https://en.wikipedia.org/wiki/Position_(geometry)) are Euclidean Vectors that start at the origin at point to something in Euclidean space.\n",
    "\n",
    "[Vector Space](https://en.wikipedia.org/wiki/Vector_space)\n",
    "[Write formal definition of vector (Spaces) as part of linear alg]\n",
    "\n",
    "## Norm / Magnitude\n",
    "\n",
    "Vector magnitude from components:\n",
    "The magnitude of $(a, b)$ is $\\|(a, b)\\|=\\sqrt{a^{2}+b^{2}}$.\n",
    "\n",
    "## Direction of Vectors\n",
    "\n",
    "Arctan has a limited range, so if you want to get an angle from the vector components. In Quadrants II and III, you'll wanna add 180.  In Quadrant IV, you'll get a negative angle which you can add 360 to to get a positive one.  \n",
    "\n",
    "If you're using [atan2](https://en.wikipedia.org/wiki/Atan2), it works a bit different than that.\n",
    "\n",
    "## Vector components from magnitude and direction\n",
    "\n",
    "The components of a vector with magnitude $\\|\\vec{u}\\|$ and direction $\\theta$ are $(\\|\\vec{u}\\| \\cos (\\theta),\\|\\vec{u}\\| \\sin (\\theta))$.\n",
    "\n",
    "## Unit Vectors\n",
    "[Unit vector](https://en.wikipedia.org/wiki/Unit_vector) is a vector of length 1.\n",
    "\n",
    "[Notation varies](https://math.stackexchange.com/questions/965477/unit-vector-symbols-names) for them, but Khan seems to prefer $\\hat i, \\hat j$.\n",
    "\n",
    "## Multiplying and Dividing Vectors\n",
    "\n",
    "**Scalar Multiplication** is scalar times a vector.  Pretty straightforward since it just multiplies the components.\n",
    "\n",
    "## Vector valued functions\n",
    "\n",
    "Typical notation for a **Vector Function** aka **Vector Valued Function** is either arrow over like $\\vec{v}$  (I prefer, cus I write it same with pencil) or else bold it.\n",
    "\n",
    "$\\vec{r}(t) = x(t)\\hat i + y(t)\\hat j$\n",
    "\n",
    "From this definition, and the limit definition of a derivative, we see that we can get the derivative of vector functions.\n",
    "\n",
    "### Vector Valued Function Differentiation\n",
    "\n",
    "The derivative of a vector is itself a vector.\n",
    "\n",
    "$\\vec{r}'(t) = x'(t)\\hat i + y'(t)\\hat j$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

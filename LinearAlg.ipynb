{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72acbd94-93c6-4dd3-8984-78ff1547c5ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Sources\n",
    "\n",
    "- [Gilbert Strang’s Class - MIT Linear Algebra Fall 2011](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/resource-index/)\n",
    " - Uses Introduction to Linear Algebra, 5th Edition\n",
    "- [3 blue 1 brown vids - Essence of Linear Algebra](https://www.youtube.com/watch?v=LyGKycYT2v0&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=10)\n",
    "- May test myself on [Khan](https://www.khanacademy.org/math/linear-algebra) but not planning to use his videos between the textbook and the above vids\n",
    "- Going to distribute [Numpy](https://numpy.org/doc/stable/user/absolute_beginners.html) stuff as I go.  Taking notes separately on that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4c47c69",
   "metadata": {},
   "source": [
    "# Introduction to Vectors (1)\n",
    "Lectures:\n",
    "* [The Geometry of Linear Equations](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/the-geometry-of-linear-equations-1/) - Note, this also covers (2.1)\n",
    "* [An Overview of Linear Algebra](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/resources/an-overview-of-linear-algebra-1/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "672b86a6",
   "metadata": {},
   "source": [
    "## Linear combinations (1.1)\n",
    "\n",
    "$cv + dw$ for linear combinations of vectors $v$ and $w$, where $c$ and $d$ are scalars."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d76a9a2a",
   "metadata": {},
   "source": [
    "\n",
    "## Lengths and Dot Products (1.2)\n",
    "The dot product of vectors $v = \\begin{bmatrix}1 \\\\ 2\\end{bmatrix}$ and $w = \\begin{bmatrix}4 \\\\ 5\\end{bmatrix}$ is $v \\cdot w = (1)(4) + (2)(5) = 4 + 10 = 14$.\n",
    "\n",
    "Some algebraic properties of the dot product:\n",
    "1. Commutative Property: For any two vectors $u$ and $v$, $u \\cdot $v = v \\cdot u$.\n",
    "2. Scalar Multiplication Property: For any two vectors $u$ and $v$ and any real number $c$, $(cu) \\cdot v = u \\cdot (cv) = c(u \\cdot v)$\n",
    "3. Distributive Property: For any 3 vectors $u$, $v$, and $w$, $u \\cdot (v+w) = u \\cdot v + u \\cdot w$.\n",
    "\n",
    "When you multiply two vectors and the dot product is zero, they are perpindicular.  More generally, the angle $\\theta$ between vectors $v$ and $w$ has:\n",
    "$$\n",
    "\\cos \\theta = \\frac{v \\cdot w}{||v||\\;||w||}\n",
    "$$\n",
    "\n",
    "The length $||v||$ of a vector is $\\sqrt{v \\cdot v}$. This follows from the pythagorean theorem.\n",
    "\n",
    "The **unit vector** is a vector with length 1. Divide any vector by its length to get a unit vector."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "132623cc",
   "metadata": {},
   "source": [
    "\n",
    "### Explanation of Angle Between Two Vectors\n",
    "\n",
    "The unit vector that makes an angle $\\theta$ with the x axis is $\\begin{bmatrix}\\cos \\theta \\\\ \\sin \\theta\\end{bmatrix}$, we can see this from the unit circle\n",
    "\n",
    "![image.png](images/unit-circle.png)\n",
    "\n",
    "Let's get a geometric understanding for the rule\n",
    "$$\n",
    "\\cos \\theta = \\frac{v \\cdot w}{||v||\\;||w||}\n",
    "$$\n",
    "\n",
    "Now suppose instead of forming $\\theta$ with the x axis, we have two unit vectors, $U$ and $u$, and they are both rotated from the x axis:\n",
    "\n",
    "![image.png](images/unit-vector-addition.png)\n",
    "\n",
    "$u \\cdot U$ would then be $\\cos{\\alpha}\\cos{\\beta} + \\sin{\\alpha}\\sin{\\beta}$. From the cosine angle addition rule in trignometry, this is equal to $\\cos(\\theta)$.\n",
    "\n",
    "So we have arrived at the preliminary rule that unit vectors $u$ and $U$ at angle $\\theta$ have:\n",
    "\n",
    "$$u \\cdot U = \\cos{\\theta}$$\n",
    "\n",
    "Combine this with our observation before that you can divide any vector by its length to get its unit vector, and we arrive at our **cosine formula** for any vectors $v$ and $w$ by just dividing their lengths:\n",
    "\n",
    "$$\n",
    "\\cos \\theta = \\frac{v \\cdot w}{||v||\\;||w||}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4867f5b",
   "metadata": {},
   "source": [
    "\n",
    "### Schwarz and Triangle Inequalities\n",
    "\n",
    "Because all cosines are between -1 and 1, it follows that the absolute value of the dot product, $|v \\cdot w|$, cannot exceed the product of the lengths, this is the **Schwarz Inequality**:\n",
    "\n",
    "$$|v \\cdot w| \\le ||v||\\: ||w||$$\n",
    "\n",
    "From the Schwarz Inequality [follows](https://math.stackexchange.com/a/91194) the **Triangle Inequality**:\n",
    "\n",
    "$$||u + v|| \\le ||u|| + ||v||$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a034db",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Independence and Dependence\n",
    "\n",
    "Vectors are **independent** if no combination other than 0 multiples gives $b=0$.  Vectors are **dependent** if multiple combinations give $b=0$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2be52981",
   "metadata": {},
   "source": [
    "## Matrices (1.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d2bb8b4",
   "metadata": {},
   "source": [
    "A matrix is **invertible** (aka **non-singular**) if it has independent (see definition above) column vectors, meaning $Ax = 0$ has only one solution between them.\n",
    "\n",
    "A matrix is **singular** if $Ax=0$ has many solutions.\n",
    "\n",
    "\n",
    "### Matrix Multiplication\n",
    "\n",
    "Via [MathIsFun](https://www.mathsisfun.com/algebra/matrix-multiplying.html):\n",
    "\n",
    "![image.svg](images/matrix-multiply.svg)\n",
    "\n",
    "It works through the dot product of each row and column.\n",
    "\n",
    "In order to multiply two matrices, the number of columns of A must equal the number of rows of B. The product\n",
    "AB will have the same number of rows as the first matrix and the same number of columns as the second.\n",
    "\n",
    "3Blue1Brown emphasized:\n",
    "- Viewing Matrices as transformation of space\n",
    "- Matrix multiplication is just one transformation after another [this may belong in subsequent section]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56e07f00",
   "metadata": {},
   "source": [
    "\n",
    "# Solving Linear Equations (2)\n",
    "\n",
    "* [Elemination with Matrices](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/pages/ax-b-and-the-four-subspaces/elimination-with-matrices/) - Lecture covering 2.2 and 2.3\n",
    "## Vectors and Linear Equations (2.1)\n",
    "\n",
    "Geometrically, it's worth noting that the dot product of each row with $x$ gives the equation of a plane.\n",
    "When the number of unknowns matches the number of equations, there is _usually_ one solution.\n",
    "\n",
    "### Matrix, Row, and Column Pictures\n",
    "\n",
    "Lets say we have $n$ equations and $n$ unknowns, and go over:\n",
    "* Matrix Form\n",
    "* Row Picture\n",
    "* Column Picture\n",
    "\n",
    "Let's look specifically at these two equations with two unknowns:\n",
    "$$\n",
    "2x - y = 0 \\\\\n",
    "-x + 2y = 3\n",
    "$$\n",
    "\n",
    "In **matrix form**, with the **coefficient matrix**, followed by the unknowns matrix, equal to solutions/right hand side would be:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2 & -1 \\\\\n",
    "-1 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These three matrices are abstractly referred to as $Ax=b$. When we are solving for $x$ (the inverse), we are abstractly solving $x = A^{-1}b$. And note that only with an invertible matrix (see below) can we solve this.\n",
    "\n",
    "The **row picture** is looking at one equation at a time, it's what we've seen before with systems of equations, or looking for where lines meet when we graph them geometrically.\n",
    "\n",
    "The **column picture** would have us formulate the equations as combinations of the columns, so:\n",
    "\n",
    "$$\n",
    "x \n",
    "\\begin{bmatrix}\n",
    "2 \\\\\n",
    "-1\n",
    "\\end{bmatrix}\n",
    "+ \n",
    "y\n",
    "\\begin{bmatrix}\n",
    "-1 \\\\\n",
    "2\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Geometrically, the column picture can solve these linear equations through vector addition, which we know geometrically means combining the column vectors each a certain number of times to produce the right hand side.\n",
    "\n",
    "### The Identity Matrix\n",
    "\n",
    "Multiplying $Ix$ where $I$ is the identity matrix, you get back the x you started with, $Ix=x$.  An example 3x3 identity matrix:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63361923",
   "metadata": {},
   "source": [
    "\n",
    "## The Idea of Elimination \n",
    "## Elimination Using Matrices \n",
    "## Rules for Matrix Operations\n",
    "## Inverse Matrices \n",
    "## Elimination = Factorization: A = LU\n",
    "## Transposes and Permutations\n",
    "# Vector Spaces and Subspaces\n",
    "## Spaces of Vectors\n",
    "## The Nullspace of A: Solving Ax = 0 and Rx = 0 \n",
    "## The Complete Solution to Ax = b\n",
    "## Independence, Basis and Dimension\n",
    "## Dimensions of the Four Subspaces\n",
    "# Orthogonality \n",
    "## Orthogonality of the Four Subspaces \n",
    "## Projections \n",
    "## Least Squares Approximations \n",
    "## Orthonormal Bases and Gram-Schmidt \n",
    "# Determinants 247\n",
    "## The Properties of Determinants \n",
    "## Permutations and Cofactors \n",
    "## Cramer’s Rule, Inverses, and Volumes \n",
    "# Eigenvalues and Eigenvectors \n",
    "## Introduction to Eigenvalues \n",
    "## Diagonalizing a Matrix \n",
    "## Systems of Differential Equations \n",
    "## Symmetric Matrices \n",
    "## Positive Definite Matrices "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb58bdd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
